{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[Introduction](#Introduction)\n",
    "\n",
    "[Functions to predict times](#Functions-to-predict-times)\n",
    "\n",
    "[Evaluation Functions](#Evaluation-Functions)\n",
    "\n",
    "[Weighted season prediction](#Weighted-season-prediction)\n",
    "\n",
    "[Probability Distribution Predictions](#Probability-Distribution-Predictions)\n",
    "\n",
    "[Model comparisons](#Model-comparisons)\n",
    "\n",
    "[Complications](#Complications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First a cell to prepare the notebook for the stuff that I might need to use. More imports can be added as necessary.\n",
    "# special IPython command to prepare the notebook for matplotlib\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from pattern import web\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# And the additional modules that I've used\n",
    "\n",
    "import fnmatch\n",
    "import os\n",
    "import pickle\n",
    "from PyPDF2 import PdfFileReader\n",
    "from tabula import read_pdf\n",
    "import urllib\n",
    "import random\n",
    "import sklearn\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib as mpl\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this fourth notebook of the series, I want to take the weight combination that I found to be the best in the previous notebook, and I want to compare it to what I am calling the <i>dist draw model</i> or <i>exponentially modified normal model</i>. In particular, this  model assumes that all of a racer's times are distributed in an exponentially modified normal form, and that there are no external factors that would influence where in the distribution a racer's time would fall for a particular race. \n",
    "\n",
    "To begin with, I need to load in the some of the dataframes that were created in previous notebooks. In particular, I want the dataframes that contain speed, prone and standing accuracies, and prone and standing range times for all biathletes over all events. In addition, I want the dataframes containing quantitative snow and weather similarities as well as altitude similarities.\n",
    "\n",
    "Previous Section: [Functions to predict times](#Functions-to-predict-times)\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "absolute_mens_speed = pd.read_pickle('absolute_mens_speed.pkl')\n",
    "absolute_mens_prone_range = pd.read_pickle('absolute_mens_prone_range.pkl')\n",
    "absolute_mens_prone_shooting = pd.read_pickle('absolute_mens_prone_shooting.pkl')\n",
    "absolute_mens_standing_range = pd.read_pickle('absolute_mens_standing_range.pkl')\n",
    "absolute_mens_standing_shooting = pd.read_pickle('absolute_mens_standing_shooting.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc_quant_weather_similarities = pd.read_pickle('wc_quant_weather_similarities.pkl')\n",
    "ibu_quant_weather_similarities = pd.read_pickle('ibu_quant_weather_similarities.pkl')\n",
    "\n",
    "wc_quant_snow_similarities = pd.read_pickle('wc_quant_snow_similarities.pkl')\n",
    "ibu_quant_snow_similarities = pd.read_pickle('ibu_quant_snow_similarities.pkl')\n",
    "\n",
    "wc_altitude_similarities = pd.read_pickle('wc_altitude_similarities.pkl')\n",
    "ibu_altitude_similarities = pd.read_pickle('ibu_altitude_similarities.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to predict times\n",
    "\n",
    "In order to have models to compare, we need to make predictions. Because these functions are taken directly from the previous notebook (Biathlon Report C-a), I'm going to write fairly minimally here. The five functions found below are:\n",
    "1. ```adjust_times```: It appears from looking at the data for individual racers that racer speeds generally increase somewhat linearly over time. The function adjust_times allows early career race speeds to be adjusted based on a linear fit of racer speed over all previous seasons (performed on a season by season basis) in order to try to mitigate the effects of time on speeds.\n",
    "2. ```build_racer_speed_distribution```: Creates a list of racer speeds by taking all of a given racer's speeds and then repeating them with a multiplicity that is dependent on the similarities between the conditions for the current competition and the individual prior competitions. A prior race with a similarity value of 1 (all predictor variables under consideration have identical or nearly identical values) would have its speed appear 10 times in the list. A prior race with a similarity value of 0.5 (the distance between the predictor variables under consideration is roughly half of the total spread for that variable) would have its speed appear 5 times on the list. ```build_racer_speed_distribution``` calls ```adjust_times``` to adjust the speeds for events that were held before the season under consideration to reflect general improvements in speed.\n",
    "3. ```build_racer_pr_distribution```:  Creates a list of n predicted total range times for a given racer in a given event. This is a fairly complicated process that involves the following steps:\n",
    "    1. For the given racer, determine which previous races (of both types) that racer has competed in. Collect range and accuracy (either prone or standing, depending on circumstance) into a pair of lists.\n",
    "    2. Using the weights associated to range times, produce a pair of weighted lists for range times and accuracy. Take a paired bootstrap sample (use the same ordered list of indices for both lists) and perform a linear regression on the results. The intercept is taken as the shooting time, and the slope is taken as the penalty loop time for any missed shots.\n",
    "    3. Using the weights associated to accuracy, produce a weighted list for accuracy. The mean of this list (or rather $a = (5-\\bar{x})/5$) will be taken to be the expected probability of making a particular shot. Five random values in the interval [0,1] are then computed. For each value, if the value is below $a$, the shot is made. Otherwise, the shot is missed.\n",
    "    4. The total range time is calculated as the sum of the shooting time and the product of the number of missed shots and the penalty loop time.\n",
    "4. ```racer_time_predict```: For a given racer, this function \n",
    "    1. first calls ```racer_speed_distribution```. Then, for each of the n desired predictions, it randomly selects a sample of size 10 from the returned list and computes the average.\n",
    "    2. next calls ```build_racer_pr_distribution``` twice, once for prone range times and once for standing range times. \n",
    "    3. finally, adds together the times in the resulting lists to produce a single list of n predicted race times for the given biathlete in the given race\n",
    "5. ```race_time_predictions```: calls ```racer_time_predict``` for each of the racers competing in a given race and stitches the outcomes together to form a single dataframe.\n",
    "\n",
    "Previous Section: [Introduction](#Introduction)\n",
    "\n",
    "Next Section: [Evaluation Functions](#Evaluation-Functions)\n",
    "\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "adjust_times : Takes a racer and his event speeds over the course of a career, finds the\n",
    "               best fit line through the data, and adjusts early speeds to reflect what \n",
    "               they would be predicted to be if the race were run in the season under \n",
    "               consideration.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "season : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "adjusted_racer : a list containing the racer's speed adjusted for the season under \n",
    "                 consideration\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "# To be called inside speed_predictions\n",
    "\n",
    "def adjust_times(racer, season):\n",
    "    \n",
    "    indices = racer.index.tolist()\n",
    "    years = [item.split(':')[1] for item in indices]\n",
    "    years = [''.join(['20',item[2:4]]) for item in years]\n",
    "    years = [float(item) for item in years]\n",
    "    years = np.array(years).reshape(-1,1)\n",
    "    speeds = np.array(racer.tolist()).reshape(-1,1)\n",
    "    \n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(years , speeds)\n",
    "    \n",
    "    coef = linreg.coef_[0][0]\n",
    "    \n",
    "    #print coef\n",
    "    # Now that we know the slope of the best fit line here, I want to create a \n",
    "    # time adjusted version of the speeds\n",
    "    \n",
    "    adjusted_racer = racer.copy()\n",
    "    \n",
    "    for i in range(len(racer)):\n",
    "        time_delta = float(season)- years[i][0]\n",
    "        #print time_delta\n",
    "        adjusted_racer[i] = adjusted_racer[i] + coef*time_delta\n",
    "        \n",
    "    return adjusted_racer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "build_racer_speed_distribution : takes a racer, season, event, and dataframes containing\n",
    "                                 similarity data about world cup vs world cup and world cup\n",
    "                                 vs ibu cup race conditions, and returns a list of speeds\n",
    "                                 from previous races, where the multiplicity of the speed\n",
    "                                 is determined by the degree of similarity between the race\n",
    "                                 of interest and the race from which the speed was taken\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "season : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "wc_sim : a dataframe containing values between 0 and 1 which codes the degree of similarity \n",
    "         between world cup (wc) races in a pairwise fashion. Pairs with a similarity value\n",
    "         of 1 were run under nearly identical conditions, while those with a similarity\n",
    "         value of 0 were run under extremely different conditions\n",
    "ibu_sim : a dataframe containing values between 0 and 1 which codes the degree of similarity \n",
    "          between world cup (wc) races and ibu cup races in a pairwise fashion. \n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "racer_predict : a list of speeds derived from the speeds of the racer's previous events. Each \n",
    "                prior speed is in the list with multiplicity n, where n is the rounded\n",
    "                product of 10 and the similarity score for the pairing between the current\n",
    "                event and the prior event\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def build_racer_speed_distribution(racer, season, event, wc_sim, ibu_sim):\n",
    "\n",
    "    col_name = ':'.join(['wc',season, event])\n",
    "    racer_data = absolute_mens_speed.loc[racer, :col_name]\n",
    "    name = racer\n",
    "    actual = float(racer_data[col_name])\n",
    "    short_racer_data = racer_data[2:-1].copy()\n",
    "    short_racer_data.dropna(inplace = True)\n",
    "    \n",
    "    # To drop later\n",
    "    #short_racer_data = short_racer_data[-20:]\n",
    "    \n",
    "    predictors = len(short_racer_data)\n",
    "    \n",
    "    year = ''.join(['20',season[2:4]])\n",
    "    short_racer_data = adjust_times(short_racer_data, year)\n",
    "    indices = short_racer_data.index.tolist()\n",
    "\n",
    "    race_weights = []\n",
    "    for index in indices:\n",
    "        split_index = index.split(':')\n",
    "        try:\n",
    "            if split_index[0] == 'wc':\n",
    "                race_weights.append(wc_sim.loc[':'.join([season,event]),\n",
    "                                               ':'.join([split_index[1],split_index[2]])])\n",
    "            else:\n",
    "                race_weights.append(ibu_sim.loc[':'.join([season,event]),\n",
    "                                                ':'.join([split_index[1],split_index[2]])])\n",
    "        except:\n",
    "            race_weights.append(0.0)\n",
    "    race_weights_rounded = [int(round(item,1)*10) for item in race_weights]\n",
    "\n",
    "    race_weights_rounded\n",
    "\n",
    "    # Making the weighted data list\n",
    "    \n",
    "    racer_predict = []\n",
    "\n",
    "    for i in range(len(short_racer_data)): \n",
    "        for j in range(race_weights_rounded[i]):\n",
    "            racer_predict.append(float(short_racer_data[i]))\n",
    "\n",
    "    return racer_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "build_racer_pr_distribution : takes a racer, season, event, and dataframes containing\n",
    "                              similarity data about world cup vs world cup and world cup\n",
    "                              vs ibu cup race conditions, and returns a list of predicted\n",
    "                              range times for the competition under consideration\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "season : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "wc_acc_sim : a dataframe containing values between 0 and 1 which codes the degree of \n",
    "             similarity between world cup (wc) races in a pairwise fashion. Pairs with \n",
    "             a similarity value of 1 were run under nearly identical conditions, while\n",
    "             those with a similarity value of 0 were run under extremely different \n",
    "             conditions. Chosen for predictive power for shooting accuracy\n",
    "ibu_acc_sim : a dataframe containing values between 0 and 1 which codes the degree of \n",
    "              similarity between world cup (wc) races and ibu cup races in a pairwise \n",
    "              fashion. Chosen for predictive power for shooting accuracy\n",
    "wc_range_sim : a dataframe containing values between 0 and 1 which codes the degree of \n",
    "               similarity between world cup (wc) races in a pairwise fashion. Chosen for\n",
    "               predictive power for range time (shooting time together with penalty time)\n",
    "ibu_range_sim : a dataframe containing values between 0 and 1 which codes the degree of \n",
    "                similarity between world cup (wc) races and ibu cup races in a pairwise \n",
    "                fashion. Chosen for predictive power for range time\n",
    "n : the length of the list of predicted range times that is returned\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "racer_predict : a list of predicted range times\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def build_racer_pr_distribution(racer, season, event, wc_acc_sim, ibu_acc_sim,\n",
    "                                wc_range_sim, ibu_range_sim,n):\n",
    "\n",
    "    col_name = ':'.join(['wc',season, event])\n",
    "    racer_time_data = absolute_mens_prone_range.loc[racer, :col_name]\n",
    "    racer_shot_data = absolute_mens_prone_shooting.loc[racer, :col_name]\n",
    "    name = racer\n",
    "    actual = float(racer_time_data[col_name])\n",
    "    short_racer_time_data = racer_time_data[2:-1].copy()\n",
    "    short_racer_shot_data = racer_shot_data[2:-1].copy()\n",
    "    short_racer_time_data.dropna(inplace = True)\n",
    "    short_racer_shot_data.dropna(inplace = True)\n",
    "    predictors = len(short_racer_shot_data)\n",
    "    \n",
    "    # To drop later\n",
    "    \n",
    "    short_racer_time_data = short_racer_time_data[-20:]\n",
    "    short_racer_shot_data = short_racer_shot_data[-20:]\n",
    "\n",
    "    \n",
    "    year = ''.join(['20',season[2:4]])\n",
    "    indices = short_racer_shot_data.index.tolist()\n",
    "    \n",
    "    # Separate weights for shooting accuracy and range times\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy_weights = []\n",
    "    for index in indices:\n",
    "        split_index = index.split(':')\n",
    "        if split_index[0] == 'wc':\n",
    "            accuracy_weights.append(wc_acc_sim.loc[':'.join([season,event]),\n",
    "                                                   ':'.join([split_index[1],split_index[2]])])\n",
    "        else:\n",
    "            try:\n",
    "                accuracy_weights.append(ibu_acc_sim.loc[':'.join([season,event]),\n",
    "                                                    ':'.join([split_index[1],split_index[2]])])\n",
    "            except: # There is missing data\n",
    "                accuracy_weights.append(0.0)\n",
    "    accuracy_weights_rounded = [int(round(item,1)*10) for item in accuracy_weights]\n",
    "\n",
    "    # Range times\n",
    "    range_weights = []\n",
    "    for index in indices:\n",
    "        split_index = index.split(':')\n",
    "        if split_index[0] == 'wc':\n",
    "            range_weights.append(wc_range_sim.loc[':'.join([season,event]),\n",
    "                                                  ':'.join([split_index[1],split_index[2]])])\n",
    "        else:\n",
    "            try:\n",
    "                range_weights.append(ibu_range_sim.loc[':'.join([season,event]),\n",
    "                                                  ':'.join([split_index[1],split_index[2]])])\n",
    "            except: # There is missing data\n",
    "                range_weights.append(0.0)\n",
    "    range_weights_rounded = [int(round(item,1)*10) for item in range_weights]\n",
    "\n",
    "    # And I'm going to have to split now to build my distributions\n",
    "    \n",
    "        # Making the weighted data list\n",
    "    \n",
    "    racer_accuracy = []\n",
    "    racer_shot = []\n",
    "    racer_time = []\n",
    "    racer_predict = []\n",
    "\n",
    "    for i in range(len(short_racer_shot_data)): \n",
    "        for j in range(accuracy_weights_rounded[i]):\n",
    "            racer_accuracy.append(float(short_racer_shot_data[i]))\n",
    "            \n",
    "    for i in range(len(short_racer_time_data)):\n",
    "        for j in range(range_weights_rounded[i]):\n",
    "            racer_shot.append(float(short_racer_shot_data[i]))\n",
    "            racer_time.append(float(short_racer_time_data[i]))\n",
    "            \n",
    "    # Build a model for shooting time and penalty loop time\n",
    "    \n",
    "    for k in range(n): \n",
    "        index_sample = np.random.choice(range(len(racer_shot)), \n",
    "                                                len(racer_shot), replace = True)\n",
    "        racer_shot_sample = [racer_shot[i] for i in index_sample]\n",
    "        racer_time_sample = [racer_time[i] for i in index_sample]\n",
    "        \n",
    "        accuracy = np.mean(racer_accuracy)/5\n",
    "        \n",
    "        linreg = LinearRegression()\n",
    "        linreg.fit(np.array(racer_shot_sample).reshape(-1,1),\n",
    "                                       np.array(racer_time_sample).reshape(-1,1))\n",
    "        loop = linreg.coef_\n",
    "        shot_time = linreg.intercept_\n",
    "        \n",
    "    # And predict number of missed shots\n",
    "\n",
    "        shooting = np.random.sample(5)\n",
    "        count = 0\n",
    "        for m in range(5):\n",
    "            if shooting[m] < accuracy:\n",
    "                count += 1\n",
    "        range_time = shot_time + count*loop\n",
    "        racer_predict.append(range_time[0][0])\n",
    "          \n",
    "    return racer_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "racer_time_predict : produces a list of predicted times for a given racer in a given \n",
    "                     competition\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer giving the desired number of times predicted\n",
    "length : a float giving the length of the ski portion of the course for the competition\n",
    "         under consideration\n",
    "weight_type : a list of lists containing the world cup and ibu similarity weightings\n",
    "              for the speed prediction and prone and standing range predictions\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "race_predictions : an n element list containing predicted total times for the given\n",
    "                   racer in the given competition\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def racer_time_predict(racer,year,event,n,length, weight_type):\n",
    "    \n",
    "    # First up, produce the speed estimates\n",
    "    \n",
    "    weightings = weight_type\n",
    "    #print weights[weightings[0]][0]\n",
    "    #print weights[weightings[0]][1]\n",
    "    \n",
    "    speed_distribution = build_racer_speed_distribution(racer, year, event, \n",
    "                                    weights[weightings[0]][0], weights[weightings[0]][1])\n",
    "    \n",
    "    ski_time_predictions = []\n",
    "    for i in range(n):\n",
    "        speed_sample = np.random.choice(speed_distribution, 10)\n",
    "        ski_time_predictions.append(length/np.mean(speed_sample))\n",
    "        \n",
    "    prone_range = build_racer_pr_distribution(racer, year, event, weights[weightings[1]][0], \n",
    "                                    weights[weightings[1]][1],weights[weightings[3]][0], \n",
    "                                              weights[weightings[3]][1],n)\n",
    "    standing_range = build_racer_pr_distribution(racer,year,event, weights[weightings[2]][0],\n",
    "                                    weights[weightings[2]][1],weights[weightings[4]][0],\n",
    "                                                 weights[weightings[4]][1],n)\n",
    "    \n",
    "    time_predictions = [x+y+z for x,y,z in zip(ski_time_predictions, \n",
    "                                               prone_range, standing_range)]\n",
    "    \n",
    "    race_predictions = [racer]\n",
    "    race_predictions.extend(time_predictions)\n",
    "    \n",
    "    return race_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "race_time_predictions : calls racer_time_predict repeatedly to create a dataframe of \n",
    "                        time predictions for all of the racers in a given competition\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer indicating the total number of time predictions to be made for each racer\n",
    "weight_type : a list of lists containing the world cup and ibu similarity weightings\n",
    "              for the speed prediction and prone and standing range predictions\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "predicted_times : a dataframe containing whose index consists of those racers who competed\n",
    "                  in the given race, and whose rows are the n predicted times for those\n",
    "                  racers\n",
    "problem_racers : a list of racers for whom racer_time_predict was unable to be executed,\n",
    "                 generally due to lack of prior race data\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def race_time_predictions(year,event,n, weight_type):\n",
    "    \n",
    "    # Find the length of the race\n",
    "    course_url = 'course_summary_%(year)s_M.pkl' %{'year' : year}\n",
    "    course_data = pd.read_pickle(course_url)\n",
    "    length = course_data.loc[course_data['Event'] == event]['Length'].tolist()[0]\n",
    "    \n",
    " #   print 'pickle data read'\n",
    " #   %memit\n",
    "    \n",
    "    # Get the list of racers\n",
    "    \n",
    "    race_code = ':'.join(['wc', year, event])\n",
    "    racer_indices = absolute_mens_speed[race_code].dropna().index.tolist()\n",
    "\n",
    "    # Make the predictions\n",
    "    \n",
    "    predicted_times = []\n",
    "    problem_racers = []\n",
    "    \n",
    "    for racer in racer_indices[:-1]:\n",
    "        try:\n",
    "            predicted_times.append(racer_time_predict(racer, year, event ,n, length, \n",
    "                                                      weight_type))\n",
    "        except:\n",
    "            problem_racers.append(racer)\n",
    "    \n",
    "    predicted_times = pd.DataFrame(predicted_times)\n",
    "    \n",
    "    predicted_times.set_index(0, drop = True, inplace = True)\n",
    "    \n",
    "    return predicted_times, problem_racers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Functions\n",
    "\n",
    "In order to evaluate the outcomes produced by our models, we need a number of functions that allow us to compare outcomes across multiple models. In order to do this, we define several additional functions.\n",
    "1. ```place_counts```: The function ```race_time_predictions``` produces a dataframe containing one row for each competitor in a competition (with the exception of those racers who have at most a single prior race) and with $n$ columns of predicted times. The function ```place_counts``` treats each column as a running of the race, and determines, for each column, the order of finish predicted for each racer. It then aggregates this data to produce a dataframe which again has a row for each competitor, but whose columns contain the number of predicted first place finishes, second place finishes, third place finishes, etc.  \n",
    "2. ```finding_percentiles```: The function ```race_time_predictions``` produces a dataframe containing one row for each competitor in a competition (with the exception of those racers who have at most a single prior race) and with $n$ columns of predicted times. The function ```finding_percentiles``` calculates, for each individual racer, various attributes of the distribution of times predicted for that racer, among them mean, median, 25th and 75th percentiles, and the difference between the racer's actual time and the mean of their predicted times. It then returns a dataframe which contains one row for each racer and columns for each attribute of their time distributions.\n",
    "3. ```evaluating_percentiles```: This function takes the dataframe that is output by ```finding_percentiles``` and, for each racer, determines where in that racers distribution of time predictions their actual time falls. The results are then returned as a two column dataframe containing the racers' names in the first column and the code for the location of their actual times within their distributions as the second. The codes for the different parts of the distribution range are as follows:\n",
    "    - 0 : actual time is faster than the minimum predicted time\n",
    "    - 1 : actual time is between the minimum and the 5th percentile of the predicted times\n",
    "    - 2 : actual time is between the 5th percentile and the 10th percentile of the predicted times\n",
    "    - 3 : actual time is between the 10th percentile and the 25th percentile of the predicted times\n",
    "    - 4 : actual time is between the 25th percentile and the median of the predicted times\n",
    "    - 5 : actual time is between the median and the 75th percentile of the predicted times\n",
    "    - 6 : actual time is between the 75th percentile and the 90th percentile of the predicted times\n",
    "    - 7 : actual time is between the 90th percentile and the 95th percentile of the predicted times\n",
    "    - 8 : actual time is between the 95th percentile and the maximum of the predicted times\n",
    "    - 9 : actual time is slower than the maximum predicted time\n",
    "4. ```evaluating_place_counts```: This function takes the dataframe that is output by ```place_counts``` and, for each racer, determines with what frequency the predicted place is correct, within one place of being correct, within two places of being correct, within five places of being correct, within ten places of being correct, and within twenty places of being correct. (For example, a racer who actually finished 35th would have all predicted finishes between 30th and 40th counted when determining the value for beining within five places of being correct.) It then returns a dataframe with one row for each competitor and one column for each of the seven categories under consideration.\n",
    "5. ```evaluating_dist_from_mean```: This function takes the dataframe output by ```finding_percentiles```. It uses the column ```dist_from_mean``` to determine for what percentage of the biathletes the mean of their distributions were within 10 seconds of their actual times, within 25 seconds of their actual times, within 50 seconds of their actual times, within 100 seconds of their actual times, within 150 seconds of their actual times, and within 200 seconds of their actual times. The result is then returned as a dataframe with only a single column. For reference, a change of 10 seconds in finishing time corresponds to a difference (on average) of 4-5 places in finishing place, while a change in finishing time of 100 seconds corresponds to a difference (on average) of 36-37 places in finishing place.\n",
    "6. ```average_from_mean```: This function takes the ```diff from mean``` column from the dataframe output by ```finding_percentiles``` and calculates both the mean of the values in the column and the mean of the absolute values of the entries in the column and returns both values as floats. The first computation gives us some indication of how balanced our errors in prediction are with respect to the actual times, since positive and negative values will cancel each other out. The second computation gives us an indication of overall error. In the case that the absolute values of these two numbers are the same (or nearly the same), it is an indication that the values being predicted tend to fall consistantly too high or too low.\n",
    "7. ```check_predictions```: This function takes the dataframe produced by ```finding_percentiles``` and returns a dataframe that, for each racer, indicates (via ```True``` or ```False```) whether that racer's actual time fell in the middle 50% of his predicted time distribution and whether his actual time fell in the middle 90% of his predicted time distribution.\n",
    "8. ```inside_outside```: This function takes the dataframe produced by ```check_predictions``` and returns two floats. The first is the percentage of the racers for whom the actual time fell inside of the middle 50% of their distribution, and the second is the percentage of the racers for whom the actual time fell inside of the middle 90% of their distribution.\n",
    "\n",
    "Previous Section: [Functions to predict times](#Functions-to-predict-times)\n",
    "\n",
    "Next Section: [Weighted season prediction](#Weighted-season-prediction)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "place_counts : treats each column of a race_time_predictions dataframe as an instance\n",
    "               of a race, and determines the finishing places for each racer within \n",
    "               that race\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "times : a dataframe that is the output of a call to race_time_predictions\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "place_count : a dataframe of integers indicating which place a given racer would have \n",
    "              had if the kth column of \n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def place_counts(times):\n",
    "    \n",
    "    predicted_places = times.copy()\n",
    "\n",
    "    for j in range(predicted_places.shape[1]):\n",
    "        predicted_places.sort_values(j+1,inplace = True)\n",
    "        for i in range(len(predicted_places)):\n",
    "            predicted_places.iloc[i,j] = i+1\n",
    "\n",
    "    place_count = pd.DataFrame(columns = range(1, len(times)+1), index = times.index)\n",
    "    \n",
    "    for racer in place_count.index:\n",
    "        for i in place_count.columns:\n",
    "            count = len([item for item in predicted_places.loc[racer] if item == i])\n",
    "            place_count.loc[racer,i] = count\n",
    "            \n",
    "    return place_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "finding_percentiles : takes the output of a call to race_time_predictions and returns a\n",
    "                      dataframe containing information about the distribution of predicted\n",
    "                      times for each racer\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "times : a dataframe that is the output of a call to race_time_predictions\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "racer_percentiles : a dataframe containing the name, mean time, standard deviation of time,\n",
    "                    minimum time, fifth percentile, tenth percentile, twenty-fifth percentile,\n",
    "                    median, seventy-fifth percentile, ninetieth percentile, ninety-fifth\n",
    "                    percentile, maximum time, actual time, and difference between the actual\n",
    "                    time and the mean predicted time for each racer\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def finding_percentiles(times, year, event):\n",
    "    \n",
    "    racer_percentiles = []\n",
    "    \n",
    "    filename = 'companal_SMSP_%(year)s_%(event)s.pkl' %{'year': year, 'event' : event}\n",
    "    event_data = pd.read_pickle(filename)\n",
    "    \n",
    "    for i in range(len(times)):\n",
    "        name = times.index[i]\n",
    "        racer = [name]\n",
    "        racer_data = times.iloc[i,:]\n",
    "        mean = np.mean(racer_data)\n",
    "        stdev = np.std(racer_data)\n",
    "        minimum = min(racer_data)\n",
    "        per5 = np.percentile(racer_data,5)\n",
    "        per10 = np.percentile(racer_data, 10)\n",
    "        per25 = np.percentile(racer_data,25)\n",
    "        median = np.median(racer_data)\n",
    "        per75 = np.percentile(racer_data,75)\n",
    "        per90 = np.percentile(racer_data,90)\n",
    "        per95 = np.percentile(racer_data,95)\n",
    "        maximum = max(racer_data)\n",
    "        actual = event_data.loc[event_data['Name'] == name]['Total Time'].tolist()[0]\n",
    "        difference = actual - mean\n",
    "        racer.extend([mean, stdev, minimum, per5, per10, per25, median, per75, per90, per95,\n",
    "                      maximum, actual, difference])\n",
    "        racer_percentiles.append(racer)\n",
    "        \n",
    "    racer_percentiles = pd.DataFrame(racer_percentiles, columns = ['Name', 'mean', 'deviation',\n",
    "                                                    'min', '5th per', '10th per', '25th per', \n",
    "                                                    'median', '75th per', '90th per', \n",
    "                                                    '95th per', 'maximum', 'actual time', \n",
    "                                                    'diff from mean'])\n",
    "    return racer_percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "evaluating_percentiles : takes a dataframe that is the output of a call to finding \n",
    "                         percentiles and returns a dataframe that indicates for each \n",
    "                         racer into what part of that racer's predicted times distribution\n",
    "                         his actual time falls\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "percentiles : a dataframe that is the output of a call to finding_percentiles\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "evaluation : a dataframe containing, for each racer, a code indicating in which portion \n",
    "             of that racer's predicted distribution his actual time falls.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def evaluating_percentiles(percentiles):\n",
    "    \n",
    "    evaluation = []\n",
    "    for i in range(len(percentiles)):\n",
    "        racer_data = percentiles.iloc[i,:]\n",
    "        name = racer_data[0]\n",
    "        actual = racer_data[-2]\n",
    "        if actual < percentiles.iloc[i,3]:\n",
    "            loc = 0\n",
    "        elif percentiles.iloc[i,3] <= actual < percentiles.iloc[i,4]:\n",
    "            loc = 1\n",
    "        elif percentiles.iloc[i,4] <= actual < percentiles.iloc[i,5]:\n",
    "            loc = 2\n",
    "        elif percentiles.iloc[i,5] <= actual < percentiles.iloc[i,6]:\n",
    "            loc = 3\n",
    "        elif percentiles.iloc[i,6] <= actual < percentiles.iloc[i,7]:\n",
    "            loc = 4\n",
    "        elif percentiles.iloc[i,7] <= actual < percentiles.iloc[i,8]:\n",
    "            loc = 5\n",
    "        elif percentiles.iloc[i,8] <= actual < percentiles.iloc[i,9]:\n",
    "            loc = 6\n",
    "        elif percentiles.iloc[i,9] <= actual < percentiles.iloc[i,10]:\n",
    "            loc = 7\n",
    "        elif percentiles.iloc[i,10] <= actual < percentiles.iloc[i,11]:\n",
    "            loc = 8\n",
    "        else:\n",
    "            loc = 9\n",
    "\n",
    "        evaluation.append([name,loc])\n",
    "        \n",
    "    evaluation = pd.DataFrame(evaluation, columns = ['Name','Location'])\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "evaluating_place_counts : takes a dataframe that is the output of place_counts and, for each\n",
    "                          racer, determines how many of the trial races had the racer placed\n",
    "                          correctly, within one place of his actual finish place, within two \n",
    "                          places of his actual finish place, etc \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "place_data : a dataframe that is the output of place_counts\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "       y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "       last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "place_evaluations : a dataframe containing a row for each racer in the competition. Each\n",
    "                    row in turn contains the number of predicted finishes places that were\n",
    "                    correct, within one place of the true place,within two places of the \n",
    "                    true place,within three places of the true place, within five places \n",
    "                    of the true place, within ten places of the true place, and within\n",
    "                    twenty places of the true place.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def evaluating_place_counts(place_data,year,event):\n",
    "    \n",
    "    # Get the actual places\n",
    "    filename = 'companal_SMSP_%(year)s_%(event)s.pkl' %{'year' : year, 'event' : event}\n",
    "    finish_place = pd.read_pickle(filename)[['Name','Total Time']]\n",
    "    finish_place.set_index('Name', inplace = True, drop = True)\n",
    "    finish_place = finish_place.loc[place_data.index]\n",
    "    finish_place.sort_values('Total Time', inplace = True)\n",
    "    finish_place.reset_index(inplace = True)\n",
    "    finish_place.reset_index(inplace = True)\n",
    "    finish_place.columns = ['Place','Name','Total Time']\n",
    "    finish_place.set_index('Name', inplace = True, drop = True)\n",
    "    \n",
    "    place_evaluations = []\n",
    "    \n",
    "    for racer in place_data.index.tolist():\n",
    "        racer_places = place_data.loc[racer]\n",
    "        racer_actual = finish_place.loc[racer,'Place']+1\n",
    "        racer_evaluation = [racer, racer_actual]\n",
    "        \n",
    "        for i in [0, 1, 2, 3, 5, 10, 20]:\n",
    "            \n",
    "            racer_range = range(racer_actual- i, racer_actual+i+1)\n",
    "            possible_range = set(range(1,len(racer_places)+1))\n",
    "            check_range = set(possible_range.intersection(racer_range))\n",
    "            count = 0\n",
    "            for j in check_range:\n",
    "                count = count + racer_places[j]\n",
    "            racer_evaluation.append(count)\n",
    "            \n",
    "        place_evaluations.append(racer_evaluation)\n",
    "        \n",
    "    place_evaluations = pd.DataFrame(place_evaluations)\n",
    "    place_evaluations.columns = ['Name','Place','Correct','Within 1','Within 2','Within 3',\n",
    "                                'Within 5','Within 10','Within 20']\n",
    "    \n",
    "    place_evaluations.sort_values('Place',inplace = True)\n",
    "    return place_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluating_dist_from_mean(diff_from_mean):\n",
    "    \n",
    "    evaluated_distances = []\n",
    "    \n",
    "    for i in [10,25,50,100,150,200]:\n",
    "        count = 0\n",
    "        for j in range(len(diff_from_mean)):\n",
    "            if abs(diff_from_mean[j])<= i:\n",
    "                count += 1\n",
    "        evaluated_distances.append(float(count)/len(diff_from_mean)*100)\n",
    "        \n",
    "    evaluated_distances = pd.DataFrame(evaluated_distances, index = ['in 10', 'in 25',\n",
    "                                                        'in 50', 'in 100', 'in 150','in 200'])\n",
    "    \n",
    "    return evaluated_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "absolute_mens_time = pd.read_pickle('absolute_mens_time.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\"quant_weather\" : [wc_quant_weather_similarities, ibu_quant_weather_similarities],\n",
    "           \"quant_snow\" : [wc_quant_snow_similarities, ibu_quant_snow_similarities],\n",
    "           \"altitude\" : [wc_altitude_similarities, ibu_altitude_similarities]}\n",
    "\n",
    "weight_combos = [['quant_snow','quant_weather','altitude','quant_snow','quant_weather']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the last time (in the previous report), I looped through all of the possible weight combinations for a given race (and repeated this for a small subset of races), this time I want to loop through all of the races in the given four season period for each of the 4 models that I have, and then to compare how each model fairs over the totality of the races. In addition, I'm going to add a couple of further measures of fit to the two that I was using previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "average_from_mean : takes the output of a call to finding_percentiles and calculates\n",
    "                    the mean of all of the entries in the 'diff from mean' column\n",
    "                    as well as the mean of all of the absolute values of the entries in the \n",
    "                    'diff from mean' column\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "diff_from_mean : the 'diff from mean' column found in the output of a call to \n",
    "                 finding_percentiles\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "average : the mean of all the entries in diff_from_mean\n",
    "abs_ave : the mean of the absolute values of teh entries in diff_from_mean\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def average_from_mean(diff_from_mean):\n",
    "    \n",
    "    average = np.mean(diff_from_mean)\n",
    "    abs_diff = [abs(item) for item in diff_from_mean]\n",
    "    abs_ave = np.mean(abs_diff)\n",
    "    \n",
    "    return average, abs_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "check_predictions : takes the output of a call to finding_percentiles and returns a \n",
    "                    dataframe indicating which racers fall within the middle 50 and 90\n",
    "                    percents of their time distributions\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe that is the output of a call to finding_percentiles\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "checked_predictions : a dataframe that indicates, for each racer in a given race, whether\n",
    "                      or not that racer's actual time fell within the middle 50% of their\n",
    "                      predicted time distribution and whether or not the actual time fell\n",
    "                      within the middle 90% of their predicted time distribution\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def check_predictions(df):\n",
    "    \n",
    "    checked_predictions = []\n",
    "    for i in range(len(df)):\n",
    "        name = df.loc[i,'Name']\n",
    "        if df.loc[i,'25th per'] <= df.loc[i,'actual time'] <= df.loc[i,'75th per']:\n",
    "            middle_50 = True\n",
    "        else:\n",
    "            middle_50 = False\n",
    "\n",
    "        if df.loc[i,'5th per'] <= df.loc[i,'actual time'] <= df.loc[i,'95th per']:\n",
    "            middle_90 = True\n",
    "        else:\n",
    "            middle_90 = False\n",
    "            \n",
    "        checked_predictions.append([name,middle_50, middle_90])\n",
    "        \n",
    "    checked_predictions = pd.DataFrame(checked_predictions, columns = ['name', 'middle_50',\n",
    "                                                                       'middle_90'])\n",
    "    \n",
    "    return checked_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "inside_outside : takes a dataframe output by check_predictions and returns the percentage\n",
    "                 of racers within the middle 50% of their distributions and the percentage\n",
    "                 of racers within the middle 90% of their distributions\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe that is the output of a call to check_predictions\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "percent_inside_50 : a float giving the percentage of racers in a given race whose actual time\n",
    "                    was in the middle 50% of their time distribution\n",
    "percent_inside_90 : a float giving the percentage of racers in a given race whose actual time\n",
    "                    was in the middle 90% of their time distribution\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def inside_outside(df):\n",
    "    \n",
    "    inside_50 = 0\n",
    "    outside_50 = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i,'middle_50']:\n",
    "            inside_50 += 1\n",
    "        else:\n",
    "            outside_50 += 1\n",
    "         \n",
    "    percent_inside_50 = inside_50/(float(inside_50) + outside_50)*100\n",
    "\n",
    "    inside_90 = 0\n",
    "    outside_90 = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i,'middle_90']:\n",
    "            inside_90 += 1\n",
    "        else:\n",
    "            outside_90 += 1\n",
    "        \n",
    "    percent_inside_90 = inside_90/(float(inside_90) + outside_90)*100\n",
    "\n",
    "    return percent_inside_50, percent_inside_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted season prediction\n",
    "\n",
    "After defining a number of functions, I could begin to use them to make predictions about race times and to evaluate the predictions that were made. Before doing so, I wanted a function that would go through all of the events in a given season, make time predictions for each race, and evaluate the quality of those predictions. The function\n",
    "1. ```evaluate_on_season``` runs through all of the events in a given season, using ```race_time_predictions``` to produce a dataframe of predicted times for each racer. From here, the functions defined in the above section allow us to produce and return four separate objects:\n",
    "    1. median_places_correct : a dataframe containing the concatenated outputs of the medians of the results of ```evaluating_place_counts``` for the events in the given season. In other words, a dataframe which contains one column for each event in the season, whose entries are the medians of the columns of the dataframes produced by ```evaluating_place_counts```. \n",
    "    2. distance_percentages : a dataframe containing the concatenated outputs of all the results of ```evaluating_dist_from_mean``` for the events in the given season.\n",
    "    3. averages_from_mean : an array containing the outputs of ```average_from_mean``` for each event in the given season.\n",
    "    4. percentages_in_center : an array containing the outputs of ```inside_outside``` for each event of the season\n",
    "    \n",
    "Next, I ran ```evaluate_on_season``` for the last four seasons for which I had data, namely 2014-15, 2015-16, 2016-17, and 2017-18. From here, I converted the arrays returned as ```averages_from_mean``` and ```percentages_in_center``` to dataframes and used pd.concat to stitch together the four resulting dataframes (from the four seasons being modeled) of each type into a single dataframe. This left me with a total of four dataframes\n",
    "- ```weight_0_averages_from_mean```\n",
    "- ```weight_0_percentages_in_center```\n",
    "- ```weight_0_distance_percentages```\n",
    "- ```weight_0_median_places_correct```\n",
    "containing information about the measures of perfomance of my model across 37 different races.\n",
    "\n",
    "Previous Section: [Evaluation Functions](#Evaluation-Functions)\n",
    "\n",
    "Next Section: [Probability Distribution Predictions](#Probability-Distribution-Predictions)\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "evaluate_on_season : for a given season, uses race_time_predictions to predict outcomes for\n",
    "                     all events and then evaluates and returns measures of goodness of fit\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "season : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "n : an integer giving the number of trials to be run for each event of the season\n",
    "weight_type :  a list of lists containing the world cup and ibu similarity weightings\n",
    "              for the speed prediction and prone and standing range predictions\n",
    "\n",
    "Returns\n",
    "-------\n",
    "median_places_correct : a dataframe containing the concatenated output of the results of \n",
    "                        evaluating_place_counts for each event of the season\n",
    "distance_percentages : a dataframe containing the concatenated output of the results of\n",
    "                       evaluating_dist_from_mean for each event of the season\n",
    "averages_from_mean : an array containing the outputs of average_from_mean for each event\n",
    "                     of the season\n",
    "percentages_in_center : an array containing the outputs of inside_outside for each event\n",
    "                        of the season\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_on_season(season, n, weight_type):\n",
    "    \n",
    "    races = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "    \n",
    "    median_places_correct = pd.DataFrame()\n",
    "    distance_percentages = pd.DataFrame()\n",
    "    averages_from_mean = []\n",
    "    percentages_in_center = []\n",
    "    \n",
    "    for race in races:\n",
    "        code = '%(season)s:%(race)s' %{'season' : season, 'race' : race}\n",
    "        print race\n",
    "        try:\n",
    "            predicted_times, problem_racers = race_time_predictions(season,race,n, weight_type)\n",
    "        \n",
    "            places = place_counts(predicted_times)\n",
    "            found_percentiles = finding_percentiles(predicted_times, season, race)\n",
    "            evaluated_percentiles = evaluating_percentiles(found_percentiles)\n",
    "            place_order_evaluations = evaluating_place_counts(places, season, race)\n",
    "            checked_percentiles = check_predictions(found_percentiles)\n",
    "\n",
    "            median_places = pd.DataFrame(place_order_evaluations.median(), columns = [code])\n",
    "            median_places_correct= pd.concat([median_places_correct,median_places], axis = 1)\n",
    "\n",
    "            distance_percentage = evaluating_dist_from_mean(found_percentiles['diff from mean'])\n",
    "            distance_percentage.columns = [code]\n",
    "            distance_percentages = pd.concat([distance_percentages, distance_percentage],\n",
    "                                                     axis = 1)\n",
    "\n",
    "            race_average_from_mean = [code]\n",
    "            race_average_from_mean.extend(average_from_mean(found_percentiles['diff from mean']))\n",
    "            averages_from_mean.append(race_average_from_mean)\n",
    "        \n",
    "            percentage_in_center = [code]\n",
    "            percentage_in_center.extend(inside_outside(checked_percentiles))\n",
    "            percentages_in_center.append(percentage_in_center)\n",
    "        \n",
    "        except:  #this race doesn't exist\n",
    "            pass\n",
    "\n",
    "\n",
    "    return median_places_correct, distance_percentages,averages_from_mean,percentages_in_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n",
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "median_places_correct, distance_percentages,averages_from_mean,percentages_in_center \\\n",
    "= evaluate_on_season('1415',10,weight_combos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n",
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "model_1415_median_places_correct, model_1415_distance_percentages,\\\n",
    "model_1415_averages_from_mean, model_1415_percentages_in_center \\\n",
    "= evaluate_on_season('1415',1000,weight_combos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n",
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "model_1516_median_places_correct, model_1516_distance_percentages,\\\n",
    "model_1516_averages_from_mean, model_1516_percentages_in_center \\\n",
    "= evaluate_on_season('1516',1000,weight_combos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n",
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "model_1617_median_places_correct, model_1617_distance_percentages,\\\n",
    "model_1617_averages_from_mean, model_1617_percentages_in_center \\\n",
    "= evaluate_on_season('1617',1000,weight_combos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n",
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "model_1718_median_places_correct, model_1718_distance_percentages,\\\n",
    "model_1718_averages_from_mean, model_1718_percentages_in_center \\\n",
    "= evaluate_on_season('1718',1000,weight_combos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what I think that I want to do next is to glue all of these things together, and then save the resulting thing to a .csv file. I will probably (I think) need to convert these things to dataframes first, but that seems fairly minor in the grand scheme of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1415_averages_from_mean = pd.DataFrame(model_1415_averages_from_mean)\n",
    "model_1415_percentages_in_center = pd.DataFrame(model_1415_percentages_in_center)\n",
    "\n",
    "model_1516_averages_from_mean = pd.DataFrame(model_1516_averages_from_mean)\n",
    "model_1516_percentages_in_center = pd.DataFrame(model_1516_percentages_in_center)\n",
    "\n",
    "model_1617_averages_from_mean = pd.DataFrame(model_1617_averages_from_mean)\n",
    "model_1617_percentages_in_center = pd.DataFrame(model_1617_percentages_in_center)\n",
    "\n",
    "model_1718_averages_from_mean = pd.DataFrame(model_1718_averages_from_mean)\n",
    "model_1718_percentages_in_center = pd.DataFrame(model_1718_percentages_in_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_0_averages_from_mean = model_1415_averages_from_mean\n",
    "\n",
    "weight_0_averages_from_mean = pd.concat([weight_0_averages_from_mean,\n",
    "                                         model_1516_averages_from_mean])\n",
    "weight_0_averages_from_mean = pd.concat([weight_0_averages_from_mean,\n",
    "                                         model_1617_averages_from_mean])\n",
    "weight_0_averages_from_mean = pd.concat([weight_0_averages_from_mean,\n",
    "                                         model_1718_averages_from_mean])\n",
    "\n",
    "#weight_0_averages_from_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_0_percentages_in_center = model_1415_percentages_in_center\n",
    "\n",
    "weight_0_percentages_in_center = pd.concat([weight_0_percentages_in_center,\n",
    "                                            model_1516_percentages_in_center])\n",
    "weight_0_percentages_in_center = pd.concat([weight_0_percentages_in_center,\n",
    "                                            model_1617_percentages_in_center])\n",
    "weight_0_percentages_in_center = pd.concat([weight_0_percentages_in_center,\n",
    "                                            model_1718_percentages_in_center])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_0_distance_percentages = model_1415_distance_percentages\n",
    "\n",
    "weight_0_distance_percentages = pd.concat([weight_0_distance_percentages,\n",
    "                                           model_1516_distance_percentages])\n",
    "weight_0_distance_percentages = pd.concat([weight_0_distance_percentages,\n",
    "                                           model_1617_distance_percentages])\n",
    "weight_0_distance_percentages = pd.concat([weight_0_distance_percentages,\n",
    "                                           model_1718_distance_percentages])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_0_median_places_correct = model_1415_median_places_correct\n",
    "\n",
    "weight_0_median_places_correct = pd.concat([weight_0_median_places_correct,\n",
    "                                            model_1516_median_places_correct])\n",
    "weight_0_median_places_correct = pd.concat([weight_0_median_places_correct,\n",
    "                                            model_1617_median_places_correct])\n",
    "weight_0_median_places_correct = pd.concat([weight_0_median_places_correct,\n",
    "                                            model_1718_median_places_correct])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "weight_0_averages_from_mean.to_csv('weight_0_averages_from_mean.csv')\n",
    "\n",
    "weight_0_percentages_in_center.to_csv('weight_0_percentages_in_center.csv')\n",
    "\n",
    "weight_0_distance_percentages.to_csv('weight_0_distance_percentages.csv')\n",
    "\n",
    "weight_0_median_places_correct.to_csv('weight_0_median_places_correct.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distribution Predictions\n",
    "\n",
    "In order to evaluate the quality of the predictions made by my model it seemed that there were two possibilities. The first, and most obvious, was to simply compare the outcomes of my model to the actual race times. Using this method of evaluation, we might say that if a racer's time for a particular race is 1500 seconds, and it is predicted to be 1501 seconds, that the model is good, while if the racer's time is predicted to be 1500 seconds but is predicted to be 1530 seconds, the model is bad. Similarly, if a racer's most likely finish is predicted to be in the top 5, and that racer finishes third, we might say that the model is good, while if the racer finishes seventh, we might say that the model is bad. The problem here is that, given the nature of biathlon, if these are the requirements to find a _good_ model, it may well be impossible to find anything but a _bad_ model. \n",
    "\n",
    "Let's talk briefly about why this might be. To begin with, the shooting aspect of biathlon adds an aspect of randomness that significantly impacts overall times. On average, a penalty lap adds 25 seconds to a racer's time directly, and also (in many cases), indirectly, as a more fatigued racer is likely to be marginally slower at the end of a race. Because these shots are independent events, it is not impossible that a biathlete who normally shoots with 90% accuracy might miss 8 out of his 10 shots (for a total time penalty in the neighborhood of over 3 minutes). Similarly, a biathlete who normally shoots with 50% accuracy might make all ten shots, which might make a particular race more than 2 minutes faster than anticipated. In addition, while it seems clear that conditions affect speed, accuracy, and shooting times, it also seems clear that the condition information that we have available is not sufficient to fully explain changes in speed, etc, even in the case that we included all of the details on all of the condition variables that we have. In other words, while we might be able to find a _best_ model for these races, we will probably be unable to find a _good_ model in the sense given above.\n",
    "\n",
    "In light of this, the best approach for evaluating the current model seems to be to compare its results to the results obtained by a naive model of the situation. That might be constructed in the following way: \n",
    "1. Observe that, for each racer, we find a wide variety of total times for the sprint race. For instance, for the last season's sprint races for Tarjei Boe, we find total times ranging from 1360 seconds to 1642 seconds, with a mean time of 1519 seconds and a standard deviation of 82.53 seconds. As a result, any naive model we choose should reflect this uncertaintly.\n",
    "2. Observe that it seems reasonable to think of these values as being drawn from some sort of distribution. We can use (and did, elsewhere) use the scipy.stats module to find the best fitting probability distribution for all of the racers who competed in at least 80 races, and to choose the probability distribution that best represented the most racers. Of the nearly 90 distributions available for consideration, only four appeared more than once in the list of best models for the 34 racers with at leat 80 times. Inspection of them revealed that two were symmetric and bimodal, neither of which was an attribute that seemed to realistically model the data that I was actually seeing. The other two, <a href = \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.exponnorm.html\">stat.exponnorm</a> and <a href = \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.johnsonsb.html\">stat.johnsonsb</a> both displayed the single peaked somewhat skewed distribution that seemed to best fit the data that I was looking at. When I tried to determine which of the two models was better over the same 34 races, I found that exponnorm was better for 17, and that johnsonsb was better for 17. I decided that since exponnorm has one fewer parameter to tune than johnsonsb, and that for at least some of the racers in some of the races we would be working with relatively few prior competitions, the better choice here would be to use exponnorm as the fit.\n",
    "3. Decide that a reasonable (naive) model would be to assume that the total times for each racer are drawn from an exponentially modified normal distribution, and that, further, we can get a good estimate of this by fitting an exponnorm distribution to all prior data and drawing values from this.\n",
    "\n",
    "Given these assumptions, we build our model using the following functions:\n",
    "1. ```racer_from_dist_draw```: This function takes the name of a racer and the codes (year and event) specifying a particular sprint competition, gets the race times for all previous races, fits an exponnorm distribution to them, and then randomly draws n predicted times from the distribution.\n",
    "2. ```predictions_from_dist_draw```: This function calls ```racer_from_dist_draw``` for each competitor in a given race and returns a dataframe with a row for each competitor and $n$ columns, one for each predicted time (for a given competitor).\n",
    "3. ```dist_draw_evaluate_on_season```: This function calls ```predictions_from_dist_draw``` for each event in the given season. It then returns four objects (which are the same as the objects returned by ```evaluate_on_season``` in weighted season prediction).\n",
    "    1. median_places_correct : a dataframe containing the concatenated outputs of the medians of the results of ```evaluating_place_counts``` for the events in the given season. In other words, a dataframe which contains one column for each event in the season, whose entries are the medians of the columns of the dataframes produced by ```evaluating_place_counts```. \n",
    "    2. distance_percentages : a dataframe containing the concatenated outputs of all the results of ```evaluating_dist_from_mean``` for the events in the given season.\n",
    "    3. averages_from_mean : an array containing the outputs of ```average_from_mean``` for each event in the given season.\n",
    "    4. percentages_in_center : an array containing the outputs of ```inside_outside``` for each event of the season\n",
    "\n",
    "We then ran ```dist_draw_evaluate_on_season``` for the seasons 2014-15, 2015-16, 2016-17, and 2017-18 (the most recent four seasons). From here, I converted the arrays returned as ```averages_from_mean``` and ```percentages_in_center``` to dataframes and used pd.concat to stitch together the four resulting dataframes (from the four seasons being modeled) of each type into a single dataframe. This left me with a total of four dataframes\n",
    "- ```dist_draw_averages_from_mean```\n",
    "- ```dist_draw_percentages_in_center```\n",
    "- ```dist_draw_distance_percentages```\n",
    "- ```dist_draw_median_places_correct```\n",
    "\n",
    "containing information about the measures of perfomance of the exponentially modified normal model across 37 different races.\n",
    "\n",
    "Previous Section: [Weighted season prediction](#Weighted-season-prediction)\n",
    "\n",
    "Next Section: [Model comparisons](#Model-comparisons)\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "racer_from_dist_draw : predicts n race times for a given racer in a given event based on the\n",
    "                       premise that the distribution of the racer's times is well modelled by\n",
    "                       an exponentially modified normal function.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer giving the number of predicted times desired for the given racer.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "racer_predict : a list of n predicted race times\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def racer_from_dist_draw(racer, year, event, n):\n",
    "    \n",
    "    col_name = ':'.join(['wc',year, event])\n",
    "    racer_data = absolute_mens_time.loc[racer, :col_name]\n",
    "    name = racer\n",
    "    actual = float(racer_data[col_name])\n",
    "    short_racer_data = racer_data[2:-1].copy()\n",
    "    short_racer_data.dropna(inplace = True)\n",
    "    #return short_racer_data\n",
    "    predictors = len(short_racer_data)\n",
    "    \n",
    "    params = stats.exponnorm.fit(short_racer_data.tolist())\n",
    "    \n",
    "    predictions = stats.exponnorm.rvs(params[0],params[1], params[2], size = n)\n",
    "    \n",
    "    racer_predict = [name]#, actual]\n",
    "    \n",
    "    racer_predict.extend(predictions)\n",
    "\n",
    "    return racer_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FOURCADE Martin',\n",
       " 1486.5137818078144,\n",
       " 1688.0420575905771,\n",
       " 1566.9096339352652,\n",
       " 1756.3231840943986,\n",
       " 1451.97440009039,\n",
       " 1591.3088890365048,\n",
       " 1751.9288046891268,\n",
       " 1721.5022443207049,\n",
       " 1546.6260766397279,\n",
       " 1775.2398424322118]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "racer_from_dist_draw('FOURCADE Martin', '1617', 'CP01',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "predictions_from_dist_draw : for a given competition, repeats predict_from_dist_draw for all\n",
    "                             competitors and returns the time predictions in the form of\n",
    "                             a dataframe\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "       y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "       last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer giving the number of predicted times desired for the given race.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "predicted_times : a dataframe containing all n of the predicted times for each racer.\n",
    "problem_racers : a list of racers for whome predict_from_dist_draw failed to execute. This\n",
    "                 is typically due to a lack of prior races for a given competitor.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def predictions_from_dist_draw(year, event, n):\n",
    "    \n",
    "    # Get the list of racers\n",
    "    \n",
    "    race_code = ':'.join(['wc', year, event])\n",
    "    racer_indices = absolute_mens_speed[race_code].dropna().index.tolist()\n",
    "\n",
    "    # Make the predictions\n",
    "    \n",
    "    predicted_times = []\n",
    "    problem_racers = []\n",
    "    \n",
    "    for racer in racer_indices[:-1]:\n",
    "        try:\n",
    "            racer_prediction = racer_from_dist_draw(racer, year, event, n)\n",
    "            if str(racer_prediction[2]) != 'nan':\n",
    "                predicted_times.append(racer_prediction)\n",
    "            else:\n",
    "                problem_racers.append(racer)\n",
    "        except: # I don't think there's enough data\n",
    "            pass\n",
    "    \n",
    "    predicted_times = pd.DataFrame(predicted_times)\n",
    "    predicted_times.set_index(0, drop = True, inplace = True)\n",
    "    \n",
    "    return predicted_times, problem_racers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "dist_draw_evaluate_on_season : for a given season, uses predictions_from_dist_draw to predict\n",
    "                               outcomes for all events and then evaluates and returns measures\n",
    "                               of goodness of fit\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "season : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "n : an integer giving the number of trials to be run for each event of the season\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "median_places_correct : a dataframe containing the concatenated output of the results of \n",
    "                        evaluating_place_counts for each event of the season\n",
    "distance_percentages : a dataframe containing the concatenated output of the results of\n",
    "                       evaluating_dist_from_mean for each event of the season\n",
    "averages_from_mean : an array containing the outputs of average_from_mean for each event\n",
    "                     of the season\n",
    "percentages_in_center : an array containing the outputs of inside_outside for each event\n",
    "                        of the season\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def dist_draw_evaluate_on_season(season, n):\n",
    "    \n",
    "    races = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "    \n",
    "    median_places_correct = pd.DataFrame()\n",
    "    distance_percentages = pd.DataFrame()\n",
    "    averages_from_mean = []\n",
    "    percentages_in_center = []\n",
    "    \n",
    "    for race in races:\n",
    "        code = '%(season)s:%(race)s' %{'season' : season, 'race' : race}\n",
    "        print race\n",
    "        try:\n",
    "            predicted_times, problem_racers = predictions_from_dist_draw(season,race,n)\n",
    "            places = place_counts(predicted_times)\n",
    "            found_percentiles = finding_percentiles(predicted_times, season, race)\n",
    "            evaluated_percentiles = evaluating_percentiles(found_percentiles)\n",
    "            place_order_evaluations = evaluating_place_counts(places, season, race)\n",
    "            checked_percentiles = check_predictions(found_percentiles)\n",
    "\n",
    "            median_places = pd.DataFrame(place_order_evaluations.median(), columns = [code])\n",
    "            median_places_correct= pd.concat([median_places_correct,median_places], axis = 1)\n",
    "\n",
    "            distance_percentage = evaluating_dist_from_mean(found_percentiles['diff from mean'])\n",
    "            distance_percentage.columns = [code]\n",
    "            distance_percentages = pd.concat([distance_percentages, distance_percentage],\n",
    "                                             axis = 1)\n",
    "\n",
    "            race_average_from_mean = [code]\n",
    "            race_average = average_from_mean(found_percentiles['diff from mean'])\n",
    "            race_average_from_mean.extend(race_average)\n",
    "            averages_from_mean.append(race_average_from_mean)\n",
    "        \n",
    "            percentage_in_center = [code]\n",
    "            percentage_in_center.extend(inside_outside(checked_percentiles))\n",
    "            percentages_in_center.append(percentage_in_center)\n",
    "        \n",
    "        except:  #this race doesn't exist\n",
    "            pass\n",
    "                \n",
    "          \n",
    "        \n",
    "    return median_places_correct, distance_percentages,averages_from_mean,percentages_in_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:1191: RuntimeWarning: divide by zero encountered in log\n",
      "  return exparg + np.log(0.5 * invK * sc.erfc(-(x - invK) / np.sqrt(2)))\n",
      "/anaconda2/lib/python2.7/site-packages/scipy/stats/_distn_infrastructure.py:2300: RuntimeWarning: Mean of empty slice.\n",
      "  muhat = tmp.mean()\n",
      "/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/anaconda2/lib/python2.7/site-packages/scipy/stats/_distn_infrastructure.py:2301: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  mu2hat = tmp.var()\n",
      "/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "dist_draw_1415_median_places_correct, dist_draw_1415_distance_percentages,\\\n",
    "dist_draw_1415_averages_from_mean,dist_draw_1415_percentages_in_center \\\n",
    "= dist_draw_evaluate_on_season('1415',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n",
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "dist_draw_1516_median_places_correct, dist_draw_1516_distance_percentages,\\\n",
    "dist_draw_1516_averages_from_mean, dist_draw_1516_percentages_in_center \\\n",
    "= dist_draw_evaluate_on_season('1516',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n",
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "dist_draw_1617_median_places_correct, dist_draw_1617_distance_percentages,\\\n",
    "dist_draw_1617_averages_from_mean, dist_draw_1617_percentages_in_center \\\n",
    "= dist_draw_evaluate_on_season('1617',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP01\n",
      "CP02\n",
      "CP03\n",
      "CP04\n",
      "CP05\n",
      "CP06\n",
      "CP07\n",
      "CP08\n",
      "CP09\n",
      "CH__\n",
      "OG__\n"
     ]
    }
   ],
   "source": [
    "dist_draw_1718_median_places_correct, dist_draw_1718_distance_percentages,\\\n",
    "dist_draw_1718_averages_from_mean, dist_draw_1718_percentages_in_center \\\n",
    "= dist_draw_evaluate_on_season('1718',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_draw_1415_averages_from_mean = pd.DataFrame(dist_draw_1415_averages_from_mean)\n",
    "dist_draw_1415_percentages_in_center = pd.DataFrame(dist_draw_1415_percentages_in_center)\n",
    "\n",
    "dist_draw_1516_averages_from_mean = pd.DataFrame(dist_draw_1516_averages_from_mean)\n",
    "dist_draw_1516_percentages_in_center = pd.DataFrame(dist_draw_1516_percentages_in_center)\n",
    "\n",
    "dist_draw_1617_averages_from_mean = pd.DataFrame(dist_draw_1617_averages_from_mean)\n",
    "dist_draw_1617_percentages_in_center = pd.DataFrame(dist_draw_1617_percentages_in_center)\n",
    "\n",
    "dist_draw_1718_averages_from_mean = pd.DataFrame(dist_draw_1718_averages_from_mean)\n",
    "dist_draw_1718_percentages_in_center = pd.DataFrame(dist_draw_1718_percentages_in_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_draw_averages_from_mean = dist_draw_1415_averages_from_mean\n",
    "\n",
    "dist_draw_averages_from_mean = pd.concat([dist_draw_averages_from_mean,\n",
    "                                       dist_draw_1516_averages_from_mean])\n",
    "dist_draw_averages_from_mean = pd.concat([dist_draw_averages_from_mean,\n",
    "                                       dist_draw_1617_averages_from_mean])\n",
    "dist_draw_averages_from_mean = pd.concat([dist_draw_averages_from_mean,\n",
    "                                       dist_draw_1718_averages_from_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_draw_percentages_in_center = dist_draw_1415_percentages_in_center\n",
    "\n",
    "dist_draw_percentages_in_center = pd.concat([dist_draw_percentages_in_center,\n",
    "                                          dist_draw_1516_percentages_in_center])\n",
    "dist_draw_percentages_in_center = pd.concat([dist_draw_percentages_in_center,\n",
    "                                          dist_draw_1617_percentages_in_center])\n",
    "dist_draw_percentages_in_center = pd.concat([dist_draw_percentages_in_center,\n",
    "                                          dist_draw_1718_percentages_in_center])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_draw_distance_percentages = dist_draw_1415_distance_percentages\n",
    "\n",
    "dist_draw_distance_percentages = pd.concat([dist_draw_distance_percentages,\n",
    "                                         dist_draw_1516_distance_percentages])\n",
    "dist_draw_distance_percentages = pd.concat([dist_draw_distance_percentages,\n",
    "                                         dist_draw_1617_distance_percentages])\n",
    "dist_draw_distance_percentages = pd.concat([dist_draw_distance_percentages,\n",
    "                                         dist_draw_1718_distance_percentages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_draw_median_places_correct = dist_draw_1415_median_places_correct\n",
    "\n",
    "dist_draw_median_places_correct = pd.concat([dist_draw_median_places_correct,\n",
    "                                          dist_draw_1516_median_places_correct])\n",
    "dist_draw_median_places_correct = pd.concat([dist_draw_median_places_correct,\n",
    "                                          dist_draw_1617_median_places_correct])\n",
    "dist_draw_median_places_correct = pd.concat([dist_draw_median_places_correct,\n",
    "                                          dist_draw_1718_median_places_correct])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "dist_draw_averages_from_mean.to_csv('dist_draw_averages_from_mean.csv')\n",
    "\n",
    "dist_draw_percentages_in_center.to_csv('dist_draw_percentages_in_center.csv')\n",
    "\n",
    "dist_draw_distance_percentages.to_csv('dist_draw_distance_percentages.csv')\n",
    "\n",
    "dist_draw_median_places_correct.to_csv('dist_draw_median_places_correct.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#dist_draw_averages_from_mean = pd.read_csv('dist_draw_averages_from_mean.csv')\n",
    "#dist_draw_percentages_in_center = pd.read_csv('dist_draw_percentages_in_center.csv')\n",
    "#dist_draw_distance_percentages = pd.read_csv('dist_draw_distance_percentages.csv')\n",
    "dist_draw_distance_percentages.set_index('Unnamed: 0', inplace = True)\n",
    "#dist_draw_median_places_correct = pd.read_csv('dist_draw_median_places_correct.csv')\n",
    "dist_draw_median_places_correct.set_index('Unnamed: 0', inplace = True)\n",
    "\n",
    "#weight_0_averages_from_mean = pd.read_csv('weight_0_averages_from_mean.csv')\n",
    "#weight_0_percentages_in_center = pd.read_csv('weight_0_percentages_in_center.csv')\n",
    "#weight_0_distance_percentages = pd.read_csv('weight_0_distance_percentages.csv')\n",
    "weight_0_distance_percentages.set_index('Unnamed: 0', inplace = True)\n",
    "#weight_0_median_places_correct = pd.read_csv('weight_0_median_places_correct.csv')\n",
    "weight_0_median_places_correct.set_index('Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparisons\n",
    "\n",
    "The next thing that I want to do is to make some sort of comparison for each of the four measure types that I've produced. I'd like to compare each of my three weighted models to the exponentially modified normal distribution model, and, assuming that they come out better than the exponentially modified normal distribution model, I'd like to decide which of those models seems to be the best. In order to do this, I began by defining some functions.\n",
    "1. ```center_percents_compare```: This function takes two dataframes (df and df1) produced by using ```evaluate_on_season``` or ```dist_draw_evaluate_on_season``` for two different models (namely, the output ```percentages_in_center```). For each race under consideration, it determines whether df or df1 has the larger value for ```inside_50``` (in other words, whether the first model or the second has a higher percentage of racers with actual times in the middle 50% of their predicted distributions). It then counts the number of times that the first model (df) is better, the number of times that the second model (df1) is better, and the number of times that they have the same value. The function then does the same thing using the ```inside_80``` columns of the dataframes. It then returns a dataframe with the results.\n",
    "2. ```averages_from_mean_compare```: This function takes two dataframes (df and df1) produced by using produced ```evaluate_on_season``` or ```dist_draw_evaluate_on_season``` for two different models (namely, the output ```average_from_mean```). For each race under consideration, it determines which model has a lower absolute value for ```average``` and which has the lower value for ```abs_ave```. These are then counted to determine in how many cases of each type the first model fairs better, in how many cases the second model fairs better, and in how many cases they fair the same. The results are then return in the form of a dataframe.\n",
    "3. ```clean_up_dataframe```: This function takes a block diagonal dataframe and returns a dataframe in which all blocks have been moved to the top of the dataframe.\n",
    "4. ```median_place_correct_compare```: This function takes two dataframes (df and df1) produced by using produced ```evaluate_on_season``` or ```dist_draw_evaluate_on_season``` for two different models (namely, the output ```median_places_correct```). For each race, it determines which of the two models (if either) fairs better in each of the 7 different categories. It then counts over all races under consideration and returns the results in the form of a dataframe.\n",
    "5. ```median_percent_improvement```: This function takes the same two dataframes as ```median_place_correct_compare``` and divides the race categories into three groups: those where the first model performed better, those where the second model performed better, and those where the two models performed equally well. For the race categories in the first group, the function that calculates the percentage improvement of the first model over the second and computes the mean of these improvements. For the race categories in teh second group, the function calculates the percentage improvement of the second model over the first and computes the mean of these improvements. These two values are then returned.\n",
    "6. ```distances_compare```: This function takes two dataframes (df and df1) produced by using produced ```evaluate_on_season``` or ```dist_draw_evaluate_on_season``` for two different models (namely, the output ```distance_percentages```). For each race, it determines which of the two models (if either) fairs better in each of the 6 different categories. It then counts over all races under consideration and returns the results in the form of a dataframe.\n",
    "7. ```distance_percent_improvement```: This function takes the same two dataframes as ```distances_compare``` and divides the race categories into three groups: those where the first model performed better, those where the second model performed better, and those where the two models performed equally well. For the race categories in the first group, the function that calculates the percentage improvement of the first model over the second and computes the mean of these improvements. For the race categories in teh second group, the function calculates the percentage improvement of the second model over the first and computes the mean of these improvements. These two values are then returned.\n",
    "    - N.B. There is one complication here. It is not unheard of, especially in the case of the smallest distances, for one or the other of the models to have a value of zero in their ```distance_percentages``` dataframe. This is obviously a problem, as division by zero leads to an infinite result, and therefore an infinite average. My solution has simply been to disregard these cases when finding these improvements. An obvious other choice would be to calculate straight differences rather than percent improvements.\n",
    "\n",
    "Previous Section: [Probability Distribution Predictions](#Probability-Distribution-Predictions)\n",
    "\n",
    "Next Section: [Complications](#Complications)\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "center_percents_compare : takes two dataframes produced by inside_outside and evaluates \n",
    "                          for how many cases each fairs better than the other\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe produced by inside_outside via dist_draw_evaluate_on_season\n",
    "df1 : a (second) dataframe produced by inside_outside via evaluate_on_season\n",
    "str1 : the name of df (as a string)\n",
    "str2 : the name of df1 (as a string)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "totals : a dataframe with counts of how frequently the first prediction method faired \n",
    "         better, how often the second faired better, and how often they tied\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def center_percents_compare(df, df1, str1, str2):\n",
    "    \n",
    "    df_better = []\n",
    "    df1_better = []\n",
    "    same = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        row = [df.iloc[i,0]]\n",
    "        if df.iloc[i,1] > df1.iloc[i,1]:\n",
    "            is_bigger_50 = 1\n",
    "        else:\n",
    "            is_bigger_50 = 0\n",
    "        if df.iloc[i,2] > df1.iloc[i,2]:\n",
    "            is_bigger_80 = 1\n",
    "        else:\n",
    "            is_bigger_80 = 0\n",
    "        row.append(is_bigger_50)\n",
    "        row.append(is_bigger_80)\n",
    "\n",
    "        df_better.append(row)\n",
    "        \n",
    "    for i in range(len(df)):\n",
    "        row = [df.iloc[i,0]]\n",
    "        if df.iloc[i,1] < df1.iloc[i,1]:\n",
    "            is_bigger_50 = 1\n",
    "        else:\n",
    "            is_bigger_50 = 0\n",
    "        if df.iloc[i,2] < df1.iloc[i,2]:\n",
    "            is_bigger_80 = 1\n",
    "        else:\n",
    "            is_bigger_80 = 0\n",
    "        row.append(is_bigger_50)\n",
    "        row.append(is_bigger_80)\n",
    "\n",
    "        df1_better.append(row)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = [df.iloc[i,0]]\n",
    "        if df.iloc[i,1] == df1.iloc[i,1]:\n",
    "            is_bigger_50 = 1\n",
    "        else:\n",
    "            is_bigger_50 = 0\n",
    "        if df.iloc[i,2] == df1.iloc[i,2]:\n",
    "            is_bigger_80 = 1\n",
    "        else:\n",
    "            is_bigger_80 = 0\n",
    "        row.append(is_bigger_50)\n",
    "        row.append(is_bigger_80)\n",
    "\n",
    "        same.append(row)\n",
    "\n",
    "    df_betterA = zip(*df_better)\n",
    "    df1_betterA = zip(*df1_better)\n",
    "    sameA = zip(*same)\n",
    "    totals = [[sum(df_betterA[1]),sum(df_betterA[2])],\n",
    "              [sum(df1_betterA[1]),sum(df1_betterA[2])],\n",
    "              [sum(sameA[1]),sum(sameA[2])]]\n",
    "    \n",
    "    totals = pd.DataFrame(totals, columns = ['middle 50%', 'middle 80%'], index =\n",
    "                     [\" \".join([str1, \"is better\"]), \" \".join([str2, \"is better\"]), 'same'])\n",
    "    \n",
    "    return totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>middle 50%</th>\n",
       "      <th>middle 80%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dist model is better</th>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my model is better</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      middle 50%  middle 80%\n",
       "dist model is better          26          35\n",
       "my model is better            11           2\n",
       "same                           0           0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_percents_compare(dist_draw_percentages_in_center,weight_0_percentages_in_center,\n",
    "                                                                \"dist model\", \"my model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what I'm seeing here is more or less what I was expecting to see here. The majority of the time, the exponentially modified normal model is doing better at producing a distribution of predicted times for which the actual time is in the middle of the distribution than the weight based models are. Why might that be? One obvious theory is that the exponentially modified normal model is better than the weight based models. Another is that the spread of the exponentially modified normal model might be considerably wider than that of the weight based models. In fact, if we look at the standard deviations of the distributions produced, we find that the standard deviations of the distributions produced by the exponentially modified normal model are more than double the standard deviations produced by the weighted models. In particular, it means that the width of the middle 50% of the exponentially modified normal distribution is approximately the same as the width of the middle 80% of the weighted distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next measure is the average distance between the actual racer times and the means of the prediction distributions. This was calculated in two different ways:\n",
    "1. The average of all differences between racer time and mean of distribution for each racer.\n",
    "2. The average of the absolute values of the differences.\n",
    "This allows us to, among other things, determine how balanced the errors are, in addition to how large they are. Here, the closer to zero these are, the better the model is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "averages_from_mean_compare : takes two dataframes produced by average_from_mean and \n",
    "                             evaluates for how many cases each fairs better than the other\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe produced by average_from_mean via dist_draw_evaluate_on_season\n",
    "df1 : a (second) dataframe produced by average_from_mean via evaluate_on_season\n",
    "str1 : the name of df (as a string)\n",
    "str2 : the name of df1 (as a string)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "totals : a dataframe with counts of how frequently the first prediction method faired \n",
    "         better, how often the second faired better, and how often they tied\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def averages_from_mean_compare(df, df1, str1, str2):\n",
    "    \n",
    "    df_better = []\n",
    "    df1_better = []\n",
    "    same = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        row = [df.iloc[i,0]]\n",
    "        if df.iloc[i,1] > df1.iloc[i,1]:\n",
    "            raw_average = 1\n",
    "        else:\n",
    "            raw_average = 0\n",
    "        if df.iloc[i,2] > df1.iloc[i,2]:\n",
    "            average_abs = 1\n",
    "        else:\n",
    "            average_abs = 0\n",
    "        row.append(raw_average)\n",
    "        row.append(average_abs)\n",
    "\n",
    "        df_better.append(row)\n",
    "        \n",
    "    for i in range(len(df)):\n",
    "        row = [df.iloc[i,0]]\n",
    "        if df.iloc[i,1] < df1.iloc[i,1]:\n",
    "            raw_average = 1\n",
    "        else:\n",
    "            raw_average = 0\n",
    "        if df.iloc[i,2] < df1.iloc[i,2]:\n",
    "            average_abs = 1\n",
    "        else:\n",
    "            average_abs = 0\n",
    "        row.append(raw_average)\n",
    "        row.append(average_abs)\n",
    "\n",
    "        df1_better.append(row)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = [df.iloc[i,0]]\n",
    "        if df.iloc[i,1] == df1.iloc[i,1]:\n",
    "            raw_average = 1\n",
    "        else:\n",
    "            raw_average = 0\n",
    "        if df.iloc[i,2] == df1.iloc[i,2]:\n",
    "            average_abs = 1\n",
    "        else:\n",
    "            average_abs = 0\n",
    "        row.append(raw_average)\n",
    "        row.append(average_abs)\n",
    "\n",
    "        same.append(row)\n",
    "\n",
    "    df_betterA = zip(*df_better)\n",
    "    df1_betterA = zip(*df1_better)\n",
    "    sameA = zip(*same)\n",
    "    totals = [[sum(df_betterA[1]),sum(df_betterA[2])],\n",
    "              [sum(df1_betterA[1]),sum(df1_betterA[2])],\n",
    "              [sum(sameA[1]),sum(sameA[2])]]\n",
    "    \n",
    "    totals = pd.DataFrame(totals, columns = ['average', 'absolute average'], index =\n",
    "                          [\" \".join([str1, \"is better\"]), \" \".join([str2, \"is better\"]), 'same'])\n",
    "    \n",
    "    return totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average</th>\n",
       "      <th>absolute average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dist model is better</th>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my model is better</th>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      average  absolute average\n",
       "dist model is better        4                25\n",
       "my model is better         33                12\n",
       "same                        0                 0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages_from_mean_compare(dist_draw_averages_from_mean,weight_0_averages_from_mean,\n",
    "                                                                   \"dist model\", \"my model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so here what we're seeing is that  our weighted model fairs better than our exponentially modified normal distribution model roughly 90% of the time in terms of average (in other words, the absolute value of the average distance between the mean of the racer's distribution and the racer's actual time was smaller for the our weighted model than for the exponentially modified normal model about 90% of the time). By contrast, we see that the exponentially modified normal model fairs better about 2/3s of the time when looking at the absolute averages, which indicates that the errors of the exponentially modified normal model are more balanced about the actual times than are those of the weighted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about for the other two measures, which were the two measures that we were considering in deciding which of the weight models we should continue with? Beginning with median_places_correct, we first need to clean up the dataframe that was produced..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "clean_up_dataframe : takes a block diagonal dataframe and converts it to a more\n",
    "                     standard form (because there's obviously something odd happening \n",
    "                     with my concatenation command)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a block diagonal dataframe\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df1 : the original dataframe with the blocks collapsed into only a single set of rows\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def clean_up_dataframe(df):\n",
    "    \n",
    "    df1 = pd.DataFrame(columns = df.columns.tolist(), index = df.index.tolist())\n",
    "    length = len(df)/4\n",
    "    \n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(length):\n",
    "            stuff = [df.iloc[j,i],df.iloc[j+length,i],df.iloc[j+2*length,i],\n",
    "                                         df.iloc[j+3*length,i]]\n",
    "            stuff = [x for x in stuff if str(x) != 'nan']\n",
    "            df1.iloc[j,i] = max(stuff)\n",
    "            \n",
    "    df1.dropna(how = 'any', axis = 'rows', inplace = True)\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_draw_median_places_correct = clean_up_dataframe(dist_draw_median_places_correct)\n",
    "weight_0_median_places_correct = clean_up_dataframe(weight_0_median_places_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I need to write the function that will allow me to make these comparisons. Here I want two functions:\n",
    "1. a function to compare how the two models do in terms of which has more successes in each category\n",
    "2. a function to compute how much better a given model fairs than another overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "median_place_correct_compare : takes the results of evaluating_place_counts for two\n",
    "                               different models, and returns a dataframe indicated\n",
    "                               with what frequency each of the models faired best in \n",
    "                               each category\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe produced by evaluating_place_counts via dist_draw_evaluate_on_season\n",
    "df1 : a (second) dataframe produced by evaluating_place_counts via evaluate_on_season\n",
    "str1 : the name of df (as a string)\n",
    "str2 : the name of df1 (as a string)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "totals : a dataframe indicating for how many race each method faired better for each of the \n",
    "         place count ranges under consideration\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def median_place_correct_compare(df, df1, str1, str2):\n",
    "    \n",
    "    df_better = []\n",
    "    df1_better = []\n",
    "    same = []\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        row = [df.columns.tolist()[i]]\n",
    "        for j in range(len(df)):\n",
    "            if df.iloc[j,i] > df1.iloc[j,i]:\n",
    "                score = 1\n",
    "            else:\n",
    "                score = 0\n",
    "            row.append(score)\n",
    "        df_better.append(row)\n",
    "        \n",
    "    for i in range(df.shape[1]):\n",
    "        row = [df.columns.tolist()[i]]\n",
    "        for j in range(len(df)):\n",
    "            if df.iloc[j,i] < df1.iloc[j,i]:\n",
    "                score = 1\n",
    "            else:\n",
    "                score = 0\n",
    "            row.append(score)\n",
    "        df1_better.append(row)\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        row = [df.columns.tolist()[i]]\n",
    "        for j in range(len(df)):\n",
    "            if df.iloc[j,i] == df1.iloc[j,i]:\n",
    "                score = 1\n",
    "            else:\n",
    "                score = 0\n",
    "            row.append(score)\n",
    "        same.append(row)\n",
    "\n",
    "    #print distances_compared\n",
    "        \n",
    "    df_betterA = zip(*df_better)\n",
    "    df1_betterA = zip(*df1_better)\n",
    "    sameA = zip(*same)\n",
    "    \n",
    "    totals = [[sum(df_betterA[2]),sum(df_betterA[3]),sum(df_betterA[4]),\n",
    "              sum(df_betterA[5]),sum(df_betterA[6]),sum(df_betterA[7]),sum(df_betterA[8])],\n",
    "              [sum(df1_betterA[2]),sum(df1_betterA[3]),sum(df1_betterA[4]),\n",
    "              sum(df1_betterA[5]),sum(df1_betterA[6]),sum(df1_betterA[7]),sum(df1_betterA[8])],\n",
    "              [sum(sameA[2]),sum(sameA[3]),sum(sameA[4]),\n",
    "              sum(sameA[5]),sum(sameA[6]),sum(sameA[7]),sum(sameA[8])]]\n",
    "    \n",
    "    totals = pd.DataFrame(totals, columns = df.index.tolist()[1:8],\n",
    "                index = [\" \".join([str1, \"is better\"]), \" \".join([str2, \"is better\"]), 'same'])\n",
    "    return totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Within 1</th>\n",
       "      <th>Within 2</th>\n",
       "      <th>Within 3</th>\n",
       "      <th>Within 5</th>\n",
       "      <th>Within 10</th>\n",
       "      <th>Within 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dist model is better</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my model is better</th>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Correct  Within 1  Within 2  Within 3  Within 5  \\\n",
       "dist model is better        0         0         0         0         0   \n",
       "my model is better         35        36        37        37        37   \n",
       "same                        2         1         0         0         0   \n",
       "\n",
       "                      Within 10  Within 20  \n",
       "dist model is better          0          0  \n",
       "my model is better           37         37  \n",
       "same                          0          0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_place_correct_compare(dist_draw_median_places_correct, weight_0_median_places_correct,\n",
    "                                                                    \"dist model\", \"my model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, looking at these, I can see that the exponentially modified normal distribution never fairs better than the weighted model and performs as well as the weighted model three times.  In total, the weighted model performs better than the exponentially modified normal model 256/259 = 99% of the time. This is significant, particularly since, in general, we're most interested in the order of finish for a particular race; the exact race times are something that we're less concerned with, except in the cases where some sort of record is at stake.\n",
    "\n",
    "Another question is, when the exponentially modified normal model is better, how much better is it? When one of the weighted models is better, how much better is it? For this, I want to consider percent improvement. In other words, if the exponentially modified normal model has 5 in a category, and the weighted model has 7 in that category, the weighted model will have faired 40% beter than the exponentially modified normal model in that category. Conversely, if the exponentially modified normal model has 7 in a category and the weighted model has 5 in that category, the exponentially modified normal model will have faired 40% better than the weighted model in that category.\n",
    "\n",
    "Once all of these percentages have been calculated, I'll calculate the averages for each group (exponentially modified normal better and weighted better). Categories where the exponentially modified normal and weighted models have the same value will simply be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "median_percent_improvement : takes two dataframes produced by evaluating_place_counts\n",
    "                             and computes the average improvement of the first over the\n",
    "                             second where the first is better, and of the second over the \n",
    "                             first where the second is better\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe produced by evaluating_place_counts via dist_draw_evaluate_on_season\n",
    "df1 : a (second) dataframe produced by evaluating_place_counts via evaluate_on_season\n",
    "str1 : the name of df (as a string)\n",
    "str2 : the name of df1 (as a string)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df_ave_improvement : the average percentage by which the first method improves over the\n",
    "                     second in cases where the first method is better\n",
    "df1_ave_improvement : the average percentage by which the second method improves over the\n",
    "                      first in cases where the second method is better\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def median_percent_improvement(df, df1):\n",
    "    \n",
    "    df_better = []\n",
    "    df1_better = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for j in range(df.shape[1]):\n",
    "            if df.iloc[i,j] > df1.iloc[i,j]:\n",
    "                improvement = (df.iloc[i,j]-df1.iloc[i,j])/df1.iloc[i,j]\n",
    "                df_better.append(improvement)\n",
    "            elif df.iloc[i,j] < df1.iloc[i,j]:\n",
    "                improvement = (df1.iloc[i,j]-df.iloc[i,j])/df.iloc[i,j]\n",
    "                df1_better.append(improvement)                \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    df_ave_improvement = np.mean(df_better)*100\n",
    "    df1_ave_improvement = np.mean(df1_better)*100\n",
    "    return df_ave_improvement, df1_ave_improvement\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan, 30.060452749115878)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_percent_improvement(dist_draw_median_places_correct, weight_0_median_places_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, looking at these results we see that, in the cases where the weighted model fairs the best, it is, on average, about 30% better than the exponentially modified normal model. Since there are no cases where the exponentially modified normal model fairs the best, we have no value returned there. In other words, not only does the weighted model outperform the exponentially modified normal model nearly 99% of the time, it outperforms the exponentially modified normal model on average by 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last measure that needs to be considered is that of distances. In particular, the measure that calculates for what percentage of racers in a given race is their actual time within 10 seconds of the mean, within 25 seconds of the mean, etc. Once again the dataframes were sewn together oddly, so I'll begin by fixing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_0_distance_percentages = clean_up_dataframe(weight_0_distance_percentages)\n",
    "dist_draw_distance_percentages = clean_up_dataframe(dist_draw_distance_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "distances_compare : takes the results of evaluating_dist_from_mean for two\n",
    "                    different models, and returns a dataframe indicated\n",
    "                    with what frequency each of the models faired best in \n",
    "                     each category\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe produced by evaluating_dist_from_mean via dist_draw_evaluate_on_season\n",
    "df1 : a (second) dataframe produced by evaluating_dist_from_mean via evaluate_on_season\n",
    "str1 : the name of df (as a string)\n",
    "str2 : the name of df1 (as a string)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "totals : a dataframe indicating for how many races each method faired better for each of the \n",
    "         place count ranges under consideration\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def distances_compare(df, df1, str1, str2):\n",
    "    \n",
    "    df_better = []\n",
    "    df1_better = []\n",
    "    same = []\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        row = [df.columns.tolist()[i]]\n",
    "        for j in range(len(df)):\n",
    "            if df.iloc[j,i] > df1.iloc[j,i]:\n",
    "                score = 1\n",
    "            else:\n",
    "                score = 0\n",
    "            row.append(score)\n",
    "        df_better.append(row)\n",
    "        \n",
    "    for i in range(df.shape[1]):\n",
    "        row = [df.columns.tolist()[i]]\n",
    "        for j in range(len(df)):\n",
    "            if df.iloc[j,i] < df1.iloc[j,i]:\n",
    "                score = 1\n",
    "            else:\n",
    "                score = 0\n",
    "            row.append(score)\n",
    "        df1_better.append(row)\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        row = [df.columns.tolist()[i]]\n",
    "        for j in range(len(df)):\n",
    "            if df.iloc[j,i] == df1.iloc[j,i]:\n",
    "                score = 1\n",
    "            else:\n",
    "                score = 0\n",
    "            row.append(score)\n",
    "        same.append(row)\n",
    "\n",
    "    #print distances_compared\n",
    "        \n",
    "    df_betterA = zip(*df_better)\n",
    "    df1_betterA = zip(*df1_better)\n",
    "    sameA = zip(*same)\n",
    "    \n",
    "    totals = [[sum(df_betterA[1]),sum(df_betterA[2]),sum(df_betterA[3]),\n",
    "              sum(df_betterA[4]),sum(df_betterA[5]),sum(df_betterA[6])],\n",
    "              [sum(df1_betterA[1]),sum(df1_betterA[2]),sum(df1_betterA[3]),\n",
    "              sum(df1_betterA[4]),sum(df1_betterA[5]),sum(df1_betterA[6])],\n",
    "              [sum(sameA[1]),sum(sameA[2]),sum(sameA[3]),\n",
    "              sum(sameA[4]),sum(sameA[5]),sum(sameA[6])]]\n",
    "    \n",
    "    totals = pd.DataFrame(totals, columns = df.index.tolist()[0:8],\n",
    "              index = [\" \".join([str1, \"is better\"]), \" \".join([str2, \"is better\"]), 'same'])\n",
    "    return totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in 10</th>\n",
       "      <th>in 25</th>\n",
       "      <th>in 50</th>\n",
       "      <th>in 100</th>\n",
       "      <th>in 150</th>\n",
       "      <th>in 200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dist model is better</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my model is better</th>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      in 10  in 25  in 50  in 100  in 150  in 200\n",
       "dist model is better      9     10     13      11      11      11\n",
       "my model is better       23     25     24      26      26      20\n",
       "same                      5      2      0       0       0       6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_compare(dist_draw_distance_percentages, weight_0_distance_percentages,\n",
    "                                                                  \"dist model\", \"my model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these results, we see that my model gives better results that the exponentially modified normal model more often than not. To be somewhat more precise, my model is better in  144/222 $\\approx$ 65% of cases, the exponentially modified normal normal model is better in 65/222 $\\approx$ 29% of cases, and the two models are the same in 13/222 $\\approx$ 6% of cases. \n",
    "\n",
    "Again, we want to ask the questions: When my model is better, how much better is it? When the exponentially modified normal model is better, how much better is it? This leads to the next function, ```distance_percent_improvement```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "distance_percent_improvement : takes two dataframes produced by dist_draw_evaluate_on_season\n",
    "                             and computes the average improvement of the first over the\n",
    "                             second where the first is better, and of the second over the \n",
    "                             first where the second is better\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe produced by evaluating_dist_from_mean via dist_draw_evaluate_on_season\n",
    "df1 : a (second) dataframe produced by evaluating_dist_from_mean via evaluate_on_season\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df_ave_improvement : the average percentage by which the first method improves over the\n",
    "                     second in cases where the first method is better\n",
    "df1_ave_improvement : the average percentage by which the second method improves over the\n",
    "                      first in cases where the second method is better\n",
    "                      \n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def distance_percent_improvement(df, df1):\n",
    "    \n",
    "    df_better = []\n",
    "    df1_better = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for j in range(df.shape[1]-1):\n",
    "            if (df.iloc[i,j] > df1.iloc[i,j]):\n",
    "                if (df1.iloc[i,j] > 0.0):\n",
    "                    improvement = (df.iloc[i,j]-df1.iloc[i,j])/df1.iloc[i,j]\n",
    "                    df_better.append(improvement)\n",
    "            elif df.iloc[i,j] < (df1.iloc[i,j]):\n",
    "                if (df.iloc[i,j] > 0.0):\n",
    "                    improvement = (df1.iloc[i,j]-df.iloc[i,j])/df.iloc[i,j]\n",
    "                    #print df.iloc[i,j], df1.iloc[i,j], improvement\n",
    "                    df1_better.append(improvement)                \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    df_ave_improvement = np.mean(df_better)*100\n",
    "    df1_ave_improvement = np.mean(df1_better)*100\n",
    "    return df_ave_improvement, df1_ave_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111.59012423537027, 154.46858066804899)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_percent_improvement(dist_draw_distance_percentages, weight_0_distance_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these, we find that, in the cases where the weighted model is better (roughly 65% of the time), they are better by 154% on average (so, if the exponentially modified normal model has 20% of racers in a particular range, the weighted models will have 51% of racers in that range), while in the cases where the exponentially modified normal models are better (roughly 29% of the time (equal values are not counted in either category)), the exponentially modified normal model is better by 112% on average. We can then calculate that, excluding cases where one of the models had a value of zero in a category and the other did not, that the weighted model is better on average by:\n",
    "$$\\frac{\\mathbf{-52.8}*32 + 154.47*65}{100} = 83.51\\%$$\n",
    "which is a substantial improvement.\n",
    "\n",
    "<u>NB</u>: To find the value $-52.8$ as the percentage improvement for my model when the exponentially modified normal model is in fact the better performer, we have\n",
    "$$\\begin{aligned}\n",
    "\\frac{a-b}{b} = 1.12 &\\Rightarrow a-b = 1.12b &\\\\\n",
    "                    &\\Rightarrow a = 2.12b &\\\\\n",
    "                    &\\Rightarrow \\frac{b-a}{a} &= \\frac{b-2.12b}{2.12b}\\\\\n",
    "                                           &      &= \\frac{1-2.12}{2.12}\\\\\n",
    "                                           &      &= -0.528\n",
    "\\end{aligned}$$\n",
    "which is -52.8%.\n",
    "<!-- add something about computing the -41 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is perhaps worthwhile here to consider how both of these models fair versus a random assignment of racers into places in terms of ```places_correct```. Unfortunately, this is the only measure that I can realistically do this for, or I would try to do it for the other measures as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "median_random_number_correct : computes the expected median number of assignments within k \n",
    "                               places of the correct assignment for a randomly ordered list\n",
    "                               of n values when the experiment is repeated m times\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "n : the number of racers\n",
    "m : the number of trials\n",
    "k : indicates the size of the range of acceptable answers\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "expected_count : the average expected number of racers within k places of their actual times  \n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def median_random_number_correct(n, m, k):\n",
    "    \n",
    "    # n is the number of racers\n",
    "    # m is the number of trials\n",
    "    # k measure the size of the range of acceptable answers\n",
    "    \n",
    "    counts = []\n",
    "    rate = ((2.0*k+1)/n)*m\n",
    "    for j in range(int(n-2*k)):\n",
    "        counts.append(rate)\n",
    "    for i in range(k):\n",
    "        rate = (2.0*k+1-(i+1))/n*m\n",
    "        counts.append(rate)\n",
    "        counts.append(rate)\n",
    "        \n",
    "    expected_count = np.median(counts)\n",
    "    return expected_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1415:CH__</th>\n",
       "      <th>1415:CP01</th>\n",
       "      <th>1415:CP02</th>\n",
       "      <th>1415:CP03</th>\n",
       "      <th>1415:CP04</th>\n",
       "      <th>1415:CP05</th>\n",
       "      <th>1415:CP06</th>\n",
       "      <th>1415:CP07</th>\n",
       "      <th>1415:CP08</th>\n",
       "      <th>1415:CP09</th>\n",
       "      <th>...</th>\n",
       "      <th>1617:CP09</th>\n",
       "      <th>1718:CP01</th>\n",
       "      <th>1718:CP02</th>\n",
       "      <th>1718:CP03</th>\n",
       "      <th>1718:CP04</th>\n",
       "      <th>1718:CP06</th>\n",
       "      <th>1718:CP07</th>\n",
       "      <th>1718:CP08</th>\n",
       "      <th>1718:CP09</th>\n",
       "      <th>1718:OG__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Place</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct</th>\n",
       "      <td>7.93651</td>\n",
       "      <td>10</td>\n",
       "      <td>9.52381</td>\n",
       "      <td>9.61538</td>\n",
       "      <td>10.4167</td>\n",
       "      <td>10</td>\n",
       "      <td>9.80392</td>\n",
       "      <td>10.5263</td>\n",
       "      <td>10.2041</td>\n",
       "      <td>11.3636</td>\n",
       "      <td>...</td>\n",
       "      <td>9.43396</td>\n",
       "      <td>9.34579</td>\n",
       "      <td>9.17431</td>\n",
       "      <td>9.70874</td>\n",
       "      <td>9.61538</td>\n",
       "      <td>9.09091</td>\n",
       "      <td>10</td>\n",
       "      <td>9.80392</td>\n",
       "      <td>11.7647</td>\n",
       "      <td>11.3636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within 1</th>\n",
       "      <td>23.8095</td>\n",
       "      <td>30</td>\n",
       "      <td>28.5714</td>\n",
       "      <td>28.8462</td>\n",
       "      <td>31.25</td>\n",
       "      <td>30</td>\n",
       "      <td>29.4118</td>\n",
       "      <td>31.5789</td>\n",
       "      <td>30.6122</td>\n",
       "      <td>34.0909</td>\n",
       "      <td>...</td>\n",
       "      <td>28.3019</td>\n",
       "      <td>28.0374</td>\n",
       "      <td>27.5229</td>\n",
       "      <td>29.1262</td>\n",
       "      <td>28.8462</td>\n",
       "      <td>27.2727</td>\n",
       "      <td>30</td>\n",
       "      <td>29.4118</td>\n",
       "      <td>35.2941</td>\n",
       "      <td>34.0909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within 2</th>\n",
       "      <td>39.6825</td>\n",
       "      <td>50</td>\n",
       "      <td>47.619</td>\n",
       "      <td>48.0769</td>\n",
       "      <td>52.0833</td>\n",
       "      <td>50</td>\n",
       "      <td>49.0196</td>\n",
       "      <td>52.6316</td>\n",
       "      <td>51.0204</td>\n",
       "      <td>56.8182</td>\n",
       "      <td>...</td>\n",
       "      <td>47.1698</td>\n",
       "      <td>46.729</td>\n",
       "      <td>45.8716</td>\n",
       "      <td>48.5437</td>\n",
       "      <td>48.0769</td>\n",
       "      <td>45.4545</td>\n",
       "      <td>50</td>\n",
       "      <td>49.0196</td>\n",
       "      <td>58.8235</td>\n",
       "      <td>56.8182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within 3</th>\n",
       "      <td>55.5556</td>\n",
       "      <td>70</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>67.3077</td>\n",
       "      <td>72.9167</td>\n",
       "      <td>70</td>\n",
       "      <td>68.6275</td>\n",
       "      <td>73.6842</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>79.5455</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0377</td>\n",
       "      <td>65.4206</td>\n",
       "      <td>64.2202</td>\n",
       "      <td>67.9612</td>\n",
       "      <td>67.3077</td>\n",
       "      <td>63.6364</td>\n",
       "      <td>70</td>\n",
       "      <td>68.6275</td>\n",
       "      <td>82.3529</td>\n",
       "      <td>79.5455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within 5</th>\n",
       "      <td>87.3016</td>\n",
       "      <td>110</td>\n",
       "      <td>104.762</td>\n",
       "      <td>105.769</td>\n",
       "      <td>114.583</td>\n",
       "      <td>110</td>\n",
       "      <td>107.843</td>\n",
       "      <td>115.789</td>\n",
       "      <td>112.245</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>103.774</td>\n",
       "      <td>102.804</td>\n",
       "      <td>100.917</td>\n",
       "      <td>106.796</td>\n",
       "      <td>105.769</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>107.843</td>\n",
       "      <td>129.412</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within 10</th>\n",
       "      <td>166.667</td>\n",
       "      <td>210</td>\n",
       "      <td>200</td>\n",
       "      <td>201.923</td>\n",
       "      <td>218.75</td>\n",
       "      <td>210</td>\n",
       "      <td>205.882</td>\n",
       "      <td>221.053</td>\n",
       "      <td>214.286</td>\n",
       "      <td>238.636</td>\n",
       "      <td>...</td>\n",
       "      <td>198.113</td>\n",
       "      <td>196.262</td>\n",
       "      <td>192.661</td>\n",
       "      <td>203.883</td>\n",
       "      <td>201.923</td>\n",
       "      <td>190.909</td>\n",
       "      <td>210</td>\n",
       "      <td>205.882</td>\n",
       "      <td>247.059</td>\n",
       "      <td>238.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within 20</th>\n",
       "      <td>325.397</td>\n",
       "      <td>410</td>\n",
       "      <td>390.476</td>\n",
       "      <td>394.231</td>\n",
       "      <td>427.083</td>\n",
       "      <td>410</td>\n",
       "      <td>401.961</td>\n",
       "      <td>431.579</td>\n",
       "      <td>418.367</td>\n",
       "      <td>465.909</td>\n",
       "      <td>...</td>\n",
       "      <td>386.792</td>\n",
       "      <td>383.178</td>\n",
       "      <td>376.147</td>\n",
       "      <td>398.058</td>\n",
       "      <td>394.231</td>\n",
       "      <td>372.727</td>\n",
       "      <td>410</td>\n",
       "      <td>401.961</td>\n",
       "      <td>482.353</td>\n",
       "      <td>465.909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1415:CH__ 1415:CP01 1415:CP02 1415:CP03 1415:CP04 1415:CP05  \\\n",
       "Place           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Correct     7.93651        10   9.52381   9.61538   10.4167        10   \n",
       "Within 1    23.8095        30   28.5714   28.8462     31.25        30   \n",
       "Within 2    39.6825        50    47.619   48.0769   52.0833        50   \n",
       "Within 3    55.5556        70   66.6667   67.3077   72.9167        70   \n",
       "Within 5    87.3016       110   104.762   105.769   114.583       110   \n",
       "Within 10   166.667       210       200   201.923    218.75       210   \n",
       "Within 20   325.397       410   390.476   394.231   427.083       410   \n",
       "\n",
       "          1415:CP06 1415:CP07 1415:CP08 1415:CP09    ...    1617:CP09  \\\n",
       "Place           NaN       NaN       NaN       NaN    ...          NaN   \n",
       "Correct     9.80392   10.5263   10.2041   11.3636    ...      9.43396   \n",
       "Within 1    29.4118   31.5789   30.6122   34.0909    ...      28.3019   \n",
       "Within 2    49.0196   52.6316   51.0204   56.8182    ...      47.1698   \n",
       "Within 3    68.6275   73.6842   71.4286   79.5455    ...      66.0377   \n",
       "Within 5    107.843   115.789   112.245       125    ...      103.774   \n",
       "Within 10   205.882   221.053   214.286   238.636    ...      198.113   \n",
       "Within 20   401.961   431.579   418.367   465.909    ...      386.792   \n",
       "\n",
       "          1718:CP01 1718:CP02 1718:CP03 1718:CP04 1718:CP06 1718:CP07  \\\n",
       "Place           NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "Correct     9.34579   9.17431   9.70874   9.61538   9.09091        10   \n",
       "Within 1    28.0374   27.5229   29.1262   28.8462   27.2727        30   \n",
       "Within 2     46.729   45.8716   48.5437   48.0769   45.4545        50   \n",
       "Within 3    65.4206   64.2202   67.9612   67.3077   63.6364        70   \n",
       "Within 5    102.804   100.917   106.796   105.769       100       110   \n",
       "Within 10   196.262   192.661   203.883   201.923   190.909       210   \n",
       "Within 20   383.178   376.147   398.058   394.231   372.727       410   \n",
       "\n",
       "          1718:CP08 1718:CP09 1718:OG__  \n",
       "Place           NaN       NaN       NaN  \n",
       "Correct     9.80392   11.7647   11.3636  \n",
       "Within 1    29.4118   35.2941   34.0909  \n",
       "Within 2    49.0196   58.8235   56.8182  \n",
       "Within 3    68.6275   82.3529   79.5455  \n",
       "Within 5    107.843   129.412       125  \n",
       "Within 10   205.882   247.059   238.636  \n",
       "Within 20   401.961   482.353   465.909  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_medians = pd.DataFrame(columns = weight_0_median_places_correct.columns.tolist(),\n",
    "                                index = weight_0_median_places_correct.index.tolist())\n",
    "\n",
    "k_codes = {1 : 0, 2 : 1, 3 : 2, 4 : 3, 5 : 5, 6 : 10, 7 : 20}\n",
    "for i in range(weight_0_median_places_correct.shape[1]):\n",
    "    n = np.rint(weight_0_median_places_correct.iloc[0,i]*2)\n",
    "    m = 1000\n",
    "    for j in range(1,weight_0_median_places_correct.shape[0]):\n",
    "        predicted = median_random_number_correct(n,m,k_codes[j])\n",
    "        expected_medians.iloc[j,i] = predicted\n",
    "        \n",
    "expected_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Within 1</th>\n",
       "      <th>Within 2</th>\n",
       "      <th>Within 3</th>\n",
       "      <th>Within 5</th>\n",
       "      <th>Within 10</th>\n",
       "      <th>Within 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>my model is better</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chance is better</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Correct  Within 1  Within 2  Within 3  Within 5  \\\n",
       "my model is better       37        37        37        37        37   \n",
       "chance is better          0         0         0         0         0   \n",
       "same                      0         0         0         0         0   \n",
       "\n",
       "                    Within 10  Within 20  \n",
       "my model is better         37         37  \n",
       "chance is better            0          0  \n",
       "same                        0          0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_place_correct_compare(weight_0_median_places_correct,expected_medians,\n",
    "                                                                 \"my model\", \"chance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Within 1</th>\n",
       "      <th>Within 2</th>\n",
       "      <th>Within 3</th>\n",
       "      <th>Within 5</th>\n",
       "      <th>Within 10</th>\n",
       "      <th>Within 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dist model is better</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chance is better</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Correct  Within 1  Within 2  Within 3  Within 5  \\\n",
       "dist model is better       37        37        37        37        37   \n",
       "chance is better            0         0         0         0         0   \n",
       "same                        0         0         0         0         0   \n",
       "\n",
       "                      Within 10  Within 20  \n",
       "dist model is better         37         36  \n",
       "chance is better              0          1  \n",
       "same                          0          0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_place_correct_compare(dist_draw_median_places_correct, expected_medians,\n",
    "                                                                 \"dist model\", \"chance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62.715128388264283, nan)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_percent_improvement(weight_0_median_places_correct,expected_medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22.742061756860394, 1.7268757443429865)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_percent_improvement(dist_draw_median_places_correct, expected_medians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the weighted model does better than random chance here in every single category (which is perhaps not a very high barrier, but does suggest that there is some structure here), while the exponentially modified normal model does better than random chance in every category but one. In addition, the weighted model has an average improvement of 62.7% over the weighted model, which is nearly triple the 22.7% average improvement of the exponentially modified normal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Complications\n",
    "\n",
    "The most obvious complication with this model is that using it requires that we know (or at least be able to predict) something about the altitude of the race site, the snow conditions during the race, and the weather conditions during the race. While the first of these is relatively trivial to obtain by looking at one of the myriad interactive topographical world maps available on the internet, and one might reasonably be able to predict the last of them by looking at the hourly weather forecast before the beginning of the race, predicting the snow conditions poses a potential problem. The hope would be that by looking at the weather forecast, and perhaps at weather conditions for the few days prior to the event, we might be able to make some reasonable guess at what the snow conditions were likely to be like. This is certainly a shortcoming of this approach though, particularly as the snow conditions are the variable that we are using to inform our speed predictions, which account for around 80% of the total time. (And a change of 0.5 m/s in average speed over a 10000 m race produces a change in total time of around a minute and a half.)  \n",
    "\n",
    "\n",
    "**Note**: While the hourly forecast is typically not available more than 24 hours before a competition, the list of racers is typically available only hours before a competition, so this does not pose a real problem.\n",
    "\n",
    "Previous Section: [Model comparisons](#Model-comparisons)\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
