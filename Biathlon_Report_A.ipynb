{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "\n",
    "[Introduction](#Introduction)\n",
    "\n",
    "[Collecting the Race Data](#Collecting-the-race-data)\n",
    "\n",
    "[Collecting the IBU Cup Sprint Data](#Collecting-IBU-Cup-sprint-data)\n",
    "\n",
    "[Deleting Bad Racer Data](#Deleting-bad-racer-data)\n",
    "\n",
    "[Collecting the Course Data](#Collecting-the-course-data)\n",
    "\n",
    "[Collecting the Weather Data](#Collecting-the-weather-data)\n",
    "\n",
    "[Collecting IBU Cup course and weather data](#Collecting-IBU-Cup-course-and-weather-data)\n",
    "\n",
    "[Adjustments to course and weather data](#Adjustments-to-course-and-weather-data)\n",
    "\n",
    "[Exploring Relationships](#Exploring-Relationships)\n",
    "\n",
    "[Effect of Conditions on Speed](#Effects-of-conditions-on-speed)\n",
    "\n",
    "[Evaluating Impacts on Speed](#Evaluating-Impacts-on-Speed)\n",
    "\n",
    "[Effect of Conditions on Shooting Accuracy](#Effects-of-conditions-on-shooting-accuracy)\n",
    "\n",
    "[Evaluating Impacts on Shooting Accuracy](#Evaluating-Impacts-on-Accuracy)\n",
    "\n",
    "[Effect of Conditions on Range and Penalty Times](#Effects-of-conditions-on-range-and-penalty-times)\n",
    "\n",
    "[Evaluating Impacts on Range Times](#Evaluating-Impacts-on-Range-Times)\n",
    "\n",
    "[Evaluating Impacts on Penalty Loop Times](#Evaluating-Impacts-on-Penalty-Loop-Times)\n",
    "\n",
    "[Handing off to the next notebook](#Handing-off-to-the-next-notebook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The biathlon is a winter individual sport that consists of alternating stints of cross country skiing and target shooting. There are four major event types (not including relays) which exist in both men's and women's versions: sprint, pursuit, individual, and mass start. Of these, the sprint is the most frequently run, and typically has the largest number of competitors. (Pursuit and mass start both limit the number of competitors in a race in order to avoid overcrowding at the shooting range, while individual is run only a few times per season.)\n",
    "\n",
    "The men's sprint race consists of a roughly 3300m ski loop (due to the fact that these events take place outside, there is some variation in this distance) followed by 5 shots in a prone position, then another 3300m ski loop followed by 5 shots in a standing position, and finally another 3300m ski loop before the finish line. Each missed shot requires the biathlete to ski a 150m penalty lap before continuing with the course. For the sprint, racers leave the start gate at 30 second intervals, which means that they are not in direct competition with each other. \n",
    "\n",
    "There are two levels of international biathlon competition, the higher of the two is the World Cup circuit, the other is the IBU (International Biathlon Union) Cup circuit. Most biathletes start their careers on the IBU Cup circuit before the most successful of them move up to compete on the World Cup circut. In this project, I am be interested in making predictions about outcomes of World Cup events, in part because the athletes who don't move up to the World Cup circuit tend not to compete for more than a few seasons, however, I will be collecting data from IBU Cup races as well in order to have data about each racer from the beginning of their career.\n",
    "\n",
    "In this project, I hope to explore the effects of different conditions (snow, temperature, amount of climb) on the results of men's sprint races, as well as to use the results of earlier races to make some predictions about the outcomes of later races. \n",
    "\n",
    "[Next Section: Collecting the Race Data](#Collecting-the-race-data)\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First a cell to prepare the notebook.\n",
    "# special IPython command to prepare the notebook for matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# And the additional modules that I've used\n",
    "\n",
    "import pickle\n",
    "from PyPDF2 import PdfFileReader\n",
    "from tabula import read_pdf\n",
    "import urllib\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "#import requests\n",
    "#from pattern import web\n",
    "#from fnmatch import fnmatch\n",
    "#import math\n",
    "#import fnmatch\n",
    "#import os\n",
    "#import sklearn\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.datasets import make_regression\n",
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#from sklearn.linear_model import Lasso\n",
    "#from sklearn.linear_model import LassoCV\n",
    "#from sklearn.linear_model import Ridge\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn import linear_model\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#import joblib\n",
    "\n",
    "#import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CollectingData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the race data\n",
    "\n",
    "The first step, obviously, was to collect the data. As it turned out, this was more difficult than I had envisioned, because the results of the races were stored in pdf files which needed to be downloaded and read using [tabula-py](https://blog.chezo.uno/tabula-py-extract-table-from-pdf-into-python-dataframe-6c7acfa5f302). To further complicate things, the [International Biathlon Union's results website](https://biathlonresults.com) buries access to the pdf files in several layers of java code, which makes downloading the source code useless. Eventually I was able to discover that the pdf files of the competition results could be accessed directly using urls that looked like this:\n",
    "\n",
    "https://ibu.blob.core.windows.net/docs/1617/BT/SWRL/CP04/SMSP/BT_C77B_1.0.pdf\n",
    "\n",
    "or more generally, like:\n",
    "\n",
    "https://ibu.blob.core.windows.net/season/BT/SWRL/event/race/BT_C77B_1.0.pdf\n",
    "\n",
    "In order to retrieve these pdf files and store the data that I wanted from them in a form that was both accessible and useful for me, I used the following group of functions:\n",
    "1. ```get_analysis_file```: This function takes the season, event, and type of a given World Cup race, and finds the url of the pdf file containing the competition analysis data. It then retrieves the pdf and stores it on the hard drive as biathlon.pdf.\n",
    "2. ```flatten_row```: This function takes a row from the tabula-py reading of a biathlon.pdf file and returns it as a list of values.\n",
    "3. ```competition_analysis_splitSP```: This function takes a the dataframe produced by reading a single page of a competition analysis pdf using tabula-py, along with the year and event in which the race was held. It then parses it as follows:\n",
    "    - First, eliminate any racers found at the end of the page who were disqualified, abandonned the race partway through, or did not start. Since these racers are listed at the very end of the file, any subsequent pages of the pdf can be disregarded.\n",
    "    - Find all rows in which the word \"Cumulative\" occurs. These serve as anchor rows that allow us to locate all the other needed information.\n",
    "    - Get the names of the competitors on the given page. The rows containing this information can be found directly above the rows with \"Cumulative\" in them. These will make up the first column of the dataframe.\n",
    "    - Get the shooting times and missed shots. The rows containing this information are found two rows after the anchor rows. Add these columns to the dataframe.\n",
    "    - Get the times for the individual course loops. The rows containing this information are found four rows after the anchor rows. Add these columns to the dataframe.\n",
    "    - Get the range times. The rows containing this information are found three rows after the anchor rows. Add these columns to the dataframe.\n",
    "    - For seasons beginning with 2011-2012, get the penalty times. The rows containing this information are found five rows after the anchor rows. Add these columns to the dataframe. (Note that prior to the 2011-2012 season, the penalty times are included in the range times.)\n",
    "4. ```competition_analysis_mergeSP```: This function uses tabula-py to read in each page of a biathlon.pdf file, sends the resulting page to ```competition_analysis_splitSP```, and then concatenates the resulting dataframes into a single large dataframe.\n",
    "5. ```pickle_competition_analysisSP```: This function takes a takes a year, event, and race type and  calls ```get_analysis_file``` to download the relevant competition analysis file to the hard drive. It then calls ```competition_analysis_merge``` to extract the desired data and place it into a dataframe. Finally, it stores the resulting dataframe as a pickle file.\n",
    "6. ```pickle_biathlon_seasonSP```: This function loops through all possible events for a given year and attempts to run ```pickle_competition_analysisSP``` on each. A try and except structure handles cases where there is not actually a race of the desired type (for this project, a men's sprint) held during the given event.\n",
    "\n",
    "I then looped through all the seasons for which I could obtain the requisite World Cup data (there are no [competition data summary](https://ibu.blob.core.windows.net/docs/0910/BT/SWRL/CP01/SMSP/BT_C82_1.0.pdf) files available before the 2004-2005 season) using ```pickle_biathlon_season``` and stored the dataframes produced as pickle files on the hard drive. Unfortunately, using tabula-py effectively requires specifying the area of the table to be read, and, while in theory there should be a consistant layout for each season, in practice each host site seems to have the ability to design their own layouts. As a result, some of the competition analysis files were not correctly read. \n",
    "\n",
    "In order to read these files, I used a slightly altered version of ```competition_analysis_mergeSP``` together with ```pickle_competition_analysisSP``` to obtain the desired data for those files that were previously problematic.\n",
    "\n",
    "Finally, because the Turin (2006) and Sochi (2014) ```Olympic Games``` used a completely different form of url for their competition analysis files, I used ```pickle_competition_analysisSP``` on manually obtained pdf files in order to get and store the necessary data.\n",
    "\n",
    "Previous Section: [Introduction](#Introduction)\n",
    "\n",
    "Next Section: [Collecting the IBU Cup Sprint Data](#Collecting-IBU-Cup-sprint-data)\n",
    "\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "get_analysis_file : takes the season, event, and type of a given World \n",
    "                    Cup race, and finds the url of the pdf file containing the \n",
    "                    competition analysis data, then retrieves it and stores\n",
    "                    it as a pdf file named biathlon.pdf\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year: the years of the biathlon season, in form y1y2 where y1 is the last 2 digits\n",
    "      of the first year and y2 is the last two digits of the second year\n",
    "event: a four character code specifying the event. Possibilities are “CP01”, “CP02”,\n",
    "       . . ., “CP09”, “OG__”,”CH__”\n",
    "race: a four character code specifying the type of race. Possibilities are SWIN, SWSP,\n",
    "      SWPU, SWMS, SMIN, SMSP, SMPU, SMMS\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "Stores a .pdf file on the hard drive as biathlon.pdf. \n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_analysis_file(year,event,race):\n",
    "    codes = {\"SWIN\": \"A\", \"SMIN\": \"A\", \"SWSP\": \"B\", \"SMSP\": \"B\", \"SWPU\": \"D\", \"SMPU\": \"D\",\n",
    "             \"SWMS\": \"D\", \"SMMS\": \"D\"}\n",
    "    L = codes[race]\n",
    "    \n",
    "    if year in [\"0102\", \"0203\",\"0304\"]:\n",
    "        if race in [\"SWMS\", \"SMMS\"]:\n",
    "            L = \"D\"\n",
    "        url = (\n",
    "        \"https://ibu.blob.core.windows.net/docs/%(y)s/BT/SWRL/%(e)s/%(r)s/BT_O77%(L)s_1.0.pdf\"\n",
    "               %{\"e\": event, \"r\":race, \"y\":year, \"L\":L} )       \n",
    "    \n",
    "    else:\n",
    "        url = (\n",
    "        \"https://ibu.blob.core.windows.net/docs/%(y)s/BT/SWRL/%(e)s/%(r)s/BT_C77%(L)s_1.0.pdf\"\n",
    "               %{\"e\": event, \"r\":race, \"y\":year, \"L\":L})\n",
    "        \n",
    "    urllib.urlretrieve(url,\"biathlon.pdf\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "flatten_row : Takes a row from the tabula-py reading of a biathlon pdf file and returns it as\n",
    "              a list of values\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe produced by a tabula-py reading of a biathlon pdf file\n",
    "index : the index of the row to be flattened\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "flattened_row : the flattened version of the row\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def flatten_row(df, index):\n",
    "    \n",
    "    row = df.iloc[index,:].tolist()\n",
    "    \n",
    "    flat_row = []\n",
    "\n",
    "    for i in range(len(row)):\n",
    "        flat_row.append(unicode(row[i]).split())\n",
    "\n",
    "    flat_row = [item for sublist in flat_row for item in sublist]\n",
    "    \n",
    "    flattened = []\n",
    "\n",
    "    for i in range(len(flat_row)): \n",
    "        if len(unicode(flat_row[i]).split('+')) > 1:\n",
    "            dumb = unicode(flat_row[i]).split('+')\n",
    "            dumber = []\n",
    "            dumber.append(dumb[0])\n",
    "            for i in range(1, len(dumb)):\n",
    "                dumber.append(\"+\".join([\"\", dumb[i]]))\n",
    "            flattened.append(dumber)\n",
    "        else:\n",
    "            flattened.append(flat_row[i])\n",
    "        \n",
    "    flattened_row = []\n",
    "\n",
    "    for item in flattened:\n",
    "        if type(item) is list:\n",
    "            for i in range(len(item)):\n",
    "                flattened_row.append(item[i])\n",
    "        else:\n",
    "            flattened_row.append(item)\n",
    "\n",
    "    flattened_row = [item for item in flattened_row if item != '' and item != 'X']\n",
    "        \n",
    "    return flattened_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "competition_analysis_splitSP : takes a the dataframe produced by reading a single page \n",
    "                               of a competition analysis pdf using tabula-py, along with\n",
    "                               the year and event in which the race was held and, for each\n",
    "                               racer, extracts name, shooting times, missed shots, course\n",
    "                               time, range time, and, where relevant, penalty time. It then \n",
    "                               returns them as a dataframe.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "race: the data from a single page of a pdf file containing a competition analysis page\n",
    "      for an individual race\n",
    "year : the season code for the race\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : a dataframe containing a subset of the data from the file\n",
    "continuer : a boolean indicating whether or not the next page of the pdf file should be\n",
    "            read or not (assuming that it exists)\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def competition_analysis_splitSP(race, year):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    continuer = True\n",
    "    \n",
    "    # First drop any all NaN columns\n",
    "    \n",
    "    race.dropna(axis = 1, how = \"all\", inplace = True)\n",
    "    \n",
    "    # And then replace any remaining NaNs with X\n",
    "    race.fillna(\"X\", inplace = True)\n",
    "    \n",
    "    # Step 0 is to get rid of any bad data (non starters, non finishers, disqualifications)\n",
    "    # at the end of the page\n",
    "    \n",
    "    data_end = -1\n",
    "    for i in range(len(race)):\n",
    "        if \"Jury\" in unicode(race.iloc[i,0]):\n",
    "            data_end = i\n",
    "            continuer = False\n",
    "            break\n",
    "        elif \"Did\" in unicode(race.iloc[i,0]):\n",
    "            data_end = i\n",
    "            continuer = False\n",
    "            break\n",
    "        elif \"Disqualification\" in unicode(race.iloc[i,0]):\n",
    "            data_end = i\n",
    "            continuer = False\n",
    "            break\n",
    "        elif \"Lapped\" in unicode(race.iloc[i,0]):\n",
    "            data_end = i\n",
    "            continuer = False\n",
    "            break\n",
    "    \n",
    "    if data_end != -1:\n",
    "        race.drop(race.index[data_end:], inplace = True)\n",
    "    \n",
    "    # Step 1 is to find the data. Because of the way that the data on the page is \n",
    "    # structured, one can use the locations of the rows containing the expression \n",
    "    # \"Cumulative Time\" as a means of locating all the needed rows.\n",
    "    \n",
    "    rows = []\n",
    "    for i in range(len(race)):\n",
    "        if \"Cumulative\" in race.iloc[i,0]:\n",
    "            rows.append(i)\n",
    "\n",
    "    # Now, I know that the names are in the rows just above the rows that have \"Cumulative \n",
    "    # Time\" in them\n",
    "    names = []\n",
    "    for row in rows:\n",
    "            \n",
    "        flat_name_row = flatten_row(race, row-1)\n",
    "        names.append(\" \".join(flat_name_row[2:len(flat_name_row)-5]))\n",
    "        \n",
    "    df = pd.DataFrame(names, columns = ['Name'])\n",
    "    \n",
    "    # Also, I know that the shooting information is three rows after the rows with \"Cumulative\n",
    "    # Time\" I want to collect both the time spent shooting and the number of misses from this\n",
    "    # row and create two arrays from it\n",
    "    \n",
    "    shooting_times = []\n",
    "    shooting_misses = []\n",
    "    for row in rows:\n",
    "                \n",
    "        flat_shooting_row = flatten_row(race, row+2)\n",
    "        \n",
    "        shot_row = [flat_shooting_row[1], flat_shooting_row[5], flat_shooting_row[9]]\n",
    "        shooting_misses.append(shot_row)\n",
    "        shot_row = [flat_shooting_row[2], flat_shooting_row[6], flat_shooting_row[10]]\n",
    "        shooting_times.append(shot_row)\n",
    "        \n",
    "    shooting_times = pd.DataFrame(shooting_times, columns = ['sh1','sh2','tot sh'])\n",
    "    shooting_misses = pd.DataFrame(shooting_misses, columns = ['P1','S1','T'])\n",
    "    \n",
    "    df = pd.concat([df, shooting_times, shooting_misses], axis = 1)\n",
    "        \n",
    "    # Next, I want to get the course time for each loop, which is in the 5th row after the\n",
    "    # \"Cumulative Time\" rows\n",
    "    \n",
    "    loop_times = []\n",
    "    for row in rows:\n",
    "        flat_loop_row = flatten_row(race, row + 4)\n",
    "        loop_time = [flat_loop_row[2], flat_loop_row[5], flat_loop_row[8], flat_loop_row[11]]\n",
    "        loop_times.append(loop_time)\n",
    "        \n",
    "    loop_times = pd.DataFrame(loop_times, columns = [\"L1\",\"L2\",'L3','Total Ski'])\n",
    "    \n",
    "    df = pd.concat([df, loop_times], axis = 1)\n",
    "    \n",
    "    # For the sake of completeness, I want to get the range times\n",
    "    \n",
    "    range_times = []\n",
    "    for row in rows:\n",
    "        flat_range_row = flatten_row(race, row + 3)\n",
    "        range_time = [flat_range_row[2], flat_range_row[5], flat_range_row[8]]\n",
    "        range_times.append(range_time)\n",
    "        \n",
    "    range_times = pd.DataFrame(range_times, columns = ['r1','r2','tot r'])\n",
    "    \n",
    "    df = pd.concat([df, range_times], axis = 1)\n",
    "    \n",
    "    \n",
    "    if year in [\"1112\",\"1213\",\"1314\",\"1415\",\"1516\",\"1617\",'1718']:\n",
    "        penalty_times = []\n",
    "        for row in rows:\n",
    "            flat_penalty_row = flatten_row(race, row + 5)\n",
    "            penalty_time = [flat_penalty_row[2], flat_penalty_row[3], flat_penalty_row[4]]\n",
    "            penalty_times.append(penalty_time)\n",
    "        \n",
    "        penalty_times = pd.DataFrame(penalty_times, columns = ['pen1', 'pen2', 'tot pen'])\n",
    "    \n",
    "        df = pd.concat([df, penalty_times], axis = 1)\n",
    "\n",
    "    if year in [\"1112\",\"1213\",\"1314\",\"1415\",\"1516\",\"1617\",'1718']:\n",
    "        cols = ['Name', 'sh1','sh2', 'P1','S1', \"L1\",\"L2\",'L3','r1','r2', 'pen1', 'pen2', \n",
    "                'tot sh', 'T','Total Ski','tot r','tot pen']    \n",
    "    else:\n",
    "         cols = ['Name', 'sh1','sh2', 'P1','S1', \"L1\",\"L2\",'L3', 'r1','r2', 'tot sh', 'T', \n",
    "                 'Total Ski','tot r']\n",
    "    df = df.reindex_axis([cols], axis=1)\n",
    "        \n",
    "    return df, continuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "competition_analysis_mergeSP : Uses tabula-py to read in each page of a biathlon.pdf file,\n",
    "                               sends the resulting page to competition_analysis_splitSP, \n",
    "                               and then concatenates the resulting dataframes into a single\n",
    "                               large dataframe.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "file_name: a competition analysis file for a biathlon race\n",
    "year: the four digit code for a biathlon season\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "results: a dataframe which contains the combined data from all of the pages of the pdf file\n",
    "\n",
    "Note\n",
    "----\n",
    "\n",
    "There are two sets of areas given for many of the years here. This is due to the fact that, \n",
    "although in theory the layout of the pdf pages should be consistant across any given season,\n",
    "in practice this is not true, as the individual host sites seem to produce their own pdf \n",
    "files for each race. As a result, there are a handful of races for which it will be necessary\n",
    "to run a competition_analysis_mergeSP a second time with a slightly different choice of areas.\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def competition_analysis_mergeSP(file_name, year):\n",
    "    \n",
    "        # each season has a different part of the page to find the data\n",
    "    areas = {\"0405\": [(204, 37, 706, 559), (204, 37, 706, 559)], \n",
    "         # \"0405\": [(145, 19, 777, 575), (145, 19, 777, 575)], # special cases\n",
    "         \"0506\": [(202, 37, 701, 559), (202, 37, 701, 559)], \n",
    "         # \"0506\": [(159, 20, 737, 575),(159, 20, 737, 575)], # special cases\n",
    "         \"0607\": [(203, 37, 716, 559), (203, 37, 716, 559)],\n",
    "         \"0708\": [(192, 37, 692, 559), (192, 37, 692, 559)],\n",
    "         \"0809\": [(192, 37, 692, 559), (192, 37, 692, 559)], \n",
    "         #\"0809\": [(140, 28, 722, 584), (140, 28, 722, 584)],# special cases\n",
    "         \"0910\": [(192, 37, 692, 559), (192, 37, 692, 559)], \n",
    "         #\"0910\": [(140, 28, 722, 584), (140, 28, 722, 584)],# special cases\n",
    "         \"1011\": [(162, 27, 727, 572), (18, 26, 814, 568)],\n",
    "         \"1112\": [(190, 27, 686, 568), (20, 27, 815, 586)],\n",
    "         \"1213\": [(194, 26, 691, 571), (20, 26, 667, 571)], \n",
    "        # \"1213\": [(194, 26, 691, 571), (20, 26, 744, 571)], # special cases   \n",
    "        # \"1213\": [(194, 26, 691, 571), (194, 26, 691, 571)],# special cases\n",
    "         \"1314\": [(193, 26, 689, 571), (20, 26, 662, 571)], \n",
    "         # special cases\"1314\": [(136, 20, 717, 576),(136, 20, 717, 576)],# special cases\n",
    "         \"1415\": [(137, 26, 704, 568), (20, 26, 662, 568)],\n",
    "         \"1516\": [(137, 26, 705, 568), (20, 26, 663, 568)],\n",
    "         #\"1617\": [(151, 20, 702, 574), (20, 20, 728, 574)], # special cases\n",
    "         \"1617\": [(151, 20, 727, 576), (20, 20, 759, 576)], \n",
    "            \"1718\": [(150, 19, 748, 576), (20, 20, 759, 576)]}\n",
    "\n",
    "    # Figure out how many pages are in the document\n",
    "    \n",
    "    with open(file_name,'r') as f:\n",
    "        doc = PdfFileReader(f)\n",
    "        pages = doc.getNumPages()\n",
    "        \n",
    "    # Get the data from the first page of the pdf\n",
    "        \n",
    "    race = read_pdf(file_name, pages = 1, area = areas[year][0], guess = False,\n",
    "                    multiple_tables = False)\n",
    "        \n",
    "    try:\n",
    "        data = competition_analysis_splitSP(race, year)\n",
    "        results = data[0]\n",
    "        to_continue = data[1]\n",
    "    except:\n",
    "        if year in [\"1112\",\"1213\",\"1314\",\"1415\",\"1516\",\"1617\",'1718']:\n",
    "            results = pd.DataFrame(columns = ['Name', 'sh1','sh2', 'P1','S1', \"L1\",\"L2\",'L3',\n",
    "                                              'r1','r2','pen1', 'pen2', 'tot sh', 'T',\n",
    "                                              'Total Ski','tot r','tot pen'])\n",
    "        else:\n",
    "            results = pd.DataFrame(columns = ['Name', 'sh1','sh2', 'P1','S1', \"L1\",\"L2\",'L3',\n",
    "                                              'r1','r2', 'tot sh', 'T', 'Total Ski','tot r'])\n",
    "        to_continue = True\n",
    "    \n",
    "    # Get the data from the remaining pages of the pdf \n",
    "    \n",
    "    for i in range(1,pages):\n",
    "        \n",
    "        if to_continue is True:\n",
    "        \n",
    "            race = read_pdf(file_name, pages = i + 1, area = areas[year][1], guess = False,\n",
    "                            multiple_tables = False)\n",
    "            try:\n",
    "                race == None\n",
    "            except:\n",
    "                try:\n",
    "                    data = competition_analysis_splitSP(race, year)\n",
    "                    result = data[0]\n",
    "                    to_continue = data[1]\n",
    "                    results = pd.concat([results, result], axis = 0)\n",
    "                except:\n",
    "                    pass\n",
    "    results.reset_index(inplace = True)\n",
    "    results.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the layout of the competition analysis pdf changes from season to season, and so the areas that are being read also need to change from season to season. Furthermore, due to minor changes in the layout of some of the competition analysis files, even within a single season, the code for competition_analysis_mergeSP contains commented out area values that can be used in cases where a single race is unable to be read correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "pickle_competition_analysisSP : takes the url of a biathlon data web page, determines whether\n",
    "                                the page is actually a pdf file or not, and it if it, extracts\n",
    "                                the relevant data, puts it into a data frame, and then pickles\n",
    "                                the resulting data.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SWMS,\n",
    "       SMIN, SMSP, SMPU, SMMS\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "stores a pickled dataframe object on the hard drive\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def pickle_competition_analysisSP(year, event, racecode):\n",
    "    \n",
    "    # get the biathlon pdf\n",
    "    \n",
    "    filename = \"companal_%(race)s_%(year)s_%(event)s.pkl\" %{\"race\": racecode, \"year\": year,\n",
    "                                                            \"event\": event}\n",
    "    get_analysis_file(year,event,racecode) #stored in biathlon.pdf\n",
    "    \n",
    "    # check whether you have a valid biathlon data file\n",
    "    with open(\"biathlon.pdf\",\"r\") as source:\n",
    "        begin = source.read(100)\n",
    "        if \"BlobNotFound\" not in begin:\n",
    "            data = competition_analysis_mergeSP(\"biathlon.pdf\",year)\n",
    "            data.to_pickle(filename)\n",
    "            \n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "pickle_biathlon_seasonSP : loops through all sprint races in a given biathlon season, applies \n",
    "                            pickle_competition_analysisSP to each, and pickles the resulting\n",
    "                            dataframes\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "season : the code of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "stores a pickled data frame object on the hard drive for each sprint event in the season.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def pickle_biathlon_seasonSP(season):\n",
    "    \n",
    "    failed_races = []\n",
    "\n",
    "    races = [\"SMSP\"]\n",
    "    events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "\n",
    "    for race in races:\n",
    "        for event in events:\n",
    "            try:\n",
    "                pickle_competition_analysisSP(season, event, race)\n",
    "            except (ValueError, AttributeError, IndexError, AssertionError, TypeError):\n",
    "                failed_races.append([season, event, race])\n",
    "    \n",
    "    return failed_races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then ran ```pickle_biathlon_seasonSP``` over each season from 2004-2005 to 2017-2018. Prior to the 2004-2005 season, I had access to performance data for the biathletes, but not to data on the race and weather conditions which I suspected might be impacting speeds and shooting accuracies. \n",
    "\n",
    "The code looked something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: PdfFileReader stream/file object is not in binary mode. It may not be read correctly. [pdf.py:1079]\n",
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0506\n",
      "0607\n",
      "0708\n",
      "0809\n",
      "0910\n",
      "1011\n",
      "1112\n",
      "1213\n",
      "1314\n",
      "1415\n",
      "1516\n",
      "1617\n",
      "1718\n"
     ]
    }
   ],
   "source": [
    "# This has beginning and end prints for each season because, at least on my elderly mac, \n",
    "# it takes a couple of hours and it's nice to have some sense of where it is\n",
    "\n",
    "failed_races = []\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    print season\n",
    "    failures = pickle_biathlon_seasonSP(season)\n",
    "    failed_races.append(failures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extending the list of failed races by a list of races for which the resulting dataframe\n",
    "# was much shorter than expected\n",
    "\n",
    "failed_races1 = [item for sublist in failed_races for item in sublist]\n",
    "failed_races1.extend([['0405','CP07','SMSP'],['0809', 'CP07', 'SMSP'],['0809','CH__','SMSP'],\n",
    "                      ['0910','OG__','SMSP'],['1213','CP02','SMSP']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "competition_analysis_mergeSP\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "file_name: a competition analysis file for a biathlon race\n",
    "year: the four digit code for a biathlon season\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "results: a dataframe which contains the combined data from all of the pages of the pdf file\n",
    "\n",
    "Note\n",
    "----\n",
    "\n",
    "Here I am using the alternative areas in order to obtain the sprint data for those races for\n",
    "which the layout of the pdf file was somewhat different than the standard layout for the year\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def competition_analysis_mergeSP(file_name, year):\n",
    "    \n",
    "        # each season has a different part of the page to find the data\n",
    "    areas = {#\"0405\": [(204, 37, 706, 559), (204, 37, 706, 559)], \n",
    "         \"0405\": [(145, 19, 777, 575), (145, 19, 777, 575)], # special cases\n",
    "         #\"0506\": [(202, 37, 701, 559), (202, 37, 701, 559)], \n",
    "         \"0506\": [(159, 20, 737, 575),(159, 20, 737, 575)], # special cases\n",
    "         \"0607\": [(203, 37, 716, 559), (203, 37, 716, 559)],\n",
    "         \"0708\": [(192, 37, 692, 559), (192, 37, 692, 559)],\n",
    "         #\"0809\": [(192, 37, 692, 559), (192, 37, 692, 559)], \n",
    "         \"0809\": [(140, 28, 722, 584), (140, 28, 722, 584)],# special cases\n",
    "         #\"0910\": [(192, 37, 692, 559), (192, 37, 692, 559)], \n",
    "         \"0910\": [(140, 28, 722, 584), (140, 28, 722, 584)],# special cases\n",
    "         \"1011\": [(162, 27, 727, 572), (18, 26, 814, 568)],\n",
    "         \"1112\": [(190, 27, 686, 568), (20, 27, 815, 586)],\n",
    "         #\"1213\": [(194, 26, 691, 571), (20, 26, 667, 571)], \n",
    "         \"1213\": [(194, 26, 691, 571), (20, 26, 744, 571)], # special cases   \n",
    "        # \"1213\": [(194, 26, 691, 571), (194, 26, 691, 571)],# special cases\n",
    "         #\"1314\": [(193, 26, 689, 571), (20, 26, 662, 571)], \n",
    "        \"1314\":[(143, 21, 727, 575),(143, 21, 727, 575)], # special cases\n",
    "         \"1415\": [(137, 26, 704, 568), (20, 26, 662, 568)],\n",
    "         \"1516\": [(137, 26, 705, 568), (20, 26, 663, 568)],\n",
    "         \"1617\": [(151, 20, 702, 574), (20, 20, 728, 574)], # special cases\n",
    "         #\"1617\": [(151, 20, 727, 576), (20, 20, 759, 576)] \n",
    "            \"1718\": [(150, 19, 748, 576), (20, 20, 759, 576)]}\n",
    "\n",
    "    # Figure out how many pages are in the document\n",
    "    \n",
    "    with open(file_name,'r') as f:\n",
    "        doc = PdfFileReader(f)\n",
    "        pages = doc.getNumPages()\n",
    "        \n",
    "    # Get the data from the first page of the pdf\n",
    "        \n",
    "    race = read_pdf(file_name, pages = 1, area = areas[year][0], guess = False,\n",
    "                    multiple_tables = False, encoding = 'utf8')\n",
    "        \n",
    "    try:\n",
    "        data = competition_analysis_splitSP(race, year)\n",
    "        results = data[0]\n",
    "        to_continue = data[1]\n",
    "    except:\n",
    "        if year in [\"1112\",\"1213\",\"1314\",\"1415\",\"1516\",\"1617\",'1718']:\n",
    "            results = pd.DataFrame(columns = ['Name', 'sh1','sh2', 'P1','S1', \"L1\",\"L2\",'L3',\n",
    "                                              'r1','r2', 'pen1', 'pen2', 'tot sh', 'T',\n",
    "                                              'Total Ski','tot r','tot pen'])\n",
    "        else:\n",
    "            results = pd.DataFrame(columns = ['Name', 'sh1','sh2', 'P1','S1', \"L1\",\"L2\",'L3', \n",
    "                                              'r1','r2', 'tot sh', 'T', 'Total Ski','tot r'])\n",
    "        to_continue = True\n",
    "    \n",
    "    # Get the data from the remaining pages of the pdf \n",
    "    \n",
    "    for i in range(1,pages):\n",
    "        \n",
    "        if to_continue is True:\n",
    "        \n",
    "            race = read_pdf(file_name, pages = i + 1, area = areas[year][1], guess = False,\n",
    "                            multiple_tables = False, encoding = 'utf8')\n",
    "            try:\n",
    "                race == None\n",
    "            except:\n",
    "                try:\n",
    "                    data = competition_analysis_splitSP(race, year)\n",
    "                    result = data[0]\n",
    "                    to_continue = data[1]\n",
    "                    results = pd.concat([results, result], axis = 0)\n",
    "                except:\n",
    "                    pass\n",
    "    results.reset_index(inplace = True)\n",
    "    results.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "still_failed = []\n",
    "for i in range(len(failed_races1)):\n",
    "    try:\n",
    "        pickle_competition_analysisSP(failed_races1[i][0], failed_races1[i][1],\n",
    "                                      failed_races1[i][2])\n",
    "    except:\n",
    "        still_failed.append(failed_races1[i])\n",
    "        \n",
    "still_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://ibu.blob.core.windows.net/docs/1314/BT/SWRL/OG__/SMSP/BT_C77B_1.1.pdf\"\n",
    "\n",
    "urllib.urlretrieve(url,\"biathlon.pdf\")\n",
    "\n",
    "results_sochi = competition_analysis_mergeSP('biathlon.pdf', '1314')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://ibu.blob.core.windows.net/docs/0506/BT/SWRL/OG__/SMSP/BT_C77B_1.1.pdf'\n",
    "\n",
    "urllib.urlretrieve(url, 'biathlon.pdf')\n",
    "\n",
    "results_torino = competition_analysis_mergeSP('biathlon.pdf', '0506')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_sochi.to_pickle('companal_SMSP_1314_OG__.pkl')\n",
    "results_torino.to_pickle('companal_SMSP_0506_OG__.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Collecting IBU Cup sprint data\n",
    "\n",
    "In fact, the [International Biathlon Union](https://biathlonworld.com)(IBU) runs two separate cup seasons, in addition to various youth and junior competitions. The first is the World Cup, which has the top hundred or so racers. The races above are the men's sprint competions from these races. There is another cup though, the IBU cup. Although not all biathletes in the IBU cup will ever make the transition to the World Cup circuit, most World Cup racers have spent at least some time competing in the IBU cup circuit. As a result, their results on the IBU cup circuit might be useful for informing predictions about their results on the World Cup circuit. To that end, we will also collect the mens sprint data for the IBU cup races from 2004-2005 until the present season. The functions that we use here are analogous to those used for collecting the World Cup data, and so I will list them without further description. \n",
    "\n",
    "1. ```find_comp_url```: This function, together with ```get_comp_file```, is analogous to ```get_analysis_file```\n",
    "2. ```get_comp_file```: This function, together with ```find_comp_url```, is analogous to ```get_analysis_file```\n",
    "3. ```ibu_competition_analysis_splitSP```: This function is analogus to ```competition_analysis_splitSP```\n",
    "4. ```ibu_competition_analysis_mergeSP```: This function is analogous to ```competition_analysis_mergeSP```\n",
    "5. ```ibu_pickle_competition_analysisSP```: This function is analogous to ```pickle_competition_analysisSP```\n",
    "6. ```ibu_pickle_biathlon_seasonSP```: This function is analogous to ```pickle_biathlon_seasonSP```\n",
    "\n",
    "The IBU Cup races presented a few problems that were not encountered with the World Cup races.\n",
    "1. Some of the IBU Cup events each year run not one but two men's sprint races. It is therefore necessary to ensure that the results of both races are taken into consideration.\n",
    "2. Some of the IBU Cup files have shooting times included. Some of the IBU Cup files do not have shooting times included. Unfortunately, this does not seem to be a season by season thing but rather a race by race thing, due perhaps to the equipment set up at a given course. That means that in getting the shot data, it will be necessary to somehow distinguish between these two cases on the fly, rather than as a season by season thing.\n",
    "3. Finally, there is a problem that is exclusive to the 2017-2018 season. The files for the 2017-2018 races have odd spacing for the IBU Cup races, which means that those racers who are slowest at the first shooting end up with an improperly parsed missed standing shots value. Here, the simplest fix seems to be to simply get rid of those rows for which this is an issue. To do this, we have the function\n",
    "    - ```dump_bad_rows```: This function takes a ibu competition analysis dataframe and checks the column ```'S1'``` for entries that cannot be a number of missed shots (that is to say, values that do not belong to the set {0, 1, 2, 3, 4, 5} and eliminates those rows from the dataframe.\n",
    "\n",
    "Previous Section: [Collecting the Race Data](#Collecting-the-race-data)\n",
    "\n",
    "\n",
    "Next Section: [Deleting Bad Racer Data](#Deleting-bad-racer-data)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "find_comp_url : Finds and return the url for the pdf of the competition analysis file \n",
    "                associated to a race in the ibu cup series\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the season in which the race took place, given as a four digit string of the form y1y2,\n",
    "        where y1 is the last two digits of the year in which the season started and y2 is the\n",
    "        last two digits of the year in which the season ended.\n",
    "event : a four character code indicating the event within a season with the possibilities CP01,\n",
    "        CP02, CP03, CP04, CP05, CP06, CP07, CP08 (regular events), and CH__ (championships)\n",
    "race : a four character code indicating the race within an event which has possible values\n",
    "        SMSP, SMPU, SMIN, SWSP, SWPU, SWIN\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "comp_url : the url at which the competition analysis pdf can be found\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# competition analysis urls\n",
    "\n",
    "def find_comp_url(year,event,race):\n",
    "\n",
    "    if (year in ['0405','0506','0607','0708','0809','0910','1011','1112']):\n",
    "        cup = \"SCEU\"\n",
    "    else:\n",
    "        if event in ['CH__']:\n",
    "            cup = 'SCEU'\n",
    "        else:\n",
    "            cup = \"SIBU\"\n",
    "        \n",
    "    comp_url = (\"https://ibu.blob.core.windows.net/docs/%(y)s/BT/%(c)s/%(e)s/%(r)s/BT_C77%(L)s_1.0.pdf\"\n",
    "                %{\"y\" : year, \"c\" : cup, \"e\" : event, \"r\" : race, \"L\" : \"B\"})\n",
    "    \n",
    "    return comp_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "get_comp_file : retrieves the competition analysis file from the web and stores it as\n",
    "                a pdf file at \"ibu_cup.pdf\"\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SMIN,\n",
    "        SMSP, SMPU\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "Stores a .pdf file at ibu_cup.pdf. \n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_comp_file(year,event,race):\n",
    "    url = find_comp_url(year, event, race)\n",
    "        \n",
    "    urllib.urlretrieve(url,\"ibu_cup.pdf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Competition Analysis Areas, ibu cup races. Note that the first entry for each season\n",
    "# is the data area on the first page of the pdf, while the second entry is the data area\n",
    "# for all subsequent pages\n",
    "\n",
    "competition_area = {'0405' : [(203, 37, 704, 560), (203, 37, 704, 560)] ,\n",
    "                    '0506' : [(203, 37, 704, 560), (203, 37, 704, 560)],\n",
    "                    '0607' : [(203, 37, 704, 560), (203, 37, 704, 560)],\n",
    "                    '0708' : [(197, 38, 736, 559), (197, 38, 736, 559)],\n",
    "                    '0809' : [(192, 38, 695, 558), (192, 38, 695, 558)],\n",
    "                    '0910' : [(192, 38, 695, 558), (192, 38, 695, 558)],\n",
    "                    '1011' : [(161, 26, 736, 569), (19, 26, 672, 569)],\n",
    "                    '1112' : [(191, 26, 688, 572), (20, 26, 815, 572)],\n",
    "                    '1213' : [(193, 26, 694, 572), (20, 26, 784, 572)],\n",
    "                    '1314' : [(194, 26, 696, 572), (20, 26, 716, 572)],\n",
    "                    '1415' : [(136, 26, 710, 572), (20, 26, 666, 572)],\n",
    "                    '1516' : [(136, 26, 695, 572), (20, 26, 662, 572)],\n",
    "                    '1617' : [(151, 20, 706, 576), (20, 20, 718, 576)],\n",
    "                    '1718' : [(151, 20, 706, 576), (20, 20, 718, 576)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_competition_analysis_splitSP : takes a data frame produced by using read_pdf from the \n",
    "                                   tabula package to read a single page of a read in \n",
    "                                   competition analysis file, cleans it up, and returns the\n",
    "                                   desired subset of the data in a format that is \n",
    "                                   straightforward to use.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "race : the data from a single page of a pdf file containing a competition analysis page\n",
    "year : the season code for the race\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : a dataframe containing a subset of the data from the file\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_competition_analysis_splitSP(race, year):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    continuer = True\n",
    "    \n",
    "    # First drop any all NaN columns\n",
    "    \n",
    "    race.dropna(axis = 1, how = \"all\", inplace = True)\n",
    "    \n",
    "    # And then replace any remaining NaNs with X\n",
    "    race.fillna(\"X\", inplace = True)\n",
    "    \n",
    "    # Step 0 is to get rid of any bad data (non starters, non finishers, disqualifications)\n",
    "    # at the end of the page\n",
    "    \n",
    "    data_end = -1\n",
    "    for i in range(len(race)):\n",
    "        if \"Jury\" in unicode(race.iloc[i,0]):\n",
    "            data_end = i\n",
    "            continuer = False\n",
    "            break\n",
    "        elif \"Did\" in unicode(race.iloc[i,0]):\n",
    "            data_end = i\n",
    "            continuer = False\n",
    "            break\n",
    "        elif \"Disqualification\" in unicode(race.iloc[i,0]):\n",
    "            data_end = i\n",
    "            continuer = False\n",
    "            break\n",
    "        elif \"Lapped\" in unicode(race.iloc[i,0]):\n",
    "            data_end = i\n",
    "            continuer = False\n",
    "            break\n",
    "    \n",
    "    if data_end != -1:\n",
    "        race.drop(race.index[data_end:], inplace = True)\n",
    "    \n",
    "    # Step 1 is to find the data. Because of the way that the data on the page is structured,\n",
    "    # one can use the locations of the rows containing the expression \"Cumulative Time\" as a\n",
    "    # means of locating all the needed rows.\n",
    "    \n",
    "    rows = []\n",
    "    for i in range(len(race)):\n",
    "        if \"Cumulative\" in race.iloc[i,0]:\n",
    "            rows.append(i)\n",
    "\n",
    "    cols = ['Name', 'P1','S1','T', 'sh1','sh2','tot sh', \"L1\",\"L2\",'L3','Total Ski',\n",
    "            'r1','r2','tot r']\n",
    "    cols_long = ['Name', 'P1','S1','T', 'sh1','sh2','tot sh', \"L1\",\"L2\",'L3','Total Ski',\n",
    "                    'r1','r2','tot r', 'pen1', 'pen2', 'tot pen' ]\n",
    "    if year in ['1415','1516','1617','1718']:\n",
    "        df = pd.DataFrame(columns = cols_long)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    for row in rows:\n",
    "        try:\n",
    "            racer = []\n",
    "            # Get the racers name\n",
    "            flat_name_row = flatten_row(race, row-1)\n",
    "            racer.append(\" \".join(flat_name_row[2:len(flat_name_row)-5]))\n",
    "            \n",
    "            # Get the shot info\n",
    "            flat_shooting_row = flatten_row(race, row+2)\n",
    "            if len(flat_shooting_row)<6:\n",
    "                shot_row = [flat_shooting_row[1],flat_shooting_row[2],flat_shooting_row[3]]\n",
    "                racer.extend(shot_row)\n",
    "                shot_row = ['X','X','X']\n",
    "                racer.extend(shot_row)\n",
    "            else:\n",
    "                shot_row = [flat_shooting_row[1], flat_shooting_row[5], flat_shooting_row[9]]\n",
    "                racer.extend(shot_row)\n",
    "                shot_row = [flat_shooting_row[2], flat_shooting_row[6], flat_shooting_row[10]]\n",
    "                racer.extend(shot_row)\n",
    "            \n",
    "            # Get the loop times\n",
    "            flat_loop_row = flatten_row(race, row + 4)\n",
    "            loop_time = [flat_loop_row[2], flat_loop_row[5], flat_loop_row[8], \n",
    "                                                                         flat_loop_row[11]]\n",
    "            racer.extend(loop_time)\n",
    "            \n",
    "            # Get the range times\n",
    "            flat_range_row = flatten_row(race, row + 3)\n",
    "            range_time = [flat_range_row[2], flat_range_row[5], flat_range_row[8]]\n",
    "            racer.extend(range_time)\n",
    "            \n",
    "            # If relevant, get the penalty times\n",
    "            if year in ['1415','1516','1617','1718']:\n",
    "                flat_penalty_row = flatten_row(race, row+5)\n",
    "                penalty_time = [flat_penalty_row[2], flat_penalty_row[3], flat_penalty_row[4]]\n",
    "                racer.extend(penalty_time)\n",
    "            \n",
    "            # Attach the column names\n",
    "            if year in ['1415','1516','1617','1718']:\n",
    "                racer = pd.DataFrame([racer], columns = cols_long)\n",
    "                df = pd.concat([df,racer])\n",
    "            else:\n",
    "                racer = pd.DataFrame([racer], columns = cols)\n",
    "                df = pd.concat([df, racer])\n",
    "        except:\n",
    "            pass\n",
    "    if year in ['1415','1516','1617','1718']:\n",
    "        new_cols = ['Name', 'sh1','sh2', 'P1','S1', \"L1\",\"L2\",'L3','r1','r2', 'pen1', 'pen2', \n",
    "                'tot sh', 'T','Total Ski','tot r','tot pen'] \n",
    "    else:\n",
    "        new_cols = ['Name', 'sh1','sh2', 'P1','S1', \"L1\",\"L2\",'L3',\n",
    "           'r1','r2', 'tot sh', 'T','Total Ski','tot r']\n",
    "    df = df.reindex_axis([new_cols], axis=1)\n",
    "    \n",
    "    return df, continuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_competition_analysis_mergeSP : takes a pdf file containing the competition analysis data\n",
    "                                    for a given race and returns the desired subset of the \n",
    "                                    data in the form of a single dataframe\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "file_name: a competition analysis pdf file from a biathlon race\n",
    "year : the four digit string indicating the season in which the competition was held\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "results: a dataframe containing the combined data from all of the pages of the pdf file\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_competition_analysis_mergeSP(file_name, year):\n",
    "    \n",
    "        # each season has a different part of the page to find the data\n",
    "    areas = {'0405' : [(203, 37, 704, 560), (203, 37, 704, 560)] ,\n",
    "            '0506' : [(203, 37, 704, 560), (203, 37, 704, 560)],\n",
    "            '0607' : [(203, 37, 704, 560), (203, 37, 704, 560)],\n",
    "            '0708' : [(197, 38, 736, 559), (197, 38, 736, 559)],\n",
    "            '0809' : [(192, 38, 695, 558), (192, 38, 695, 558)],\n",
    "            '0910' : [(192, 38, 695, 558), (192, 38, 695, 558)],\n",
    "            '1011' : [(161, 26, 736, 569), (19, 26, 784, 569)],\n",
    "            '1112' : [(191, 26, 694, 573), (20, 26, 815, 573)],\n",
    "            '1213' : [(193, 26, 694, 572), (20, 26, 784, 572)],\n",
    "            '1314' : [(194, 26, 696, 572), (20, 26, 716, 572)],\n",
    "            '1415' : [(136, 26, 710, 572), (20, 26, 666, 572)],\n",
    "            '1516' : [(136, 26, 695, 572), (20, 26, 662, 572)],\n",
    "            '1617' : [(151, 20, 706, 576), (20, 20, 718, 576)],\n",
    "            '1718' : [(151, 20, 706, 576), (20, 20, 718, 576)]}\n",
    "\n",
    "    # Figure out how many pages are in the document\n",
    "    \n",
    "    with open(file_name,'r') as f:\n",
    "        doc = PdfFileReader(f)\n",
    "        pages = doc.getNumPages()\n",
    "        \n",
    "    \n",
    "    # Get the data from the first page of the pdf\n",
    "    \n",
    "    \n",
    "    race = read_pdf(file_name, pages = 1, area = areas[year][0], guess = False, \n",
    "                    multiple_tables = False)\n",
    "    \n",
    "    data = ibu_competition_analysis_splitSP(race, year)\n",
    "    results = data[0]\n",
    "    to_continue = data[1]\n",
    "    \n",
    "    # Get the data from the remaining pages of the pdf \n",
    "    \n",
    "    for i in range(1,pages):\n",
    "        \n",
    "        if to_continue is True:\n",
    "    \n",
    "    \n",
    "            race = read_pdf(file_name, pages = i + 1, area = areas[year][1], guess = False, \n",
    "                            multiple_tables = False)\n",
    "        \n",
    "            try:\n",
    "                race == None\n",
    "            except:\n",
    "                data = ibu_competition_analysis_splitSP(race, year)\n",
    "                result = data[0]\n",
    "                to_continue = data[1]\n",
    "                results = pd.concat([results, result], axis = 0)\n",
    "                                \n",
    "    results.reset_index(inplace = True)\n",
    "    results.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_pickle_competition_analysisSP : takes the url of a biathlon data web page, determines \n",
    "                                    whether the page is actually a pdf file or not, and \n",
    "                                    in the case that it is, extracts the relevant data,\n",
    "                                    puts it into a data frame, and then pickles the resulting \n",
    "                                    data.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SMIN,\n",
    "        SMSP, SMPU\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "stores a pickled data frame object on the hard drive\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def ibu_pickle_competition_analysisSP(year, event, racecode):\n",
    "    \n",
    "    # get the biathlon pdf\n",
    "    \n",
    "    filename = (\"ibu_%(race)s_%(year)s_%(event)s.pkl\" \n",
    "                %{\"race\": racecode, \"year\": year, \"event\": event})\n",
    "    get_comp_file(year,event,racecode) #stored in ibu_cup.pdf\n",
    "    \n",
    "    \n",
    "    # check whether you have a valid biathlon data file\n",
    "    with open(\"ibu_cup.pdf\",\"r\") as source:\n",
    "        begin = source.read(100)\n",
    "        if \"BlobNotFound\" not in begin:\n",
    "             # run biathlon_data to extract the data\n",
    "            data = ibu_competition_analysis_mergeSP(\"ibu_cup.pdf\",year)\n",
    "            data.to_pickle(filename)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_pickle_biathlon_seasonSP : performs ibu_pickle_competition_analysisSP on every men's\n",
    "                                sprint race in a given biathlon season\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "season : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "failed_races : a list of races for which we were unable to pickle an object to save\n",
    "\n",
    "and, in addition, stores a pickled dataframe to the hard drive for every event that is \n",
    "successfully pickled\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_pickle_biathlon_seasonSP(season):\n",
    "    \n",
    "    failed_races = []\n",
    "\n",
    "    races = [\"SMSP\",\"SMSPS\"]\n",
    "    events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CH__']\n",
    "\n",
    "    for race in races:\n",
    "        for event in events:\n",
    "            try:\n",
    "                ibu_pickle_competition_analysisSP(season, event, race)\n",
    "            except:\n",
    "                failed_races.append([season, event, race])\n",
    "    \n",
    "    return failed_races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0405\n",
      "0506\n",
      "0607\n",
      "0708\n",
      "0809\n",
      "0910\n",
      "1011\n",
      "1112\n",
      "1213\n",
      "1314\n",
      "1415\n",
      "1516\n",
      "1617\n",
      "1718\n"
     ]
    }
   ],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415','1516','1617','1718']\n",
    "\n",
    "ibu_failures = []\n",
    "\n",
    "for season in seasons:\n",
    "    print season\n",
    "    ibu_failures.extend(ibu_pickle_biathlon_seasonSP(season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "dump_bad_rows : takes a ibu dataframe and eliminates those racer rows for which odd spacing\n",
    "                in the pdf file lead to improperly parsed data being read in\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SMIN,\n",
    "        SMSP, SMPU\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "Nothing, the dataframe is changed in place rather than by making copies\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def dump_bad_rows(year, event, race):\n",
    "    \n",
    "    filename = 'ibu_%(race)s_%(year)s_%(event)s.pkl' %{'year' : year,\n",
    "                                                       'event' : event, 'race' : race}\n",
    "    df = pd.read_pickle(filename)\n",
    "\n",
    "    bad_rows = []\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        if df.loc[i,'S1'] not in [0, 1, 2, 3, 4, 5, '0','1','2','3','4','5',\n",
    "                                  '0.0', '1.0', '2.0', '3.0', '4.0', '5.0']:\n",
    "            bad_rows.append(i)\n",
    "\n",
    "    df.drop(df.index[bad_rows], inplace=True)\n",
    "\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    if len(df) > 10:\n",
    "        df.to_pickle(filename)\n",
    "    else:\n",
    "        print 'check file for', year, event, race\n",
    "    \n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check file for 1415 CH__ SMSP\n"
     ]
    }
   ],
   "source": [
    "years = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "         '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CH__']\n",
    "races = ['SMSP','SMSPS']\n",
    "\n",
    "missing_files = []\n",
    "for year in years:\n",
    "    for event in events:\n",
    "        for race in races:\n",
    "            try:\n",
    "                dump_bad_rows(year, event, race)\n",
    "            except:\n",
    "                missing_files.append([year, event, race])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting bad racer data\n",
    "\n",
    "In biathlon competitions, each missed shot requires that the athlete ski a 150 meter penalty loop (which takes in the neighborhood of 25 to 30 seconds). If an athlete fails to ski some or all of his penalty loops, he is then assigned a time penalty of two minutes for each penalty loop that he fails to ski. These time penalties are assessed relatively infrequently (around twice a year on average), but have the potential to severely skew the data when they occur, particularly in those cases where an athlete missed all five shots, failed to ski any penalty laps, and was thus assessed a ten minute penalty. As a result, I have chosen to drop those racers who were assessed penalties of this type from the races in which the infractions occurred.\n",
    "\n",
    "(And, since reading the code for all of these deletions isn't very interesting, here's a [link](#Collecting-the-course-data) to jump to the next section.)\n",
    "\n",
    "Previous Section: [Collecting the IBU Cup Sprint Data](#Collecting-IBU-Cup-sprint-data)\n",
    "\n",
    "\n",
    "Next Section: [Collecting the Course Data](#Collecting-the-course-data)\n",
    "\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0405 CP02 ISA Hidenori\n",
    "race_data = pd.read_pickle('companal_SMSP_0405_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'ISA Hidenori'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0405_CP02.pkl')\n",
    "\n",
    "# 0405 CP04 KLETCHEROV Michail\n",
    "race_data = pd.read_pickle('companal_SMSP_0405_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'KLETCHEROV Michail'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0405_CP04.pkl')\n",
    "\n",
    "# 0405 CP09 VITEK Zdenek\n",
    "race_data = pd.read_pickle('companal_SMSP_0405_CP09.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'VITEK Zdenek'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0405_CP09.pkl')\n",
    "\n",
    "# 0506 CP01 MOYSEY Peter\n",
    "race_data = pd.read_pickle('companal_SMSP_0506_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'MOYSEY Peter'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0506_CP01.pkl')\n",
    "\n",
    "# 0506 CP02 OS Alexander, NOVIKOV Sergei\n",
    "race_data = pd.read_pickle('companal_SMSP_0506_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'OS Alexander'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'NOVIKOV Sergei'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0506_CP02.pkl')\n",
    "\n",
    "# 0708 CP08 SUMANN Christopher\n",
    "race_data = pd.read_pickle('companal_SMSP_0708_CP08.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'SUMANN Christopher'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0708_CP08.pkl')\n",
    "\n",
    "# 0708 CP09 YAROSHENKO Dmitri\n",
    "race_data = pd.read_pickle('companal_SMSP_0708_CP09.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'YAROSHENKO Dmitri'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0708_CP09.pkl')\n",
    "\n",
    "# 0809 CP04 TOBRELUTS Indrek\n",
    "race_data = pd.read_pickle('companal_SMSP_0809_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'TOBRELUTS Indrek'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0809_CP04.pkl')\n",
    "\n",
    "# 0910 CP02 LEGUELLEC Jean Philippe\n",
    "race_data = pd.read_pickle('companal_SMSP_0910_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'LEGUELLEC Jean Philippe'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0910_CP02.pkl')\n",
    "\n",
    "# 0910 CP03 KAUPPINEN Jarkko, KALDVEE Martten\n",
    "race_data = pd.read_pickle('companal_SMSP_0910_CP03.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'KAUPPINEN Jarkko'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'KALDVEE Martten'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0910_CP03.pkl')\n",
    "\n",
    "# 0910 CP05 KOSARAC Namanja\n",
    "race_data = pd.read_pickle('companal_SMSP_0910_CP05.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'KOSARAC Namanja'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_0910_CP05.pkl')\n",
    "\n",
    "# 1011 CH__ LOPATIC Stefan\n",
    "race_data = pd.read_pickle('companal_SMSP_1011_CH__.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'LOPATIC Stefan'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1011_CH__.pkl')\n",
    "\n",
    "# 1112 CP07 EBERHARD Tobias\n",
    "race_data = pd.read_pickle('companal_SMSP_1112_CP07.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'EBERHARD Tobias'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1112_CP07.pkl')\n",
    "\n",
    "# 1213 CP01 KLETCHEROV Martin\n",
    "race_data = pd.read_pickle('companal_SMSP_1213_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'KLETCHEROV Martin'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1213_CP01.pkl')\n",
    "\n",
    "# 1213 CP06 FOURCADE Simon, OTCENAS Martin\n",
    "race_data = pd.read_pickle('companal_SMSP_1213_CP06.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'FOURCADE Simon'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'OTCENAS Martin'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1213_CP06.pkl')\n",
    "\n",
    "# 1213 CP09 CHRISTIANSEN Vetle Sjastad\n",
    "race_data = pd.read_pickle('companal_SMSP_1213_CP09.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'CHRISTIANSEN Vetle Sjastad'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1213_CP09.pkl')\n",
    "\n",
    "# 1314 CP07 FILLON MAILLET Quentin\n",
    "race_data = pd.read_pickle('companal_SMSP_1314_CP07.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'FILLON MAILLET Quentin'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1314_CP07.pkl')\n",
    "\n",
    "# 1314 CP09 BOGETVEIT Haavard\n",
    "race_data = pd.read_pickle('companal_SMSP_1314_CP09.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'BOGETVEIT Haavard'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1314_CP09.pkl')\n",
    "\n",
    "# 1415 CP04 EBERHARD Julian\n",
    "race_data = pd.read_pickle('companal_SMSP_1415_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'EBERHARD Julian'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1415_CP04.pkl')\n",
    "\n",
    "# 1415 CH__ PODKORYTOV Vassiliy\n",
    "race_data = pd.read_pickle('companal_SMSP_1415_CH__.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'PODKORYTOV Vassiliy'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1415_CH__.pkl')\n",
    "\n",
    "# 1415 CP09 STEPHAN Christoph\n",
    "race_data = pd.read_pickle('companal_SMSP_1415_CP09.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'STEPHAN Christoph'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('companal_SMSP_1415_CP09.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similarly, I have assessed time penalty data for IBU cup races\n",
    "\n",
    "# 0506 CP02 EMONTS Ralph\n",
    "race_data = pd.read_pickle('ibu_SMSP_0506_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'EMONTS Ralph'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0506_CP02.pkl')\n",
    "\n",
    "# 0506 CP03 NAVARRO PEREZ Jose Ramon ,PRADES REVERTER Benjamin\n",
    "race_data = pd.read_pickle('ibu_SMSP_0506_CP03.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'NAVARRO PEREZ Jose Ramon'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'PRADES REVERTER Benjamin'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0506_CP03.pkl')\n",
    "\n",
    "# 0506 CP04 PERRAS Scott\n",
    "race_data = pd.read_pickle('ibu_SMSP_0506_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'PERRAS Scott'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0506_CP04.pkl')\n",
    "\n",
    "# 0506 CH__ DEBAYLE Yann, FOIDL Hans Peter\n",
    "race_data = pd.read_pickle('ibu_SMSP_0506_CH__.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'DEBAYLE Yann'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'FOIDL Hans Peter'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0506_CH__.pkl')\n",
    "\n",
    "# 0607 CP02 GJELLAND Egil, DAMJANOVSKI Darko\n",
    "race_data = pd.read_pickle('ibu_SMSP_0607_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'GJELLAND Egil'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'DAMJANOVSKI Darko'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0607_CP02.pkl')\n",
    "\n",
    "\n",
    "# 0607 CP03 FREI Thomas\n",
    "race_data = pd.read_pickle('ibu_SMSP_0607_CP03.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'FREI Thomas'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0607_CP03.pkl')\n",
    "\n",
    "\n",
    "# 0607 CP06(S) COOL Herbert\n",
    "race_data = pd.read_pickle('ibu_SMSPS_0607_CP06.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'COOL Herbert'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSPS_0607_CP06.pkl')\n",
    "\n",
    "\n",
    "# 0708 CP07 ICOSKI Gjorgji, STOJANOSKI Dejan\n",
    "race_data = pd.read_pickle('ibu_SMSP_0708_CP07.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'ICOSKI Gjorgji'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'STOJANOSKI Dejan'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0708_CP07.pkl')\n",
    "\n",
    "\n",
    "# 0809 CP01 DRIFFILL Craig\n",
    "race_data = pd.read_pickle('ibu_SMSP_0809_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'DRIFFILL Craig'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0809_CP01.pkl')\n",
    "\n",
    "\n",
    "# 0809 CP02 TADEJEVIC Zvonimir\n",
    "race_data = pd.read_pickle('ibu_SMSP_0809_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'TADEJEVIC Zvonimir'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0809_CP02.pkl')\n",
    "\n",
    "\n",
    "# 0809 CP07 HODZIC Admir\n",
    "race_data = pd.read_pickle('ibu_SMSP_0809_CP07.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'HODZIC Admir'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0809_CP07.pkl')\n",
    "\n",
    "\n",
    "# 0809 CH__ OTCENAS Martin\n",
    "race_data = pd.read_pickle('ibu_SMSP_0809_CH__.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'OTCENAS Martin'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0809_CH__.pkl')\n",
    "\n",
    "\n",
    "# 0910 CP02 HRKALOVIC Emir\n",
    "race_data = pd.read_pickle('ibu_SMSP_0910_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'HRKALOVIC Emir'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0910_CP02.pkl')\n",
    "\n",
    "\n",
    "# 0910 CP03 CEPULIS Darius, RASTIC Ajlan, ANGELIS Apostolos\n",
    "race_data = pd.read_pickle('ibu_SMSP_0910_CP03.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'CEPULIS Darius'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'RASTIC Ajlan'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'ANGELIS Apostolos'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0910_CP03.pkl')\n",
    "\n",
    "\n",
    "# 0910 CP05 RASTIC Demir\n",
    "race_data = pd.read_pickle('ibu_SMSP_0910_CP05.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'RASTIC Demir'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0910_CP05.pkl')\n",
    "\n",
    "\n",
    "# 0910 CP06 NAUDI BERMON Miquel\n",
    "race_data = pd.read_pickle('ibu_SMSP_0910_CP06.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'NAUDI BERMON Miquel'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_0910_CP06.pkl')\n",
    "\n",
    "\n",
    "# 1011 CP01(S) ICOSKI Gjorgji, GORBALD Paul\n",
    "race_data = pd.read_pickle('ibu_SMSPS_1011_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'ICOSKI Gjorgji'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'GORBALD Paul'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSPS_1011_CP01.pkl')\n",
    "\n",
    "\n",
    "# 1011 CP02 EHRHART Ludwig\n",
    "race_data = pd.read_pickle('ibu_SMSP_1011_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'EHRHART Ludwig'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1011_CP02.pkl')\n",
    "\n",
    "\n",
    "# 1011 CP03 BURIC Matej\n",
    "race_data = pd.read_pickle('ibu_SMSP_1011_CP03.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'BURIC Matej'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1011_CP03.pkl')\n",
    "\n",
    "\n",
    "# 1011 CP04 SLOOF Lucien\n",
    "race_data = pd.read_pickle('ibu_SMSP_1011_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'SLOOF Lucien'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1011_CP04.pkl')\n",
    "\n",
    "\n",
    "# 1011 CP07 SHIPULIN Anton, PALEVSKI Radi, PETROV Andrej\n",
    "race_data = pd.read_pickle('ibu_SMSP_1011_CP07.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'SHIPULIN Anton'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'PALEVSKI Radi'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'PETROV Andrej'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1011_CP07.pkl')\n",
    "\n",
    "\n",
    "# 1112 CP04 DAMJANOVSKI Darko\n",
    "race_data = pd.read_pickle('ibu_SMSP_1112_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'DAMJANOVSKI Darko'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1112_CP04.pkl')\n",
    "\n",
    "\n",
    "# 1112 CP08 KEIFER Alexander\n",
    "race_data = pd.read_pickle('ibu_SMSP_1112_CP08.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'KEIFER Alexander'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1112_CP08.pkl')\n",
    "\n",
    "\n",
    "# 1213 CP02 SLETTEMARK Oystein\n",
    "race_data = pd.read_pickle('ibu_SMSP_1213_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'SLETTEMARK Oystein'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1213_CP02.pkl')\n",
    "\n",
    "\n",
    "# 1213 CP05 ARNAULT Clement\n",
    "race_data = pd.read_pickle('ibu_SMSP_1213_CP05.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'ARNAULT Clement'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1213_CP05.pkl')\n",
    "\n",
    "\n",
    "# 1213 CP06 MAEDA Ryo\n",
    "race_data = pd.read_pickle('ibu_SMSP_1213_CP06.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'MAEDA Ryo'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1213_CP06.pkl')\n",
    "\n",
    "\n",
    "# 1314 CP01 STANOESKI Tosho\n",
    "race_data = pd.read_pickle('ibu_SMSP_1314_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'STANOESKI Tosho'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1314_CP01.pkl')\n",
    "\n",
    "\n",
    "# 1314 CP02 CRNKOVIC Kresimir, CHENG Fangming\n",
    "race_data = pd.read_pickle('ibu_SMSP_1314_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'CRNKOVIC Kresimir'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'CHENG Fangming'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1314_CP02.pkl')\n",
    "\n",
    "\n",
    "# 1314 CP04 CUENOT Gaspard, KUEHN Johannes\n",
    "race_data = pd.read_pickle('ibu_SMSP_1314_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'CUENOT Gaspard'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'KUEHN Johannes'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1314_CP04.pkl')\n",
    "\n",
    "\n",
    "# 1314 CP05 ORHAN Erkan\n",
    "race_data = pd.read_pickle('ibu_SMSP_1314_CP05.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'ORHAN Erkan'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1314_CP05.pkl')\n",
    "\n",
    "\n",
    "# 1415 CP01 LEE Su-Young, REITER Michael\n",
    "race_data = pd.read_pickle('ibu_SMSP_1415_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'LEE Su-Young'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'REITER Michael'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1415_CP01.pkl')\n",
    "\n",
    "\n",
    "# 1415 CP03 MARSANIC Mihael\n",
    "race_data = pd.read_pickle('ibu_SMSP_1415_CP03.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'MARSANIC Mihael'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1415_CP03.pkl')\n",
    "\n",
    "\n",
    "# 1516 CP01(S) VAHTRA Eno\n",
    "race_data = pd.read_pickle('ibu_SMSPS_1516_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'VAHTRA Eno'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSPS_1516_CP01.pkl')\n",
    "\n",
    "\n",
    "# 1516 CP02 GOMBOS Karoly\n",
    "race_data = pd.read_pickle('ibu_SMSP_1516_CP02.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'GOMBOS Karoly'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1516_CP02.pkl')\n",
    "\n",
    "\n",
    "# 1516 CP04 MILOSHEVSKI Velche\n",
    "race_data = pd.read_pickle('ibu_SMSP_1516_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'MILOSHEVSKI Velche'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1516_CP04.pkl')\n",
    "\n",
    "\n",
    "# 1516 CP04(S) PETROVIC Filip\n",
    "race_data = pd.read_pickle('ibu_SMSPS_1516_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'PETROVIC Filip'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSPS_1516_CP04.pkl')\n",
    "\n",
    "\n",
    "# 1516 CP07 TALIHAERM Johan, PETROVIC Filip\n",
    "race_data = pd.read_pickle('ibu_SMSP_1516_CP07.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'TALIHAERM Johan'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'PETROVIC Filip'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1516_CP07.pkl')\n",
    "\n",
    "\n",
    "# 1516 CH__ REITER Michael\n",
    "race_data = pd.read_pickle('ibu_SMSP_1516_CH__.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'REITER Michael'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1516_CH__.pkl')\n",
    "\n",
    "\n",
    "# 1516 CP08 JOVANOSKI Borce\n",
    "race_data = pd.read_pickle('ibu_SMSP_1516_CP08.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'JOVANOSKI Borce'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1516_CP08.pkl')\n",
    "\n",
    "\n",
    "# 1516 CP08(S) STANOESKI Tosho\n",
    "race_data = pd.read_pickle('ibu_SMSPS_1516_CP08.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'STANOESKI Tosho'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSPS_1516_CP08.pkl')\n",
    "\n",
    "\n",
    "# 1617 CP01 GOND Balazs\n",
    "race_data = pd.read_pickle('ibu_SMSP_1617_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'GOND Balazsh'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1617_CP01.pkl')\n",
    "\n",
    "\n",
    "# 1617 CP04 KRISTEJN Lukas, RAZQUIN MANGADO Alejandro\n",
    "race_data = pd.read_pickle('ibu_SMSP_1617_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'KRISTEJN Lukas'].index, inplace = True)\n",
    "race_data.drop(race_data[race_data.Name == u'RAZQUIN MANGADO Alejandro'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1617_CP04.pkl')\n",
    "\n",
    "\n",
    "# 1617 CP04(S) YEREMIN Roman\n",
    "race_data = pd.read_pickle('ibu_SMSPS_1617_CP04.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'YEREMIN Roman'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSPS_1617_CP04.pkl')\n",
    "\n",
    "\n",
    "# 1617 CP08 PADDER Rameez Ahmad\n",
    "race_data = pd.read_pickle('ibu_SMSP_1617_CP08.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'PADDER Rameez Ahmad'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1617_CP08.pkl')\n",
    "\n",
    "\n",
    "# 1718 CP01(S) FLANAGAN Jeremy\n",
    "race_data = pd.read_pickle('ibu_SMSPS_1718_CP01.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'FLANAGAN Jeremy'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSPS_1718_CP01.pkl')\n",
    "\n",
    "\n",
    "# 1718 CP08 TREIER Jan\n",
    "race_data = pd.read_pickle('ibu_SMSP_1718_CP08.pkl')\n",
    "race_data.drop(race_data[race_data.Name == u'TREIER Jan'].index, inplace = True)\n",
    "race_data.reset_index(inplace = True, drop = True)\n",
    "race_data.to_pickle('ibu_SMSP_1718_CP08.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Collecting the course data\n",
    "\n",
    "In order to consider the effects of conditions on race times and shooting accuracy, not to mention computing accurate speeds, it was necessary to collect a certain amount of data on the weather and course conditions. This data can be found at urls that look like:\n",
    "\n",
    "https://ibu.blob.core.windows.net/docs/0809/BT/SWRL/CH__/SMSP/BT_C82_1.1.pdf\n",
    "\n",
    "or more generally, like:\n",
    "\n",
    "https://ibu.blob.core.windows.net/docs/<font color = red>season</font>/BT/SWRL/<font color = red>event</font>/<font color = red>race</font>/BT_C82_1.0.pdf\n",
    "\n",
    "Due to the layout of these pages, it was simpler to collect all of the weather information over the course of a season and all of the course information over the course of a season separately, and then to pickle the resulting dataframes. As a result, we produce both a weather dataframe and a course dataframe for each season. We begin by collecting the course data.\n",
    "\n",
    "In order to do this, we use the following functions:\n",
    "1. ```get_summary_file```: This function takes a year, event, and race type, finds the url of the associated competition data summary, then downloads the file and stores it at summary.pdf.\n",
    "2. ```extract_distance```: This function takes in a string  containing a distance and strips any letters from the string. It then converts any distances given in kilometers into meters and returns the relevant distances in meters.\n",
    "3. ```extract_values```: This function takes a string containing a value and strips non numerical information from it.\n",
    "4. ```course_layout_split```: This function takes as input the contents of the cell containing the course layout, which can be of many forms. It calls ```extract_distance``` to remove extraneous information and to convert all lengths to meters and returns the lengths of the individual laps.\n",
    "5. ```get_course_info```: This function reads a pdf file containing the competition data summary for a race and returns the course information in the form of a dataframe containing a single line.\n",
    "6. ```collect_course_info```: This function calls ```get_course_info``` for each men's sprint race in a world cup season, concatenates the returned dataframes, and stores the resulting dataframe as a pickle file.\n",
    "\n",
    "I ran ```collect_course_info``` over all the seasons for which competition data summary files existed (from 2004-2005 on). Once again, some of the Olympic files posed a problem, once because of a non standard url (as above), and once because of a nonstandard pdf layout. In both cases, I added the necessary information to the relevent course file manually.\n",
    "\n",
    "Previous Section: [Deleting Bad Racer Data](#Deleting-bad-racer-data)\n",
    "\n",
    "\n",
    "Next Section: [Collecting the Weather Data](#Collecting-the-weather-data)\n",
    "\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "get_summary_file : downloads a biathlon results competition data summary file to the hard \n",
    "                    drive \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SMIN,\n",
    "        SMSP, SMPU\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "Stores a .pdf file at summary.pdf. \n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_summary_file(year,event,race):\n",
    "\n",
    "    url = (\"https://ibu.blob.core.windows.net/docs/%(y)s/BT/SWRL/%(e)s/%(r)s/BT_C82_1.0.pdf\" \n",
    "           %{\"e\": event, \"r\":race, \"y\":year})\n",
    "        \n",
    "    urllib.urlretrieve(url,\"summary.pdf\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function \n",
    "--------\n",
    "\n",
    "extract_distance : takes in a string of containing a distance, strips excess data, and returns \n",
    "                    the relevant distances in meters\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "string : a string which contains a distance, possibly in meters, possibly in kilometers, as \n",
    "        well as a possible unit or course description\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "dist : the distance, stripped of any other words or symbols and converted into meters if\n",
    "        necessary\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_distance(string):\n",
    "    \n",
    "    multiple = False\n",
    "    # First, split the string to allow extraction.\n",
    "\n",
    "    split_string = string.split()\n",
    "    \n",
    "    # Next, get rid of any pieces that are just color, unit, etc\n",
    "    \n",
    "    to_strip = ['yellow', 'green', 'blue', 'brown', 'red', 'course', 'km', 'm', 'Yellow',\n",
    "                'Green','Blue','Brown','Red','Course']\n",
    "    digits = ['0','1','2','3','4','5','6','7','8','9','0','.']\n",
    "    \n",
    "    to_keep = []\n",
    "    for k in split_string:\n",
    "        if k not in to_strip:\n",
    "            to_keep.append(k)\n",
    "            \n",
    "    to_keep = \"\".join(to_keep)\n",
    "    \n",
    "            \n",
    "    # For what is left\n",
    "    \n",
    "    if 'x' in to_keep:\n",
    "        # We have multiple notation\n",
    "        multiple = True\n",
    "        \n",
    "        to_keep = \"\".join(to_keep)\n",
    "        for i in range(len(to_keep)):\n",
    "            if to_keep[i] == 'x':\n",
    "                b = i\n",
    "                \n",
    "        to_keep = to_keep[b+1:]\n",
    "        \n",
    "        number = []\n",
    "        for j in range(len(to_keep)):\n",
    "            if to_keep[j] in digits:\n",
    "                number.append(to_keep[j])\n",
    "        dist = \"\".join(number) \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        number = []\n",
    "        for j in range(len(to_keep)):\n",
    "            if to_keep[j] in digits:\n",
    "                number.append(to_keep[j])\n",
    "\n",
    "        dist = \"\".join(number) \n",
    "        \n",
    "    if float(dist) < 100:\n",
    "        dist = 1000*float(dist)\n",
    "    else:\n",
    "        dist = float(dist)\n",
    "\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "extract_values : takes a string containing a value and strips non numerical information\n",
    "                from it, e.g., HD, MT, km, etc.\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "string : a string which contains a number and may contain other symbols \n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "value : the number that remains when extraneous data has been stripped.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def extract_values(string):\n",
    "    \n",
    "    split_string = string.split()\n",
    "    \n",
    "    to_strip = [\"(HD)\", \"HD\", \"(MM)\", \"MM\", \"(MT)\", \"MT\", \"(Length)\", \"Length\", \"km\",\n",
    "                \"m\", \"Height\", \"Difference:\", \"Max.\", \"Max\", \"Climb:\", \"Total\", \"Course\",\n",
    "                \"Length:\", 'nan']\n",
    "    \n",
    "    to_keep = []\n",
    "    for k in split_string:\n",
    "        if str(k) not in to_strip:\n",
    "            to_keep.append(str(k))\n",
    "            \n",
    "    to_keep = \"\".join(to_keep)\n",
    "    \n",
    "    number = []\n",
    "    for j in range(len(to_keep)):\n",
    "        try:\n",
    "            number.append(str(int(to_keep[j])))\n",
    "        except:\n",
    "            pass\n",
    "    value = \"\".join(number) \n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "course_layout_split : takes the contents of the cell containing the course layout and \n",
    "                    returns the individual lap length\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "string : a string containing the (possibly) concatenated loop lengths for a sprint race\n",
    "race : the code for the race (SWSP for a women's sprint, SMSP for a men's sprint)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "distance : a list containing the lengths of the three laps of the sprint race.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def course_layout_split(string, race):\n",
    "    \n",
    "    distance = []\n",
    "            \n",
    "    split = string.split(\"+\")\n",
    "    if len(split) == 3: # then each of the loop distances is in here separately\n",
    "        for i in range(3):\n",
    "            distance.append(extract_distance(split[i]))\n",
    "        \n",
    "    elif len(split) == 4 : # They're hopefully skiing the two middle laps together\n",
    "        distance.append(extract_distance(split[0]))\n",
    "        distance.append(extract_distance(split[1]) + extract_distance(split[2]))\n",
    "        distance.append(extract_distance(split[3]))\n",
    "        \n",
    "    else: # The same distance is skied for each lap\n",
    "        distance.extend([extract_distance(split[0]),extract_distance(split[0]),\n",
    "                         extract_distance(split[0])])\n",
    "        \n",
    "                    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "get_course_info : reads a pdf file containing the competition data summary for a race and \n",
    "                    returns the course information\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "filename : A file containing a pdf of competition data summary\n",
    "year : The season of the race (necessary to determine which portion of the page to read)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : The relevant course data in a single line dataframe\n",
    "\n",
    "Example\n",
    "-------\n",
    "\"\"\"\n",
    "\n",
    "def get_course_info(filename, year, event, race):\n",
    "    \n",
    "    summary_areas = {\"0405\": [(290, 37, 395, 557),(400, 20, 588, 557)],\n",
    "                \"0506\": [(290, 37, 395, 557),(400, 20, 588, 557)],\n",
    "                \"0607\": [(290, 37, 395, 557),(400, 20, 588, 557)],\n",
    "                \"0708\": [(273, 37, 380, 557),(400, 35, 560, 560)],\n",
    "                \"0809\": [(273, 37, 380, 557),(400, 20, 590, 557)],\n",
    "                \"0910\": [(273, 37, 380, 557),(273, 20, 590, 586)],\n",
    "                \"1011\": [(242, 26, 350, 568),(380, 26, 550, 568)],\n",
    "                \"1112\": [(270, 26, 375, 568),(400, 26, 590, 568)],\n",
    "                \"1213\": [(275, 26, 381, 568),(400, 26, 590, 568)],\n",
    "                \"1314\": [(275, 26, 381, 568),(275, 26, 600, 568)],\n",
    "                \"1415\": [(217, 26, 323, 568),(350, 26, 530, 568)],\n",
    "                \"1516\": [(221, 26, 326, 568),(300, 26, 580, 568)],\n",
    "                \"1617\": [(252, 19, 386, 577),(400, 19, 590, 577)],\n",
    "                \"1718\": [(252, 19, 386, 577),(400, 19, 590, 577)]}\n",
    "    \n",
    "    summary_areas_OG = {\"0506\" : [(250, 21, 355, 572), (434, 21, 515, 572)]}\n",
    "    \n",
    "\n",
    "    data = read_pdf(filename,area = summary_areas[year][1], guess = False, encoding = 'utf8')\n",
    "    \n",
    "    \n",
    "    cols = ['Year', 'Event', 'Race','Loop 1', 'Loop 2', 'Loop 3', \n",
    "            'Length', 'Height Diff', 'Max Climb', 'Total Climb']\n",
    "    \n",
    "    info = [year, event, race]\n",
    "    \n",
    "    # First, I want to find where the Course Information cell is, because my stuff is there\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if \"Course Information\" in unicode(data.iloc[i,0]):\n",
    "            b = i\n",
    "        \n",
    "    # The important information should be in rows b+1 to b+5\n",
    "    \n",
    "    # Get the loop information and add it to info\n",
    "    \n",
    "    distance = course_layout_split(data.iloc[b+1, 0], race)\n",
    "    info.extend(distance)\n",
    "    \n",
    "    # Get the other information and add it to the info\n",
    "    \n",
    "    for i in range(2,6):\n",
    "        stuff = data.iloc[b + i, :]\n",
    "        stuff = [str(item) for item in stuff]\n",
    "        string = \" \".join(stuff)\n",
    "        value = extract_values(string)\n",
    "        info.append(value)\n",
    "    \n",
    "    info = pd.DataFrame([info], columns = cols)\n",
    "    \n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "collect_course_info : collects the course information for all of the men's or women's \n",
    "                        sprint races for an entire world cup season\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : The code for the season under consideration\n",
    "gender : M or W\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "data : A data frame containing the data summary for all the men's or women's races in the \n",
    "        given season\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def collect_course_info(year, gender):\n",
    "    \n",
    "    if gender == 'M':\n",
    "        racecodes = ['SMSP']\n",
    "    else:\n",
    "        racecodes = ['SWSP']\n",
    "        \n",
    "    events = ['CP01', 'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__',\n",
    "              'OG__']\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Year', 'Event', 'Race','Loop 1', 'Loop 2', 'Loop 3', \n",
    "                                 'Length', 'Height Diff', 'Max Climb', 'Total Climb'])\n",
    "\n",
    "    for event in events:\n",
    "        for race in racecodes:\n",
    "            get_summary_file(year,event,race)\n",
    "            try:\n",
    "                data = get_course_info(\"summary.pdf\", year, event, race)\n",
    "                df = pd.concat([df, data])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    pickle = \"course_summary_%(year)s_%(gender)s.pkl\" %{\"year\": year, \"gender\": gender}\n",
    "            \n",
    "    df.reset_index(drop = True, inplace = True)    \n",
    "        \n",
    "    df.to_pickle(pickle)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    collect_course_info(season, 'M')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Problems with the Sochi  and PyeongChang Olympic Games\n",
    "\n",
    "Due to a nonstandardized choice of url for the competition summary files for the Sochi Olympics and then nonstandardized choice of layout for the PyeongChang Olympics, I judged that it would be simpler to simply add the relevant data for these events by hand rather than to try to write a function to put it in. The code for that is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the Sochi Olympics\n",
    "\n",
    "df = pd.read_pickle(\"course_summary_1314_M.pkl\")\n",
    "\n",
    "data = [['1314','OG__','SMSP',3393,3388,3494,10275,57,28,381]]\n",
    "\n",
    "df1 = pd.DataFrame(data,columns = ['Year', 'Event', 'Race','Loop 1', 'Loop 2', 'Loop 3', \n",
    "                                 'Length', 'Height Diff', 'Max Climb', 'Total Climb'])\n",
    "\n",
    "df = pd.concat([df, df1])\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df.to_pickle(\"course_summary_1314_M.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the PyeongChang Olympics\n",
    "\n",
    "df = pd.read_pickle(\"course_summary_1718_M.pkl\")\n",
    "\n",
    "data = [['1718','OG__','SMSP',3159,3424,3230,9813,37,37,348]]\n",
    "\n",
    "df1 = pd.DataFrame(data,columns = ['Year', 'Event', 'Race','Loop 1', 'Loop 2', 'Loop 3', \n",
    "                                 'Length', 'Height Diff', 'Max Climb', 'Total Climb'])\n",
    "\n",
    "df = pd.concat([df, df1])\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df.to_pickle(\"course_summary_1718_M.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Collecting the weather data\n",
    "\n",
    "The weather data for the races (temperature, humidity, wind, etc) can be found on the same pdf as the course data. In order to keep things managable, I collected the weather data separately, and stored it in a separate dataframe from the course data.\n",
    "\n",
    "The functions that were used for collecting the weather information are as follows:\n",
    "1. ```get_weather_info```: This function takes a pdf file containing a competition data summary and uses tabula-py to read in the portion containing data about weather conditions. It then collects information about weather, snow conditions, air temperature, snow temperature, humidity, and wind speed and returns that information in the form of a dataframe containing a single row.\n",
    "2. ```collect_weather```: This function calls ```get_weather_info``` for each men's sprint race in a world cup season, concatenates the returned dataframes, and stores the resulting dataframe as a pickle file.\n",
    "\n",
    "Once again, we run into problems with the Sochi (url) and PyeongChang (layout) Olympic Games. I again judged that the simplest solution was to enter the missing information manually.\n",
    "\n",
    "Previous Section: [Collecting the Course Data](#Collecting-the-course-data)\n",
    "\n",
    "\n",
    "Next Section: [Collecting IBU Cup course and weather data](#Collecting-IBU-Cup-course-and-weather-data)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "get_weather_info : takes a pdf file containing a competition data summary returns information \n",
    "                    about the weather and snow conditions during the given race\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "filename : A file containing a pdf of competition data summary\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SMIN,\n",
    "        SMSP, SMPU\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : The relevant weather data in a single line dataframe\n",
    "\n",
    "Example\n",
    "-------\n",
    "\"\"\"\n",
    "\n",
    "def get_weather_info(filename, year, event, race):\n",
    "    \n",
    "    summary_areas = {\"0405\": [(200, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"0506\": [(244, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"0607\": [(244, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"0708\": [(244, 20, 428, 576),(464, 37, 540, 557)],\n",
    "                \"0809\": [(244, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"0910\": [(244, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"1011\": [(200, 26, 400, 568),(442, 26, 520, 568)],\n",
    "                \"1112\": [(200, 26, 425, 568),(471, 26, 548, 568)],\n",
    "                \"1213\": [(200, 26, 425, 568),(475, 26, 552, 568)],\n",
    "                \"1314\": [(200, 26, 425, 568),(475, 26, 552, 568)],\n",
    "                \"1415\": [(180, 26, 400, 568),(417, 26, 493, 568)],\n",
    "                \"1516\": [(180, 26, 400, 568),(420, 26, 498, 568)],\n",
    "                \"1617\": [(200, 19, 450, 577),(488, 19, 576, 577)],\n",
    "                \"1718\": [(200, 19, 450, 577),(488, 19, 576, 577)]}\n",
    "    \n",
    "    data = read_pdf(filename,area = summary_areas[year][0], guess = False)\n",
    "            \n",
    "    cols = ['Year', 'Event', 'Race', 'Weather A', 'Weather B', 'Weather C', 'Weather D',\n",
    "            'Snow Cond A', 'Snow Cond B', 'Snow Cond C', 'Snow Cond D', 'Snow Temp A',\n",
    "            'Snow Temp B', 'Snow Temp C', 'Snow Temp D', 'Air Temp A', 'Air Temp B',\n",
    "            'Air Temp C','Air Temp D','Humidity A','Humidity B', 'Humidity C','Humidity D',\n",
    "            'Wind A', 'Wind B', 'Wind C', 'Wind D']\n",
    "    \n",
    "    # Find the upper left corner of the data, which is the word 'Weather'\n",
    "    \n",
    "    info = [year, event, race]\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if unicode(data.iloc[i,j]) == 'Weather':\n",
    "                row = i\n",
    "                column = j\n",
    "        \n",
    "    # Drop all extraneous rows\n",
    "    \n",
    "    data.drop(data.index[row+8:], inplace = True)\n",
    "        \n",
    "    data.drop(data.index[:row], inplace = True)\n",
    "        \n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # Drop any columns that are now entirely nan values\n",
    "    \n",
    "    data.dropna(axis = 1, how = \"all\", inplace = True)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data.iloc[i,0] == \"Weather\":\n",
    "            row1 = i\n",
    "        if data.iloc[i,0] == \"Humidity\":\n",
    "            row2 = i\n",
    "\n",
    "    difference = row2 - row1\n",
    "    \n",
    "    if difference > 4:\n",
    "        race = data.values.tolist()\n",
    "        race_1 = []\n",
    "        for i in range(len(race[1])):\n",
    "            race_1.append(\" \".join([unicode(race[1][i]),unicode(race[3][i])]))\n",
    "        new_race = [race[0], race_1, race[4],race[5], race[6], race[7]]\n",
    "        data = pd.DataFrame(new_race)\n",
    "    \n",
    "    # Fill the dataframe row\n",
    "    \n",
    "    if data.shape[1] > 5:\n",
    "    \n",
    "        data = data.values.tolist()\n",
    "    \n",
    "        cond = [item for item in data[0] if unicode(item) not in ['nan','NaN']]\n",
    "        for i in range(1,5):\n",
    "            info.append(cond[i])\n",
    "\n",
    "        cond = [item for item in data[1] if unicode(item) not in ['nan','NaN']]            \n",
    "        for i in range(1,5):\n",
    "            info.append(cond[i])\n",
    "        \n",
    "        cond = [item for item in data[2] if unicode(item) not in ['nan','NaN']]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "        \n",
    "        cond = [item for item in data[3] if unicode(item) not in ['nan','NaN']]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "        \n",
    "        cond = [item for item in data[4] if unicode(item) not in ['nan','NaN']]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "        \n",
    "        cond = [item for item in data[5] if unicode(item) not in ['nan','NaN']]\n",
    "        for i in range(1,5):\n",
    "            length = len(unicode(cond[i]).split())\n",
    "            info.append(unicode(cond[i]).split()[length-2])\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        data = data.values.tolist()\n",
    "        \n",
    "        cond = data[0]\n",
    "        for i in range(1,5):\n",
    "            info.append(cond[i])\n",
    "            \n",
    "        cond = data[1]\n",
    "        for i in range(1,5):\n",
    "            info.append(cond[i])\n",
    "            \n",
    "        cond = data[2]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "            \n",
    "        cond = data[3]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "            \n",
    "        cond = data[4]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "            \n",
    "        cond = data[5]\n",
    "        for i in range(1,5):\n",
    "            length = len(unicode(cond[i]).split())\n",
    "            info.append(unicode(cond[i]).split()[length-2])\n",
    "    # Make it a dataframe\n",
    "    \n",
    "    df = pd.DataFrame([info], columns = cols)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "collect_weather : passes through all of the sprint races for a given season and collects \n",
    "                    the weather data into a single data frame \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "gender : M or W\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "data : A data frame containing the data summary for all the men's or women's races in the \n",
    "        given season\n",
    "\n",
    "NB: columns coded 'A' give conditions 30 minutes before start time, columns coded 'B' \n",
    "    give conditions at start time, columns coded 'C' give conditions 30 minutes after\n",
    "    start time, and columns coded 'D' give conditions at the finish.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def collect_weather(year, gender):\n",
    "    \n",
    "    if gender == 'M':\n",
    "        racecodes = ['SMSP']\n",
    "    else:\n",
    "        racecodes = ['SWSP']\n",
    "        \n",
    "    events = ['CP01', 'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', \n",
    "              'CH__', 'OG__']\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Year', 'Event', 'Race', 'Weather A', 'Weather B', \n",
    "                                 'Weather C', 'Weather D', 'Snow Cond A', 'Snow Cond B',\n",
    "                                 'Snow Cond C', 'Snow Cond D', 'Snow Temp A', 'Snow Temp B',\n",
    "                                 'Snow Temp C', 'Snow Temp D', 'Air Temp A', 'Air Temp B',\n",
    "                                 'Air Temp C', 'Air Temp D', 'Humidity A', 'Humidity B',\n",
    "                                 'Humidity C', 'Humidity D', 'Wind A', 'Wind B', 'Wind C',\n",
    "                                 'Wind D'])\n",
    "    \n",
    "    for event in events:\n",
    "        for race in racecodes:\n",
    "            get_summary_file(year,event,race)\n",
    "            try:\n",
    "                data = get_weather_info(\"summary.pdf\", year, event, race)\n",
    "                df = pd.concat([df, data])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    pickle = \"weather_summary_%(year)s_%(gender)s.pkl\" %{\"year\": year, \"gender\": gender}\n",
    "            \n",
    "    df.reset_index(drop = True, inplace = True)   \n",
    "        \n",
    "    df.to_pickle(pickle)\n",
    "    \n",
    "    course_file = \"course_summary_%(year)s_%(gender)s.pkl\" %{\"year\": year, \"gender\": gender}\n",
    "    course = pd.read_pickle(course_file)\n",
    "    \n",
    "    if len(course) > len(df):\n",
    "        print \"You have\", len(course), \"course files and only\", len(df), \"\"\"weather files \\\n",
    "        for the\"\"\", gender, year, \"season.\"\n",
    "    elif len(df) > len(course):\n",
    "        print \"You have\", len(df), \"weather files and only\", len(course), \"\"\"course files \\\n",
    "        for the\"\"\", gender, year, \"season.\"\n",
    "    else:\n",
    "        print \"\"\"You have the same number of weather and course files for \\\n",
    "        the\"\"\", gender, year, \"season.\"\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have the same number of weather and course files for         the M 0405 season.\n",
      "You have the same number of weather and course files for         the M 0506 season.\n",
      "You have the same number of weather and course files for         the M 0607 season.\n",
      "You have the same number of weather and course files for         the M 0708 season.\n",
      "You have the same number of weather and course files for         the M 0809 season.\n",
      "You have the same number of weather and course files for         the M 0910 season.\n",
      "You have the same number of weather and course files for         the M 1011 season.\n",
      "You have the same number of weather and course files for         the M 1112 season.\n",
      "You have the same number of weather and course files for         the M 1213 season.\n",
      "You have 9 course files and only 8 weather files         for the M 1314 season.\n",
      "You have the same number of weather and course files for         the M 1415 season.\n",
      "You have the same number of weather and course files for         the M 1516 season.\n",
      "You have the same number of weather and course files for         the M 1617 season.\n",
      "You have the same number of weather and course files for         the M 1718 season.\n"
     ]
    }
   ],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415','1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    collect_weather(season, 'M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problems with the Olympic Games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"weather_summary_1314_M.pkl\")\n",
    "\n",
    "df = [['1314','OG__', 'SMSP', 'Sky Clear', 'Partly Cloudy', 'Partly Cloudy', 'Partly Cloudy',\n",
    "       'Packed', 'Packed', 'Packed','Packed', '-0.1', '-0.1', '-0.1', '-0.1', '1.9', '0.9',\n",
    "       '0.4', '0.3', '47', '54', '56', '54', '0.1', '0.0', '0.0', '0.0']]\n",
    "      \n",
    "df = pd.DataFrame(df, columns = ['Year', 'Event', 'Race', 'Weather A', 'Weather B', \n",
    "                               'Weather C', 'Weather D', 'Snow Cond A', 'Snow Cond B', \n",
    "                               'Snow Cond C', 'Snow Cond D', 'Snow Temp A', 'Snow Temp B',\n",
    "                               'Snow Temp C', 'Snow Temp D', 'Air Temp A', 'Air Temp B',\n",
    "                               'Air Temp C', 'Air Temp D', 'Humidity A', 'Humidity B', \n",
    "                               'Humidity C', 'Humidity D', 'Wind A', 'Wind B', 'Wind C',\n",
    "                               'Wind D'])\n",
    "\n",
    "data = pd.concat([data,df])\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "data.to_pickle('weather_summary_1314_M.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"weather_summary_1718_M.pkl\")\n",
    "\n",
    "df = [['1718', 'OG__', 'SMSP', u'Light snow', u'Mostly cloudy', u'Mostly cloudy', \n",
    "       u'Mostly cloudy', u'Compact', u'Compact', u'Compact', u'Compact', u'-12.9', u'-14.3',\n",
    "       u'-14.3', u'-13.8', u'-11.0', u'-10.7', u'-11.0', u'-11.3', u'68', u'63', u'64',\n",
    "       u'64', u'1.9', u'2.7', u'1.4', u'2.3']]\n",
    "      \n",
    "df = pd.DataFrame(df, columns = ['Year', 'Event', 'Race', 'Weather A', 'Weather B', \n",
    "                               'Weather C', 'Weather D', 'Snow Cond A', 'Snow Cond B', \n",
    "                               'Snow Cond C', 'Snow Cond D', 'Snow Temp A', 'Snow Temp B',\n",
    "                               'Snow Temp C', 'Snow Temp D', 'Air Temp A', 'Air Temp B',\n",
    "                               'Air Temp C', 'Air Temp D', 'Humidity A', 'Humidity B', \n",
    "                               'Humidity C', 'Humidity D', 'Wind A', 'Wind B', 'Wind C',\n",
    "                               'Wind D'])\n",
    "\n",
    "data = pd.concat([data,df])\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "data.to_pickle('weather_summary_1718_M.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Collecting IBU Cup course and weather data\n",
    "\n",
    "In order to collect course and weather data for IBU Cup races, I used essentially the same functions as I used to get the same data for the World Cup races.\n",
    "\n",
    "The functions that are used here are:\n",
    "1. ```ibu_find_course_url```: This function, together with ```ibu_get_summary_file``` is analogous to ```get_summary_file```.\n",
    "2. ```ibu_get_summary_file```: This function, together with ```ibu_find_course_url``` is analogous to ```get_summary_file```).\n",
    "3. ```ibu_extract_distance```: This function is analogous to ```extract_distance```.\n",
    "4. ```ibu_extract_values```: This function is analogous to ```extract_values```.\n",
    "5. ```ibu_course_layout_split```: This function is analogous to ```course_layout_split```.\n",
    "6. ```ibu_get_course_info```: This function is analogous to ```get_course_info```.\n",
    "7. ```ibu_collect_course_info```: This function is analogous to ```collect_course_info```.\n",
    "8. ```ibu_get_weather_info```: This function is analogous to ```get_weather_info```.\n",
    "9. ```ibu_get_weather_info```: This function is analogous to ```get_weather_info```.\n",
    "\n",
    "Previous Section: [Collecting the Weather Data](#Collecting-the-weather-data)\n",
    "\n",
    "Next Section: [Adjustments to course and weather data](#Adjustments-to-course-and-weather-data)\n",
    "\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_find_course_url : given the year, event, and race code of a particular competition, \n",
    "                        returns the url for the competition data summary\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SMIN,\n",
    "        SMSP, SMPU\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "course_url : a string containing the url of the competition data summary pdf\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_find_course_url(year, event, race):\n",
    "    \n",
    "    if year in ['0405','0506','0607','0708','0809','0910','1011','1112']:\n",
    "        cup = \"SCEU\"\n",
    "    elif event in ['CH__']:\n",
    "        cup = \"SCEU\"\n",
    "    else:\n",
    "        cup = \"SIBU\"\n",
    "    \n",
    "    course_url = (\n",
    "    \"https://ibu.blob.core.windows.net/docs/%(y)s/BT/%(c)s/%(e)s/%(r)s/BT_C82_1.0.pdf\"\n",
    "        %{\"y\" : year, \"c\" : cup, \"e\" : event, \"r\" : race})\n",
    "    \n",
    "    return course_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Course and weather areas\n",
    "\n",
    "ibu_course_area = {'0405' : [(268, 34, 453, 560), (453, 34, 594, 560)],\n",
    "               '0506' : [(355, 37, 451, 558), (451, 37, 700, 558)],\n",
    "               '0607' : [(266, 37, 431, 558), (431, 37, 586, 558)],\n",
    "               '0708' : [(266, 37, 439, 558), (439, 37, 593, 558)],\n",
    "               '0809' : [(243, 37, 428, 558), (428, 37, 571, 558)],\n",
    "               '0910' : [(241, 37, 419, 558), (419, 37, 589, 558)],\n",
    "               '1011' : [(210, 25, 416, 568), (416, 25, 567, 568)],\n",
    "               '1112' : [(232, 25, 431, 568), (431, 25, 573, 568)],\n",
    "               '1213' : [(239, 25, 438, 568), (438, 25, 576, 568)],\n",
    "               '1314' : [(249, 25, 440, 568), (440, 25, 579, 568)],\n",
    "               '1415' : [(182, 25, 382, 568), (382, 25, 529, 568)],\n",
    "               '1516' : [(183, 25, 383, 568), (383, 25, 530, 568)],\n",
    "               '1617' : [(210, 20, 440, 572), (440, 20, 609, 572)],\n",
    "               '1718' : [(210, 20, 440, 572), (440, 20, 609, 572)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_get_summary_file : retrieves a competition data summary pdf file from the web\n",
    "                        and stores it on the hard drive\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SMIN,\n",
    "        SMSP, SMPU\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "Stores a .pdf file at summary.pdf. \n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_get_summary_file(year,event,race):\n",
    "\n",
    "    url = ibu_find_course_url(year, event, race)\n",
    "    \n",
    "    \n",
    "    urllib.urlretrieve(url,\"summary.pdf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function \n",
    "--------\n",
    "\n",
    "ibu_extract_distance : takes in a string of containing a distance, strips excess data, and\n",
    "                        returns the relevant distances in meters\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "string : a string which contains a distance, possibly in meters, possibly in kilometers, as \n",
    "        well as a possible unit or course description\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "dist : the distance, stripped of any other words or symbols and converted into meters if\n",
    "        necessary\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_extract_distance(string):\n",
    "    \n",
    "    multiple = False\n",
    "    # First, split the string to allow extraction.\n",
    "\n",
    "    split_string = string.split()\n",
    "    \n",
    "    # Next, get rid of any pieces that are just color, unit, etc\n",
    "    \n",
    "    to_strip = ['yellow', 'green', 'blue', 'brown', 'red', 'course', 'km', 'm', 'Yellow',\n",
    "                'Green','Blue','Brown','Red', 'Course']\n",
    "    digits = ['0','1','2','3','4','5','6','7','8','9','0','.']\n",
    "    \n",
    "    to_keep = []\n",
    "    for k in split_string:\n",
    "        if k not in to_strip:\n",
    "            to_keep.append(k)\n",
    "            \n",
    "    to_keep = \"\".join(to_keep)\n",
    "    \n",
    "            \n",
    "    # For what is left\n",
    "    \n",
    "    if 'x' in to_keep:\n",
    "        # We have multiple notation\n",
    "        multiple = True\n",
    "        \n",
    "        to_keep = \"\".join(to_keep)\n",
    "        for i in range(len(to_keep)):\n",
    "            if to_keep[i] == 'x':\n",
    "                b = i\n",
    "                \n",
    "        to_keep = to_keep[b+1:]\n",
    "        \n",
    "        number = []\n",
    "        for j in range(len(to_keep)):\n",
    "            if to_keep[j] in digits:\n",
    "                number.append(to_keep[j])\n",
    "        dist = \"\".join(number) \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        number = []\n",
    "        for j in range(len(to_keep)):\n",
    "            if to_keep[j] in digits:\n",
    "                number.append(to_keep[j])\n",
    "\n",
    "        dist = \"\".join(number) \n",
    "        \n",
    "    if float(dist) < 100:\n",
    "        dist = 1000*float(dist)\n",
    "    else:\n",
    "        dist = float(dist)\n",
    "\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_extract_values : takes a string containing a value and strips non numerical information\n",
    "                from it, e.g., HD, MT, km, etc.\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "string : a string which contains a number and may contain other symbols \n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "value : the number that remains when extraneous data has been stripped.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_extract_values(string):\n",
    "    \n",
    "    split_string = string.split()\n",
    "    \n",
    "    to_strip = [\"(HD)\", \"HD\", \"(MM)\", \"MM\", \"(MT)\", \"MT\", \"(Length)\", \"Length\", \"km\", \"m\", \"Height\", \"Difference:\",\n",
    "               \"Max.\", \"Max\", \"Climb:\", \"Total\", \"Course\", \"Length:\", 'nan']\n",
    "    \n",
    "    to_keep = []\n",
    "    for k in split_string:\n",
    "        if str(k) not in to_strip:\n",
    "            to_keep.append(str(k))\n",
    "            \n",
    "    to_keep = \"\".join(to_keep)\n",
    "    \n",
    "    number = []\n",
    "    for j in range(len(to_keep)):\n",
    "        try:\n",
    "            number.append(str(int(to_keep[j])))\n",
    "        except:\n",
    "            pass\n",
    "    value = \"\".join(number) \n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_course_layout_split : takes the contents of the cell containing the course layout and \n",
    "                    returns the individual lap length\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "string : a string containing the (possibly) concatenated loop lengths for a sprint race\n",
    "race : the code for the race (SWSP for a women's sprint, SMSP for a men's sprint)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "distance : a list containing the lengths of the three laps of the sprint race.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_course_layout_split(string, race):\n",
    "    \n",
    "    distance = []\n",
    "    \n",
    "    if race in ['SWSP', 'SMSP','SWSPS','SMSPS']: # (race is a sprint)\n",
    "        \n",
    "        split = string.split(\"+\")\n",
    "        if len(split) == 3: # then each of the loop distances is in here separately\n",
    "            for i in range(3):\n",
    "                distance.append(ibu_extract_distance(split[i]))\n",
    "        elif len(split) == 4 : # They're hopefully skiing the two middle laps together\n",
    "            distance.append(ibu_extract_distance(split[0]))\n",
    "            distance.append(ibu_extract_distance(split[1]) + ibu_extract_distance(split[2]))\n",
    "            distance.append(ibu_extract_distance(split[3]))\n",
    "        else: # The same distance is skied for each lap\n",
    "            distance.extend([ibu_extract_distance(split[0]),ibu_extract_distance(split[0]),\n",
    "                             ibu_extract_distance(split[0])])\n",
    "        \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_get_course_info : reads a pdf file containing the competition data summary for a race and \n",
    "                    returns the course information\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "filename : A file containing a pdf of competition data summary\n",
    "year : The season of the race (necessary to determine which portion of the page to read)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : The relevant course data in a single line dataframe\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_get_course_info(filename, year, event, race):\n",
    "    \n",
    "    summary_areas = {\"0405\": [(290, 37, 395, 557),(400, 20, 588, 557)],\n",
    "                \"0506\": [(290, 37, 395, 557),(400, 20, 588, 557)],\n",
    "                \"0607\": [(290, 37, 395, 557),(400, 20, 588, 557)],\n",
    "                \"0708\": [(273, 37, 380, 557),(400, 35, 560, 560)],\n",
    "                \"0809\": [(273, 37, 380, 557),(400, 20, 590, 557)],\n",
    "                \"0910\": [(273, 37, 380, 557),(273, 20, 590, 586)],\n",
    "                \"1011\": [(242, 26, 350, 568),(380, 26, 550, 568)],\n",
    "                \"1112\": [(270, 26, 375, 568),(400, 26, 590, 568)],\n",
    "                \"1213\": [(275, 26, 381, 568),(400, 26, 590, 568)],\n",
    "                \"1314\": [(275, 26, 381, 568),(275, 26, 600, 568)],\n",
    "                \"1415\": [(217, 26, 323, 568),(350, 26, 530, 568)],\n",
    "                \"1516\": [(221, 26, 326, 568),(300, 26, 580, 568)],\n",
    "                \"1617\": [(252, 19, 386, 577),(400, 19, 590, 577)],\n",
    "                \"1718\": [(252, 19, 386, 577),(400, 19, 590, 577)]}\n",
    "        \n",
    "\n",
    "    data = read_pdf(filename,area = summary_areas[year][1], guess = False)\n",
    "        \n",
    "    cols = ['Year', 'Event', 'Race','Loop 1', 'Loop 2', 'Loop 3', \n",
    "            'Length', 'Height Diff', 'Max Climb', 'Total Climb']\n",
    "    \n",
    "    info = [year, event, race]\n",
    "    \n",
    "    # First, I want to find where the Course Information cell is, because my stuff is there\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if \"Course Information\" in unicode(data.iloc[i,0]):\n",
    "            b = i\n",
    "        \n",
    "    # The important information should be in rows b+1 to b+5\n",
    "    \n",
    "    # Get the loop information and add it to info\n",
    "    \n",
    "    distance = ibu_course_layout_split(data.iloc[b+1, 0], race)\n",
    "    info.extend(distance)\n",
    "    \n",
    "    # Get the other information and add it to the info\n",
    "    \n",
    "    for i in range(2,6):\n",
    "        stuff = data.iloc[b + i, :]\n",
    "        stuff = [str(item) for item in stuff]\n",
    "        string = \" \".join(stuff)\n",
    "        value = ibu_extract_values(string)\n",
    "        info.append(value)\n",
    "    \n",
    "    info = pd.DataFrame([info], columns = cols)\n",
    "    \n",
    "    return info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_collect_course_info : collects the course information for all of the men's or women's \n",
    "                        sprint races for an entire IBU world cup season\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : The code for the season under consideration\n",
    "gender : M or W\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "data : A data frame containing the data summary for all the men's or women's races in the \n",
    "        given season\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_collect_course_info(year, gender):\n",
    "    \n",
    "    racecodes = ['SMSP','SMSPS']\n",
    "        \n",
    "    events = ['CP01', 'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CH__', 'OG__']\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Year', 'Event', 'Race','Loop 1', 'Loop 2', 'Loop 3', \n",
    "                                 'Length', 'Height Diff', 'Max Climb', 'Total Climb'])\n",
    "\n",
    "    for event in events:\n",
    "        for race in racecodes:\n",
    "            ibu_get_summary_file(year,event,race)\n",
    "            try:\n",
    "                data = ibu_get_course_info(\"summary.pdf\", year, event, race)\n",
    "                df = pd.concat([df, data])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    pickle = \"ibu_course_summary_%(year)s_%(gender)s.pkl\" %{\"year\": year, \"gender\": gender}\n",
    "            \n",
    "    df.reset_index(drop = True, inplace = True)    \n",
    "        \n",
    "    df.to_pickle(pickle)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    ibu_collect_course_info(season, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_get_weather_info : takes a pdf file containing a competition data summary returns\n",
    "                    information about the weather and snow conditions during the given race\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "filename : A file containing a pdf of competition data summary\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "race : a four character code specifying the race. Possibilities are SWIN, SWSP, SWPU, SMIN,\n",
    "        SMSP, SMPU\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : The relevant weather data in a single line dataframe\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_get_weather_info(filename, year, event, race):\n",
    "    \n",
    "    summary_areas = {\"0405\": [(200, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"0506\": [(244, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"0607\": [(244, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"0708\": [(244, 20, 428, 576),(464, 37, 540, 557)],\n",
    "                \"0809\": [(244, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"0910\": [(244, 20, 428, 576),(474, 37, 554, 557)],\n",
    "                \"1011\": [(200, 26, 400, 568),(442, 26, 520, 568)],\n",
    "                \"1112\": [(200, 26, 425, 568),(471, 26, 548, 568)],\n",
    "                \"1213\": [(200, 26, 425, 568),(475, 26, 552, 568)],\n",
    "                \"1314\": [(200, 26, 425, 568),(475, 26, 552, 568)],\n",
    "                \"1415\": [(180, 26, 400, 568),(417, 26, 493, 568)],\n",
    "                \"1516\": [(180, 26, 400, 568),(420, 26, 498, 568)],\n",
    "                \"1617\": [(200, 19, 450, 577),(488, 19, 576, 577)],\n",
    "                \"1718\": [(200, 19, 450, 577),(488, 19, 576, 577)]}\n",
    "    \n",
    "    data = read_pdf(filename,area = summary_areas[year][0], guess = False)\n",
    "        \n",
    "    \n",
    "    cols = ['Year', 'Event', 'Race', 'Weather A', 'Weather B', 'Weather C', 'Weather D', \n",
    "            'Snow Cond A', 'Snow Cond B', 'Snow Cond C', 'Snow Cond D', 'Snow Temp A', \n",
    "            'Snow Temp B', 'Snow Temp C', 'Snow Temp D', 'Air Temp A', 'Air Temp B', \n",
    "            'Air Temp C', 'Air Temp D', 'Humidity A', 'Humidity B','Humidity C', \n",
    "            'Humidity D', 'Wind A', 'Wind B', 'Wind C', 'Wind D']\n",
    "    \n",
    "    # Find the upper left corner of the data, which is the word 'Weather'\n",
    "    \n",
    "    info = [year, event, race]\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if unicode(data.iloc[i,j]) == 'Weather':\n",
    "                row = i\n",
    "                column = j\n",
    "        \n",
    "    # Drop all extraneous rows\n",
    "    \n",
    "    data.drop(data.index[row+8:], inplace = True)\n",
    "        \n",
    "    data.drop(data.index[:row], inplace = True)\n",
    "        \n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # Drop any columns that are now entirely nan values\n",
    "    \n",
    "    data.dropna(axis = 1, how = \"all\", inplace = True)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data.iloc[i,0] == \"Weather\":\n",
    "            row1 = i\n",
    "        if data.iloc[i,0] == \"Humidity\":\n",
    "            row2 = i\n",
    "\n",
    "    difference = row2 - row1\n",
    "    \n",
    "    if difference > 4:\n",
    "        race = data.values.tolist()\n",
    "        race_1 = []\n",
    "        for i in range(len(race[1])):\n",
    "            race_1.append(\" \".join([unicode(race[1][i]),unicode(race[3][i])]))\n",
    "        new_race = [race[0], race_1, race[4],race[5], race[6], race[7]]\n",
    "        data = pd.DataFrame(new_race)\n",
    "    \n",
    "    # Fill the dataframe row\n",
    "    \n",
    "    if data.shape[1] > 5:\n",
    "    \n",
    "        data = data.values.tolist()\n",
    "    \n",
    "        cond = [item for item in data[0] if unicode(item) not in ['nan', 'NaN']]\n",
    "        for i in range(1,5):\n",
    "            info.append(cond[i])\n",
    "\n",
    "        cond = [item for item in data[1] if unicode(item) not in ['nan', 'NaN']]            \n",
    "        for i in range(1,5):\n",
    "            info.append(cond[i])\n",
    "        \n",
    "        cond = [item for item in data[2] if unicode(item) not in ['nan', 'NaN']]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "        \n",
    "        cond = [item for item in data[3] if unicode(item) not in ['nan', 'NaN']]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "        \n",
    "        cond = [item for item in data[4] if unicode(item) not in ['nan', 'NaN']]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "        \n",
    "        cond = [item for item in data[5] if unicode(item) not in ['nan', 'NaN']]\n",
    "        for i in range(1,5):\n",
    "            length = len(unicode(cond[i]).split())\n",
    "            info.append(unicode(cond[i]).split()[length-2])\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        data = data.values.tolist()\n",
    "        \n",
    "        cond = data[0]\n",
    "        for i in range(1,5):\n",
    "            info.append(cond[i])\n",
    "            \n",
    "        cond = data[1]\n",
    "        for i in range(1,5):\n",
    "            info.append(cond[i])\n",
    "            \n",
    "        cond = data[2]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "            \n",
    "        cond = data[3]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "            \n",
    "        cond = data[4]\n",
    "        for i in range(1,5):\n",
    "            info.append(unicode(cond[i]).split()[0])\n",
    "            \n",
    "        cond = data[5]\n",
    "        for i in range(1,5):\n",
    "            length = len(unicode(cond[i]).split())\n",
    "            info.append(unicode(cond[i]).split()[length-2])\n",
    "    # Make it a dataframe\n",
    "    \n",
    "    df = pd.DataFrame([info], columns = cols)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_collect_weather : passes through all of the sprint races for a given season and collects \n",
    "                    the weather data into a single data frame \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "gender : M or W\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "data : A data frame containing the data summary for all the men's or women's races in the \n",
    "        given season\n",
    "\n",
    "NB: columns coded 'A' give conditions 30 minutes before start time, columns coded 'B' \n",
    "    give conditions at start time, columns coded 'C' give conditions 30 minutes after\n",
    "    start time, and columns coded 'D' give conditions at the finish.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_collect_weather(year, gender):\n",
    "    \n",
    "    racecodes = ['SMSP', 'SMSPS']\n",
    "            \n",
    "    events = ['CP01', 'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', \n",
    "              'CH__', 'OG__']\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Year', 'Event', 'Race', 'Weather A', 'Weather B', \n",
    "                                 'Weather C', 'Weather D', 'Snow Cond A', 'Snow Cond B',\n",
    "                                 'Snow Cond C', 'Snow Cond D', 'Snow Temp A', 'Snow Temp B',\n",
    "                                 'Snow Temp C', 'Snow Temp D', 'Air Temp A', 'Air Temp B', \n",
    "                                 'Air Temp C', 'Air Temp D', 'Humidity A', 'Humidity B',\n",
    "                                 'Humidity C', 'Humidity D', 'Wind A', 'Wind B', 'Wind C',\n",
    "                                 'Wind D'])\n",
    "    \n",
    "    for event in events:\n",
    "        for race in racecodes:\n",
    "            ibu_get_summary_file(year,event,race)\n",
    "            try:\n",
    "                data = ibu_get_weather_info(\"summary.pdf\", year, event, race)\n",
    "                df = pd.concat([df, data])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    pickle = \"ibu_weather_summary_%(year)s_%(gender)s.pkl\" %{\"year\": year, \"gender\": gender}\n",
    "            \n",
    "    df.reset_index(drop = True, inplace = True)   \n",
    "        \n",
    "    df.to_pickle(pickle)\n",
    "    \n",
    "    course_file = \"ibu_course_summary_%(year)s_%(gender)s.pkl\" %{\"year\": year, \"gender\": gender}\n",
    "    course = pd.read_pickle(course_file)\n",
    "    \n",
    "    if len(course) > len(df):\n",
    "        print \"You have\", len(course), \"course files and only\", len(df), \"\"\"weather files \\\n",
    "        for the\"\"\", gender, year, \"season.\"\n",
    "    elif len(df) > len(course):\n",
    "        print \"You have\", len(df), \"weather files and only\", len(course), \"\"\"course files \\\n",
    "        for the\"\"\", gender, year, \"season.\"\n",
    "    else:\n",
    "        print \"\"\"You have the same number of weather and course files for \\\n",
    "        the\"\"\", gender, year, \"season.\"\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have the same number of weather and course files for         the M 0405 season.\n",
      "You have the same number of weather and course files for         the M 0506 season.\n",
      "You have the same number of weather and course files for         the M 0607 season.\n",
      "You have the same number of weather and course files for         the M 0708 season.\n",
      "You have the same number of weather and course files for         the M 0809 season.\n",
      "You have the same number of weather and course files for         the M 0910 season.\n",
      "You have the same number of weather and course files for         the M 1011 season.\n",
      "You have the same number of weather and course files for         the M 1112 season.\n",
      "You have the same number of weather and course files for         the M 1213 season.\n",
      "You have the same number of weather and course files for         the M 1314 season.\n",
      "You have the same number of weather and course files for         the M 1415 season.\n",
      "You have the same number of weather and course files for         the M 1516 season.\n",
      "You have the same number of weather and course files for         the M 1617 season.\n",
      "You have the same number of weather and course files for         the M 1718 season.\n"
     ]
    }
   ],
   "source": [
    "for season in seasons:\n",
    "    ibu_collect_weather(season, 'M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Adjustments to course and weather data\n",
    "\n",
    "Although at this point we have collected course and weather data for both the World Cup and IBU Cup races, there are still a few things to be done with these data files. In particular, I want to\n",
    "1. Convert quantitative variables from strings to float values. In some cases, this will require converting numbers given in European format. In order to do this, I use the function\n",
    "    - ```euro_to_float```: This function takes as input either a string containing a number in either decimal or European format or a number (float or integer). It then converts the input into a float and returns it. (Obviously, inputs that are actually floats are returned as is.) \n",
    "2. Add altitudes to for each race. The altitudes are currently stored in a .csv file, and were obtained by using the website [elevationmap.net](https://elevationmap.net/old/) together with the names of the biathlon stadia (available in the headers of the competition analysis files). \n",
    "3. Add columns for quantitative year and quantitative event (which will be useful later).\n",
    "4. Make a handful of corrections for badly read or inaccurate data.\n",
    "\n",
    "We then perform the same steps for the IBU Cup data\n",
    "\n",
    "Previous Section: [Collecting IBU Cup course and weather data](#Collecting-IBU-Cup-course-and-weather-data)\n",
    "\n",
    "\n",
    "Next Section: [Exploring Relationships](#Exploring-Relationships)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "euro_to_float : converts numbers from European format (with commas as decimal \n",
    "                separators) to standard decimal format\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "string : a string containing a number, possibly in decimal format, possibly in European \n",
    "        format. Could also be an integer or a real number\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "string2 : a real number (float) derived from the input string\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def euro_to_float(string):\n",
    "    \n",
    "    try:\n",
    "        string2 = float(string)\n",
    "    except:    \n",
    "        string1 = string.split(',')\n",
    "        if len(string1) != 2:\n",
    "            string2 = string\n",
    "        else:\n",
    "            try:\n",
    "                string2 = float(\".\".join(string1))\n",
    "            except:\n",
    "                string2 = string\n",
    "    \n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    filename = 'weather_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    filename1 = 'ibu_weather_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    \n",
    "    df = pd.read_pickle(filename)\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    for column in columns:\n",
    "        for i in range(len(df)):\n",
    "            df.loc[i,column] = euro_to_float(df.loc[i,column])\n",
    "        \n",
    "    df.to_pickle(filename)\n",
    "    \n",
    "    df1 = pd.read_pickle(filename1)\n",
    "    \n",
    "    columns = df1.columns.tolist()\n",
    "\n",
    "    for column in columns:\n",
    "        for i in range(len(df1)):\n",
    "            df1.loc[i,column] = euro_to_float(df1.loc[i,column])\n",
    "        \n",
    "    df1.to_pickle(filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    filename = 'course_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    filename1 = 'ibu_course_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    \n",
    "    df = pd.read_pickle(filename)\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    for column in columns:\n",
    "        for i in range(len(df)):\n",
    "            df.loc[i,column] = euro_to_float(df.loc[i,column])\n",
    "        \n",
    "    df.to_pickle(filename)\n",
    "    \n",
    "    df1 = pd.read_pickle(filename1)\n",
    "    \n",
    "    columns = df1.columns.tolist()\n",
    "\n",
    "    for column in columns:\n",
    "        for i in range(len(df1)):\n",
    "            df1.loc[i,column] = euro_to_float(df1.loc[i,column])\n",
    "        \n",
    "    df1.to_pickle(filename1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding altitudes to the course summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding altitudes to the dataframe (choosing to use course_summaries here)\n",
    "\n",
    "altitudes = pd.read_csv('site_altitudes.csv')\n",
    "altitudes.set_index('Unnamed: 0', inplace = True, drop = True)\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    filename = 'course_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    df = pd.read_pickle(filename)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        season = int(df.loc[i,'Year'])\n",
    "        event = df.loc[i,'Event']\n",
    "    \n",
    "        df.loc[i,'Altitude'] = altitudes.loc[season, event]\n",
    "\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Quantifying the seasons and events\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quant_seasons = {'0405' : 1, '0506' : 2, '0607' : 3, '0708' : 4, '0809' : 5, '0910' : 6,\n",
    "                 '1011' : 7, '1112' : 8, '1213' : 9, '1314' : 10, '1415' : 11, '1516' : 12,\n",
    "                 '1617' : 13,'1718' : 14}\n",
    "quant_events = {'CP01' : [1,1], 'CP02' : [2,2], 'CP03' : [3,3], 'CP04' : [4,4], \n",
    "                'CP05' : [5,5], 'CP06' : [6,6], 'CP07' : [7,8], 'CP08' : [8,9], \n",
    "                'CP09':[10,10], 'CH__' : [9,7], 'OG__' : [7,7]}\n",
    "\n",
    "# note that the order of events in a given season is dependent on the season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding quantitative values for season and event to the dataframes \n",
    "# (choosing to use course_summaries here)\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    filename = 'course_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    df = pd.read_pickle(filename)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        season = str(int(df.loc[i,'Year']))\n",
    "        if len(season) < 4:\n",
    "            season = \"\".join(['0',season])\n",
    "        event = df.loc[i,'Event']\n",
    "    \n",
    "        if season in ['0405','1011','1112','1415','1516']:\n",
    "            df.loc[i,'Quant Event'] = quant_events[event][0]\n",
    "        else:\n",
    "            df.loc[i,'Quant Event'] = quant_events[event][1]\n",
    "        df.loc[i,'Quant Year'] = quant_seasons[season]\n",
    "        \n",
    "    df.to_pickle(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting inconsistancies in capitalization and bad data\n",
    "\n",
    "To be entirely clear here, bad data refers to cases where the data was misread from the pdf files, in which case it is being replaced by the original data from the pdf file, and cases where the given values are logically impossible (a maximum climb that exceeds the height differential of the course, for example) in which case it is replaced by the value from other races held at the same site.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a few alterations to snow and weather conditions due to inconsistancies in using\n",
    "# capitalization\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    filename = 'weather_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    df = pd.read_pickle(filename)\n",
    "    \n",
    "    df.replace('Mostly cloudy','Mostly Cloudy', inplace = True)\n",
    "    df.replace('Partly cloudy','Partly Cloudy', inplace = True)\n",
    "    df.replace('Partly cloudy night','Partly Cloudy Night', inplace = True)\n",
    "    \n",
    "    df.replace('Packed powder','Packed Powder', inplace = True)\n",
    "    df.replace('Wet and powder', 'Wet & Powder', inplace = True)\n",
    "    df.replace('Wet & powder', 'Wet & Powder', inplace = True)\n",
    "    df.replace('Wet and Powder', 'Wet & Powder', inplace = True)\n",
    "    df.replace('Light snow','Light Snow', inplace = True)\n",
    "    df.replace('Hard packed variable','Hard Packed Variable', inplace = True)\n",
    "    df.replace('Hard packed','Hard Packed', inplace = True)\n",
    "    df.replace('Spring conditions', 'Spring Conditions', inplace = True)\n",
    "    df.replace('powder', 'Powder', inplace = True)\n",
    "\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correcting for misread or missing data \n",
    "\n",
    "# PyeongChang 1718 OG__\n",
    "\n",
    "df = pd.read_pickle('weather_summary_1718_M.pkl')\n",
    "\n",
    "row_z = np.where(df['Event'] == 'OG__')[0][0]\n",
    "df.loc[row_z] = [1718.0, 'OG__', 'SMSP', 'Light Snow', 'Mostly Cloudy', 'Mostly Cloudy',\n",
    "                 'Mostly Cloudy', u'Compact', u'Compact', u'Compact', u'Compact', -12.9,\n",
    "                 -14.3, -14.3, -13.8, -11.0, -10.7, -11.0, -11.3, 68.0, 63.0, 64.0, 64.0, \n",
    "                 1.9, 2.7, 1.4, 2.3]\n",
    "\n",
    "df.to_pickle('weather_summary_1718_M.pkl')\n",
    "\n",
    "# Hochfilzen 1718 CH02\n",
    "\n",
    "df = pd.read_pickle('course_summary_1718_M.pkl')\n",
    "\n",
    "row_y = np.where(df['Event'] == 'CP02')[0][0]\n",
    "df.loc[row_y, 'Height Diff'] = 39.2\n",
    "\n",
    "df.to_pickle('course_summary_1718_M.pkl')\n",
    "\n",
    "# Nove Mesto 1617 CP03 \n",
    "## NOTE for this race and the next one, it would probably be better to fill the \n",
    "# dataframe completely\n",
    "\n",
    "df = pd.read_pickle('weather_summary_1617_M.pkl')\n",
    "\n",
    "row_a = np.where((df['Event'] == 'CP03'))[0][0]\n",
    "df.loc[row_a,'Weather C'] = 'Mostly Cloudy Night'\n",
    "df.loc[row_a,'Snow Cond C'] = 'Granular'\n",
    "df.loc[row_a,'Snow Temp C'] = -0.5\n",
    "df.loc[row_a,'Air Temp C'] = -0.2\n",
    "df.loc[row_a,'Humidity C'] = 67\n",
    "df.loc[row_a,'Wind C'] = 0.5\n",
    "\n",
    "# PyeongChang 1617 CP07\n",
    "\n",
    "row_b = np.where((df['Event'] == 'CP07'))[0][0]\n",
    "df.loc[row_b,'Weather C'] = 'Partly Cloudy Night'\n",
    "df.loc[row_b,'Snow Cond C'] = 'Hard Packed Variable'\n",
    "df.loc[row_b,'Snow Temp C'] = 0.0\n",
    "df.loc[row_b,'Air Temp C'] = 1.2\n",
    "df.loc[row_b,'Humidity C'] = 53\n",
    "df.loc[row_b,'Wind C'] = 1.3\n",
    "\n",
    "df.to_pickle('weather_summary_1617_M.pkl')\n",
    "# 1617 Championship (European style decimal was misconverted)\n",
    "\n",
    "df = pd.read_pickle('course_summary_1617_M.pkl')\n",
    "row_c = np.where((df['Event'] == 'CH__'))[0][0]\n",
    "df.loc[row_c, 'Height Diff'] = 39.2\n",
    "df.to_pickle('course_summary_1617_M.pkl')\n",
    "\n",
    "# 0607 CP02 Total Climb (Note that the given value here, 3335m is unreasonable. I replace it\n",
    "# by the value given for other races at the same site)\n",
    "\n",
    "df = pd.read_pickle('course_summary_0607_M.pkl')\n",
    "row_d = np.where((df['Event'] == 'CP02'))[0][0]\n",
    "df.loc[row_d,'Total Climb'] = 381\n",
    "df.to_pickle('course_summary_0607_M.pkl')\n",
    "\n",
    "# And replacing the Max Climb by Max Climb from another event at the same site, because \n",
    "# the given max climb exceeds the height differential\n",
    "\n",
    "df = pd.read_pickle('course_summary_0405_M.pkl')\n",
    "row_e = np.where((df['Event'] == 'CP06'))[0][0]\n",
    "df.loc[row_e,'Max Climb'] = 29\n",
    "df.to_pickle('course_summary_0405_M.pkl')\n",
    "\n",
    "df = pd.read_pickle('course_summary_0506_M.pkl')\n",
    "row_f = np.where( (df['Event'] == 'CP06'))[0][0]\n",
    "df.loc[row_f,'Max Climb'] = 29\n",
    "df.to_pickle('course_summary_0506_M.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Performing the same steps for the ibu summary dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding altitudes to the dataframe (choosing to use course_summaries here)\n",
    "\n",
    "altitudes = pd.read_csv('ibu_site_elevations.csv')\n",
    "altitudes.set_index('Year',inplace = True, drop = True)\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617', '1718']\n",
    "\n",
    "for season in seasons:\n",
    "    filename = 'ibu_course_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    df = pd.read_pickle(filename)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        season = int(df.loc[i,'Year'])\n",
    "        event = df.loc[i,'Event']\n",
    "    \n",
    "        df.loc[i,'Altitude'] = altitudes.loc[season, event]\n",
    "\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quant_seasons = {'0405' : 1, '0506' : 2, '0607' : 3, '0708' : 4, '0809' : 5, '0910' : 6, \n",
    "                 '1011' : 7, '1112' : 8, '1213' : 9, '1314' : 10, '1415' : 11, '1516' : 12,\n",
    "                 '1617' : 13, '1718' : 14}\n",
    "quant_events = {'CP01' : [1,1,1,1], 'CP02' : [2,2,2,2], 'CP03' : [3,3,3,3], \n",
    "                'CP04' : [4,4,4,4], 'CP05' : [5,5,9,5], 'CP06' : [6,7,5,6], \n",
    "                'CP07' : [8,8,7,7], 'CP08' : [9,9,8,9], 'CH__' : [7,6,6,8]}\n",
    "\n",
    "# note that the order of events in a given season is dependent on the season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding quantitative values for season and event to the dataframes \n",
    "# (choosing to use course_summaries here)\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    filename = 'ibu_course_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    df = pd.read_pickle(filename)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        season = str(int(df.loc[i,'Year']))\n",
    "        if len(season) < 4:\n",
    "            season = \"\".join(['0',season])\n",
    "        event = df.loc[i,'Event']\n",
    "    \n",
    "        if season in ['0405','0708','1314']:\n",
    "            df.loc[i,'Quant Event'] = quant_events[event][0]\n",
    "        elif season in ['0506','1112','1415','1617','1718']:\n",
    "            df.loc[i,'Quant Event'] = quant_events[event][1]\n",
    "        elif season in ['0607']:\n",
    "            df.loc[i,'Quant Event'] = quant_events[event][2]\n",
    "        else: # season in ['0809','0910','1011','1213','1516']\n",
    "            df.loc[i,'Quant Event'] = quant_events[event][3]\n",
    "        df.loc[i,'Quant Year'] = quant_seasons[season]\n",
    "        \n",
    "    df.to_pickle(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correcting for misread or missing data \n",
    "\n",
    "# Altenberg 0506 CP04\n",
    "\n",
    "df = pd.read_pickle('ibu_course_summary_0506_M.pkl')\n",
    "\n",
    "row_z = np.where((df['Event'] == 'CP04'))[0][0]\n",
    "df.loc[row_z,'Max Climb'] = 37\n",
    "\n",
    "df.to_pickle('ibu_course_summary_0506_M.pkl')\n",
    "\n",
    "# Forni Avoltri 0607 CP04 \n",
    "\n",
    "df = pd.read_pickle('ibu_course_summary_0607_M.pkl')\n",
    "\n",
    "row_a = np.where((df['Event'] == 'CP04'))[0][0]\n",
    "df.loc[row_a,'Length'] = 10000\n",
    "\n",
    "df.to_pickle('ibu_course_summary_0607_M.pkl')\n",
    "\n",
    "# Brezno Osrblie 1011 CP06 \n",
    "\n",
    "df = pd.read_pickle('ibu_course_summary_1011_M.pkl')\n",
    "\n",
    "row_b = np.where((df['Event'] == 'CP06'))[0][0]\n",
    "df.loc[row_b,'Height Diff'] = 46.5\n",
    "df.loc[row_b,'Max Climb'] = 19.5\n",
    "df.loc[row_b,'Total Climb'] = 122.5\n",
    "\n",
    "df.to_pickle('ibu_course_summary_1011_M.pkl')\n",
    "\n",
    "# Brezno Osrblie 1617 CP06\n",
    "\n",
    "df = pd.read_pickle('ibu_course_summary_1617_M.pkl')\n",
    "\n",
    "row_c = np.where((df['Event'] == 'CP06'))[0][0]\n",
    "df.loc[row_c, 'Height Diff'] = 46.8\n",
    "df.loc[row_c, 'Max Climb'] = 25.07\n",
    "df.loc[row_c, 'Total Climb'] = 381.38\n",
    "\n",
    "df.to_pickle('ibu_course_summary_1617_M.pkl')\n",
    "\n",
    "# Martell-Val Martello 1314 CP08 \n",
    "\n",
    "df = pd.read_pickle('ibu_weather_summary_1314_M.pkl')\n",
    "\n",
    "row_d = np.where((df['Event'] == 'CP08'))[0][0]\n",
    "\n",
    "df.loc[row_d,'Snow Cond A'] = 'Hard Packed Variable'\n",
    "df.loc[row_d,'Snow Cond B'] = 'Hard Packed Variable'\n",
    "\n",
    "df.to_pickle('ibu_weather_summary_1314_M.pkl')\n",
    "\n",
    "# 1617 weather problems\n",
    "\n",
    "df = pd.read_pickle('ibu_weather_summary_1617_M.pkl')\n",
    "\n",
    "row_e = np.where((df['Event'] == 'CP07'))[0][0]\n",
    "df.loc[row_e, 'Weather A'] = 'Mostly Cloudy'\n",
    "df.loc[row_e, 'Weather B'] = 'Mostly Cloudy'\n",
    "\n",
    "row_f = np.where((df['Event'] == 'CP08') & (df['Race'] == 'SMSP'))[0][0]\n",
    "df.loc[row_f, 'Snow Cond A'] = 'Packed Powder'\n",
    "df.loc[row_f, 'Snow Cond B'] = 'Packed Powder'\n",
    "\n",
    "row_g = np.where((df['Event'] == 'CP08') & (df['Race'] == 'SMSPS'))[0][0]\n",
    "df.loc[row_g, 'Snow Cond A'] = 'Packed Powder'\n",
    "df.loc[row_g, 'Snow Cond B'] = 'Packed Powder'\n",
    "\n",
    "df.to_pickle('ibu_weather_summary_1617_M.pkl')\n",
    "\n",
    "# 1718 weather problems\n",
    "\n",
    "df = pd.read_pickle('ibu_weather_summary_1718_M.pkl')\n",
    "\n",
    "row_h = np.where(df['Event'] == 'CP06')[0][0]\n",
    "df.loc[row_h, 'Snow Cond A'] = 'Packed Powder'\n",
    "df.loc[row_h, 'Snow Cond B'] = 'Packed Powder'\n",
    "\n",
    "row_i = np.where(df['Event'] == 'CP07')[0][0]\n",
    "df.loc[row_i, 'Weather A'] = 'Partly Cloudy'\n",
    "df.loc[row_i,'Weather B'] = 'Partlly Cloudy'\n",
    "\n",
    "row_j = np.where(df['Event'] == 'CP08')[0][0]\n",
    "df.loc[row_j, 'Snow Cond A'] = 'Packed Powder'\n",
    "df.loc[row_j, 'Snow Cond B'] = 'Packed Powder'\n",
    "\n",
    "df.to_pickle('ibu_weather_summary_1718_M.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a few alterations to snow and weather conditions due to inconsistancies in \n",
    "# using capitalization\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415','1516',\n",
    "           '1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    filename = 'ibu_weather_summary_%(season)s_M.pkl' %{'season' : season}\n",
    "    df = pd.read_pickle(filename)\n",
    "    \n",
    "    df.replace('Mostly cloudy','Mostly Cloudy', inplace = True)\n",
    "    df.replace('Partly cloudy','Partly Cloudy', inplace = True)\n",
    "    df.replace('Partly cloudy night','Partly Cloudy Night', inplace = True)\n",
    "    \n",
    "    df.replace('Packed powder','Packed Powder', inplace = True)\n",
    "    df.replace('Wet and powder', 'Wet & Powder', inplace = True)\n",
    "    df.replace('Wet & powder', 'Wet & Powder', inplace = True)\n",
    "    df.replace('Wet and Powder', 'Wet & Powder', inplace = True)\n",
    "    df.replace('Light snow','Light Snow', inplace = True)\n",
    "    df.replace('Hard packed variable','Hard Packed Variable', inplace = True)\n",
    "    df.replace('Hard packed','Hard Packed', inplace = True)\n",
    "    df.replace('Spring conditions', 'Spring Conditions', inplace = True)\n",
    "    df.replace('powder', 'Powder', inplace = True)\n",
    "\n",
    "\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Exploring Relationships\n",
    "\n",
    "Next we wish to explore the relationships between various course attributes and weather conditions and speed, shooting accuracy, etc on an individual basis. In particular, for each aspect of the total biathlon time (speed, prone shooting accuracy, standing shooting accuracy, prone shooting time, standing shooting time, prone penalty loop time, and standing penalty loop time), we wish to determine which of the predictor variables correlate most closely with that aspect. In doing this, we recognize that, due to the fact that we have fourteen different predictor variables, it is reasonable to expect that none of the correlation values will be particularly high.\n",
    "\n",
    "Previous Section: [Adjustments to course and weather data](#Adjustments-to-course-and-weather-data)\n",
    "\n",
    "\n",
    "Next Section: [Effect of Conditions on Speed](#Effects-of-conditions-on-speed)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of conditions on speed\n",
    "\n",
    "#### Objective\n",
    "To successfully model the average speed and standard deviation of speed for a race as a function of course characteristics and conditions. \n",
    "\n",
    "#### Step 0 : Convert 'Total Ski' into seconds\n",
    "\n",
    "I find the average ski time, and then use that to compute the average speed.  In order to do this, the first step is going to have to be to convert all of the ```Total Ski``` columns in these files from a minutes and seconds format to a seconds format.\n",
    "1. ```convert_to_seconds```: This function takes a string of the form \"h:mm:ss.s\" or \"mm:ss.s\", converts the elapsed time into seconds, and returns that number as a float.\n",
    "2. ```skitimes_to_seconds```: This function loops through all of the rows in a given competition analysis dataframe and replaces the string time entries for ```'Ski Time'``` with the time converted to seconds\n",
    "3. ```compute_speeds```: This function takes as input a season and event, retrieves the length of the men's sprint race for that event from the course summary file, and calculates the speed (in m/s) for every racer in the competition. It then adds these values to the competition analysis file as a dataframe column, and returns the  file to pickle.\n",
    "4. ```ibu_compute_speeds```: This function is analogous to ```compute_speeds```.\n",
    " \n",
    "#### Step 1: Collect the data\n",
    "\n",
    "The obvious first step here is to assemble the data. In theory, this has already been collected from the pdf pages and saved to my laptop, so what remains is to pull the pieces of it that I want and assemble it into a useable dataframe. Here, I should be able to use data all the way back to the 2004-2005 season, which will, hopefully, increase my ability to get an accurate model. There aren't actually any functions to describe here, just a lot of finding means and then putting the results into a dataframe.\n",
    "\n",
    "#### Step 2 : Add weather and snow condition data\n",
    "The next step is to add information about the weather and snow conditions to our dataframe, and then to add information about the course layout to our dataframe.\n",
    "1. ```add_course_data```: This function adds data about the course to the output of a dataframe containing \\*-response variable data\n",
    "2. ```add_weather_data```: This function adds data about the weather conditions to the output of a dataframe containing \\*-response variable and course data.\n",
    "\n",
    "#### Step 3: Quantifying snow and weather conditions\n",
    "\n",
    "In order to explore effects of each predictor variable on the response variables, it seemed useful to be able to treat all the variables, including the categorical variables, as quantitative variables. Of our four categorical variables, two, season and event, are relatively easy to quantify because their values are temporal. The other two are less easy to quantify, however. On the one hand, it seems clear that the condition of the snow will have an effect on the speed at which the racers are able to ski. On the other hand, I don't have, a priori, a very good sense of what that effect might be. Similarly, it seems likely that weather conditions might effect shooting accuracy, but again, it is unclear from just looking at the possible values taken by weather what exactly those effects might be. Ideally we would be able to rank all of the values for snow conditions and all of the values for weather from most difficult to least difficult and we would be able to use those rankings to quantify the variables.\n",
    "\n",
    "The question then becomes how to go about quantifying these categorical variables. My initial approach was to search for descriptions of snow conditions on skiing sites, looking in particular at those that gave an _ease of skiing score_ for each of their listed snow conditions. However, I ran into problems because the snow condition values that I had did not always match the snow condition values available on the sites, because they quite often gave ranges of values for ease of skiing, and because the sites were tailored for Alpine rather than Nordic skiers. Furthermore, there was nothing that was even remotely useful of that nature for weather condition data. My next approach was to simply try to group snow conditions (and weather conditions) based on which variables seemed to be most similar to me. Here the biggest problem is that I didn't know enough to have a sense of what differences are important and which are unimportant.\n",
    "\n",
    "Ultimately, I decided that the most straightforward approach was to simply use the performance data that I had to attempt to rank values taken by weather and by snow conditions. The first thing to acknowledge here is that this risks being a bit like the tail wagging the dog. In other words, by using the performance data that I have to rank the values and then using the rankings to make predictions about performance data, I risk demonstrating precisely nothing. As a result, I took a couple of precautions:\n",
    "1. Since my ultimate was to make predictions about the total race times for races in the last four seasons of my available data (from 2014-15 to 2017-18), I used only performance data from World Cup races in the first ten seasons of available data (from 2004-05 to 2013-14) to rank the values. This meant that there were a few values for both snow conditions and weather that appeared only in later races and/or ibu races. In these cases, I decided which of the values that I did have scores for they were the most like and assigned the same scores to the unranked values.\n",
    "2. I based the ranking of snow condition and weather values entirely on speed performance, and decided that, if the quantitative versions of snow conditions and weather had no impact on any other aspect, that I would eliminate them and do the best that I could with the categorical variables as categorical variables. (In fact, though their quantification is based entirely on speed, they end up on the list of most significant variables for all seven aspects of the total time.)\n",
    "\n",
    "In order to quantify the categorical variable snow conditions, I did the following:\n",
    "1. For each value taken by snow conditions during the seasons 2004-05 to 2013-14, I calculated the mean of the average speeds.\n",
    "2. I ranked them in order from fastest average to slowest average. \n",
    "3. The fastest average was given a score of 10; the slowest was given a score of 0. \n",
    "4. The range of the averages was divided into 10 equal pieces, and each remaining value was assigned a score (from 0 to 9) based on which piece of the range it was in.\n",
    "5. Any values for snow conditions that were not found in the first ten seasons of world cup data were added manually by deciding which of the known conditions they were most like.\n",
    "\n",
    "The quantification of the categorical variable weather proceeded in much the same way.\n",
    "\n",
    "Once these quantifications were complete, the variables ```quant_snow``` and ```quant_weather``` were added to the speed dataframe.\n",
    "\n",
    "#### Step 4: Exploring correlation and impact\n",
    "\n",
    "The other thing that I want to do here is to compute the correlation coefficients and total changes explained for each of the quantitative predictor variables.\n",
    "\n",
    "1. ```corr_and_change```: Given a predictor variable x and a response variable y in a dataframe df, this function determines the strength of the linear dependance of y on x and what fraction of the total change in y is predicted based on the range of values taken by x.\n",
    "2. ```collect_best_variables```: This function takes a dataframe produced by ```corr_and_change``` and chooses the predictor variables with the highest coefficients of determination and the highest percent change predicted (in both cases, with values at least half as large as the maximal values)\n",
    "\n",
    "\\*-response: Because these functions (```add_course_data``` and ```add_weather_data```) are used to add condition data to dataframes containing speed, accuracy, \\*-response variable indicates that a variety of possible response variables can by entered in this part of the description.\n",
    "\n",
    "Previous Section: [Exploring Relationships](#Exploring-Relationships)\n",
    "\n",
    "\n",
    "Next Section: [Evaluating Impacts on Speed](#Evaluating-Impacts-on-Speed)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "convert_to_seconds : takes a time string of the form \"h:mm:ss.s\" or \"mm:ss.s\" and returns \n",
    "                     the total number of seconds as a float\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "string : a time string of the form \"h:mm:ss.s\" or \"mm:ss.s\"\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "seconds : the elapsed time given by the time string in seconds\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def convert_to_seconds(string):\n",
    "    \n",
    "    try:\n",
    "        split_string = string.split(\":\")\n",
    "    \n",
    "        if len(split_string) == 1:\n",
    "            try:\n",
    "                seconds = float(string)\n",
    "            except:\n",
    "                seconds = string\n",
    "        elif len(split_string) == 2:\n",
    "            seconds = float(split_string[0]) * 60 + float(split_string[1])\n",
    "        elif len(split_string) == 3:\n",
    "            seconds = (float(split_string[0]) * 3600 + float(split_string[1]) * 60\n",
    "                       + float(split_string[2]))\n",
    "        else:\n",
    "            seconds = string\n",
    "    except:\n",
    "        seconds = string\n",
    "        \n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "skitimes_to_seconds : converts all of the ski times for a given competition to seconds\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a competition analysis dataframe\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "modifies the given dataframe in place\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def skitimes_to_seconds(df):\n",
    "    \n",
    "    for i in df.index.tolist():\n",
    "        df.loc[i,'Total Ski'] = convert_to_seconds(df.loc[i,'Total Ski'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        filename = ('companal_SMSP_%(season)s_%(event)s.pkl' \n",
    "                    %{'season' : season, 'event' : event})\n",
    "        \n",
    "        try:\n",
    "            race_data = pd.read_pickle(filename)\n",
    "            race_data_new = skitimes_to_seconds(race_data)\n",
    "            race_data_new.to_pickle(filename)\n",
    "        except: # the race does not exist or does not have data\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "compute_speeds : takes as input a season and event, retrieves the length of the men's sprint\n",
    "                 race for that event from the course summary file, then calculates speed \n",
    "                 (in m/s) for every racer in the competition (length/time), adds them to \n",
    "                 the companal file as a dataframe column, and returns the companal file to \n",
    "                 pickle\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "season : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "stores a pickle file to the hard drive\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def compute_speeds(season, event):\n",
    "    \n",
    "    course_filename = \"course_summary_%(season)s_M.pkl\" %{'season': season}\n",
    "    course_data = pd.read_pickle(course_filename)\n",
    "    course_data.set_index('Event', drop=True, inplace=True)\n",
    "\n",
    "    filename = \"companal_SMSP_%(season)s_%(event)s.pkl\" %{'season' : season, 'event' : event}\n",
    "    \n",
    "    race_data = pd.read_pickle(filename)\n",
    "    length = float(course_data.loc[event, 'Length'])\n",
    "    for i in range(len(race_data)):\n",
    "        race_data.loc[i, 'Speed'] = length/race_data.loc[i,'Total Ski']\n",
    "                \n",
    "    race_data.to_pickle(filename)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding speed columns to our event dataframes\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "\n",
    "failures = []\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        try:\n",
    "            compute_speeds(season, event)\n",
    "        except:\n",
    "            failures.append([season, event])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And adding speeds for the IBU cup races\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        filename = 'ibu_SMSP_%(season)s_%(event)s.pkl' %{'season' : season, 'event' : event}\n",
    "        \n",
    "        try:\n",
    "            race_data = pd.read_pickle(filename)\n",
    "            race_data_new = skitimes_to_seconds(race_data)\n",
    "            race_data_new.to_pickle(filename)\n",
    "        except: # the race does not exist or does not have data\n",
    "            pass\n",
    "        \n",
    "        filename = 'ibu_SMSPS_%(season)s_%(event)s.pkl' %{'season' : season, 'event' : event}\n",
    "        \n",
    "        try:\n",
    "            race_data = pd.read_pickle(filename)\n",
    "            race_data_new = skitimes_to_seconds(race_data)\n",
    "            race_data_new.to_pickle(filename)\n",
    "        except: # the race does not exist or does not have data\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_compute_speeds : takes as input a season and event, retrieves the length of the men's\n",
    "                     sprint race for that event from the course summary file, then calculates\n",
    "                     speed (in m/s) for every racer in the competition (length/time), adds\n",
    "                     them to the ibu competition analysis file as a dataframe column, and \n",
    "                     returns the competition analysis file to pickle\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "season : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "stores a pickle file to the hard drive\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_compute_speeds(season, event):\n",
    "    \n",
    "    course_filename = \"ibu_course_summary_%(season)s_M.pkl\" %{'season': season}\n",
    "    course_data = pd.read_pickle(course_filename)\n",
    "    course_data.set_index('Event', drop=True, inplace=True)\n",
    "\n",
    "    filename = \"ibu_SMSP_%(season)s_%(event)s.pkl\" %{'season' : season, 'event' : event}\n",
    "    filenameS = \"ibu_SMSPS_%(season)s_%(event)s.pkl\" %{'season' : season, 'event' : event}\n",
    "    \n",
    "    try:\n",
    "        race_data = pd.read_pickle(filename)\n",
    "        length = float(course_data.loc[event, 'Length'])\n",
    "        for i in range(len(race_data)):\n",
    "            race_data.loc[i, 'Speed'] = length/race_data.loc[i,'Total Ski']\n",
    "    \n",
    "        race_data.to_pickle(filename)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        race_data = pd.read_pickle(filenameS)\n",
    "        length = float(course_data.loc[event, 'Length'])\n",
    "        for i in range(len(race_data)):\n",
    "            race_data.loc[i, 'Speed'] = length/race_data.loc[i,'Total Ski']\n",
    "    \n",
    "        race_data.to_pickle(filenameS)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding speed columns to our event dataframes\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415'\n",
    "           ,'1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CH__']\n",
    "\n",
    "failures = []\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        try:\n",
    "            ibu_compute_speeds(season, event)\n",
    "        except:\n",
    "            failures.append([season, event])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Collect the data\n",
    "\n",
    "The obvious first step here is to assemble the data. In theory, this has already been collected from the pdf pages and saved to my laptop, so what remains is to pull the pieces of it that I want and assemble it into a useable dataframe. Here, I should be able to use data all the way back to the 2004-2005 season, which will, hopefully, increase my ability to get an accurate model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building a dataframe of mean speed and standard deviation\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "\n",
    "race_times = []\n",
    "failures = []\n",
    "\n",
    "for season in seasons:\n",
    "    course_filename = \"course_summary_%(season)s_M.pkl\" %{'season': season}\n",
    "    course_data = pd.read_pickle(course_filename)\n",
    "    course_data.set_index('Event', drop=True, inplace=True)\n",
    "\n",
    "    for event in events:\n",
    "        filename = (\"companal_SMSP_%(season)s_%(event)s.pkl\" \n",
    "                    %{'season' : season, 'event' : event})\n",
    "        \n",
    "        try:\n",
    "            race_data = pd.read_pickle(filename)\n",
    "            if len(race_data) > 50:\n",
    "                length = float(course_data.loc[event, 'Length'])\n",
    "                race_info = [season, event, length/np.mean(race_data['Total Ski']), \n",
    "                             np.std(race_data['Speed'])]\n",
    "                race_times.append(race_info)\n",
    "            else:\n",
    "                failures.append([season,event])\n",
    "        except: # This race has no companal file\n",
    "            failures.append([season,event])          \n",
    "mean_mens_sprint_speed = pd.DataFrame(race_times, \n",
    "                                      columns = ['Year','Event','Mean Speed', 'StDev Speed'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Add weather and snow condition data\n",
    "The next step is to add information about the weather and snow conditions to our dataframe, and then to add information about the course layout to our dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "add_course_data : adds data about the course to the output of a dataframe containing speed\n",
    "                  data\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe containing biathlon speed data\n",
    "row : the dataframe row that is under consideration (a particular competition)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : the original dataframe with course information added for the relevant row\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def add_course_data(df, row):\n",
    "    \n",
    "    # Get the identifying info for the race\n",
    "    season = df.loc[row,'Year']\n",
    "    event = df.loc[row,'Event']\n",
    "\n",
    "    # Figure out the filename where the relevant course data is stored\n",
    "    filename = \"course_summary_%(season)s_M.pkl\" %{'season': season}\n",
    "\n",
    "    # Load that file\n",
    "    \n",
    "    course_data = pd.read_pickle(filename)\n",
    "    \n",
    "    # Figure out which row in that filename corresponds to your row\n",
    "    \n",
    "    race_row = -1\n",
    "    \n",
    "    for i in range(len(course_data)):\n",
    "        if (course_data.loc[i,'Event'] == event) & (course_data.loc[i,'Race'] == 'SMSP'):\n",
    "            race_row = i\n",
    "        \n",
    "    # Next, grab the data and put it in the file\n",
    "    \n",
    "    if race_row > -1:\n",
    "        df.loc[row,'Length'] = float(course_data.loc[race_row,'Length'])\n",
    "        df.loc[row,'Height Diff'] = float(course_data.loc[race_row,'Height Diff'])\n",
    "        df.loc[row,'Max Climb'] = float(course_data.loc[race_row,'Max Climb'])\n",
    "        df.loc[row,'Total Climb'] = float(course_data.loc[race_row,'Total Climb'])\n",
    "        df.loc[row, 'Altitude'] = float(course_data.loc[race_row,'Altitude'])\n",
    "        df.loc[row, 'Quant Year'] = float(course_data.loc[race_row, 'Quant Year'])\n",
    "        df.loc[row, 'Quant Event'] = float(course_data.loc[race_row, 'Quant Event'])\n",
    "        \n",
    "    else:\n",
    "        df.loc[row,'Length'] = np.nan\n",
    "        df.loc[row,'Height Diff'] = np.nan\n",
    "        df.loc[row,'Max Climb'] = np.nan\n",
    "        df.loc[row,'Total Climb'] = np.nan\n",
    "        df.loc[row,'Altitude'] = np.nan\n",
    "        df.loc[row, 'Quant Year'] = np.nan\n",
    "        df.loc[row, 'Quant Event'] = np.nan\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in range(len(mean_mens_sprint_speed)):\n",
    "    mean_mens_sprint_speed = add_course_data(mean_mens_sprint_speed, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "add_weather_data : adds data about the weather conditions to the output of a dataframe\n",
    "                   containing speed and course data\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe containing biathlon speed  and course data\n",
    "row : the dataframe row that is under consideration (a particular competition)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : the original dataframe with weather information added to the relevant row\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def add_weather_data(df, row):\n",
    "    \n",
    "    # Get the identifying info for the race\n",
    "    \n",
    "    season = df.loc[row,'Year']\n",
    "    event = df.loc[row,'Event']\n",
    "\n",
    "    # Figure out the filename where the relevant course data is stored\n",
    "    \n",
    "    filename = \"weather_summary_%(season)s_M.pkl\" %{'season': season}\n",
    "\n",
    "    # Load that file\n",
    "    \n",
    "    weather_data = pd.read_pickle(filename)\n",
    "    \n",
    "    # Figure out which row in that filename corresponds to your row\n",
    "    \n",
    "    race_row = -1\n",
    "    for i in range(len(weather_data)):\n",
    "        if (weather_data.loc[i,'Event'] == event) & (weather_data.loc[i,'Race'] == 'SMSP'):\n",
    "            race_row = i\n",
    "     \n",
    "    # Next, grab the data and put it in the file\n",
    "    \n",
    "    if race_row > -1:\n",
    "        df.loc[row,'Weather'] = weather_data.loc[race_row,'Weather C']\n",
    "        df.loc[row,'Snow Cond'] = weather_data.loc[race_row,'Snow Cond C']\n",
    "        df.loc[row,'Snow Temp'] = euro_to_float(weather_data.loc[race_row,'Snow Temp C'])\n",
    "        df.loc[row,'Air Temp'] = euro_to_float(weather_data.loc[race_row,'Air Temp C'])\n",
    "        df.loc[row,'Humidity'] = euro_to_float(weather_data.loc[race_row,'Humidity C'])\n",
    "        df.loc[row,'Wind'] = euro_to_float(weather_data.loc[race_row,'Wind C'])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    else:\n",
    "        df.loc[row,'Weather'] = np.nan\n",
    "        df.loc[row,'Snow Cond'] = np.nan\n",
    "        df.loc[row,'Snow Temp'] = np.nan\n",
    "        df.loc[row,'Air Temp'] = np.nan\n",
    "        df.loc[row,'Humidity'] = np.nan\n",
    "        df.loc[row,'Wind'] = np.nan\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in range(len(mean_mens_sprint_speed)):\n",
    "    mean_mens_sprint_speed = add_weather_data(mean_mens_sprint_speed, row)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Quantifying snow and weather conditions\n",
    "\n",
    "While most of the data that I have here is quantitative, some of it is categorical. For the categorical data, I would like to introduce some sort of quantification. In the case of season and event, this was quite simple, as the seasons have a natural ordering, and then events within the seasons also have a natural ordering. For the snow conditions and weather this is somewhat more difficult. While it seems obvious that snow conditions will have an effect on speed, it isn't obvious to me (as a non cross country skier) how this is going to play out with respect the the descriptions that are given in the files. As a result, I've decided to assign values between 0 and 10 to each of the possible weather and snow condition values. To do this, I'm going to compute the average speeds associated to each of the different snow conditions and weather conditions over the first 10 seasons of my data. The fastest weather and snow conditions will be assigned a value of 10. The slowest will be assigned a value of 0. The difference between them will be divided in tenths, and each remaining condition will be assigned the value that corresponds to it's slice in the distribution. Because I ultimately want to use this to try to make predictions about more recent seasons, I'm going to restrict the data that I consider here to the first 10 seasons for which I have data (which leaves 4 seasons untouched here).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speed</th>\n",
       "      <th>Weather Quant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Condition</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Partly Cloudy Night</th>\n",
       "      <td>7.223154</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mostly Cloudy</th>\n",
       "      <td>6.868781</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partly Cloudy</th>\n",
       "      <td>6.880993</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clear</th>\n",
       "      <td>6.836928</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sunny</th>\n",
       "      <td>6.715308</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Snow</th>\n",
       "      <td>6.611454</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cloudy</th>\n",
       "      <td>6.939226</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sky Clear</th>\n",
       "      <td>6.867468</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fog</th>\n",
       "      <td>6.700835</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rain</th>\n",
       "      <td>6.430744</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wind &amp; Snow</th>\n",
       "      <td>6.343879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Light Snow</th>\n",
       "      <td>6.716619</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Speed  Weather Quant\n",
       "Condition                                   \n",
       "Partly Cloudy Night  7.223154           10.0\n",
       "Mostly Cloudy        6.868781            5.0\n",
       "Partly Cloudy        6.880993            6.0\n",
       "Clear                6.836928            5.0\n",
       "Sunny                6.715308            4.0\n",
       "Snow                 6.611454            3.0\n",
       "Cloudy               6.939226            6.0\n",
       "Sky Clear            6.867468            5.0\n",
       "Fog                  6.700835            4.0\n",
       "Rain                 6.430744            0.0\n",
       "Wind & Snow          6.343879            0.0\n",
       "Light Snow           6.716619            4.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_data = mean_mens_sprint_speed.iloc[:96]\n",
    "\n",
    "weather_averages = []\n",
    "\n",
    "for weather in set(predictor_data['Weather']):\n",
    "    weather_data = predictor_data['Weather'] == weather\n",
    "    weather_averages.append([weather,np.mean(predictor_data[weather_data]['Mean Speed'])])\n",
    "    \n",
    "weather_averages = pd.DataFrame(weather_averages, columns = ['Condition', 'Speed'])\n",
    "weather_averages.set_index('Condition', inplace = True)\n",
    "\n",
    "max_weather = max(weather_averages['Speed'])\n",
    "min_weather = min(weather_averages['Speed'])\n",
    "step = (max_weather-min_weather)/10\n",
    "\n",
    "for weather in weather_averages.index.tolist():\n",
    "    weather_averages.loc[weather, 'Weather Quant'] = int((weather_averages.loc[weather,'Speed']\n",
    "                                                          -min_weather)/step)\n",
    "    \n",
    "weather_averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem : 'Mostly Cloudy Night' appears in later data, but not in the 10 seasons that we have of World Cup data. That means that I'll have to assign it a value. I'm going to assume that it's most like Partly Cloudy Night, and assign it a value of 10. 'Light Snowfall' is also not in the first 10 seasons. This seems to be clearly very similar to 'Light Snow', and I'll assign it a value accordingly\n",
    "\n",
    "Similarly, there are conditions that appear in the IBU race data, that don't appear here. Again, I'll assign values based on which condition with a computed value it seems to be closest to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_averages.loc['Mostly Cloudy Night', 'Weather Quant'] = 10.0\n",
    "weather_averages.loc['Light Snowfall', 'Weather Quant'] = 4.0\n",
    "weather_averages.loc['Rain Snow', 'Weather Quant'] = 3\n",
    "weather_averages.loc['Windy', 'Weather Quant'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speed</th>\n",
       "      <th>Snow Quant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Condition</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Compact</th>\n",
       "      <td>7.141269</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wet &amp; Powder</th>\n",
       "      <td>6.063496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packed Powder</th>\n",
       "      <td>6.736725</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hard Packed</th>\n",
       "      <td>6.942572</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hard</th>\n",
       "      <td>6.744447</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Granular</th>\n",
       "      <td>6.724335</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Powder</th>\n",
       "      <td>6.583711</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wet</th>\n",
       "      <td>6.712424</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fresh</th>\n",
       "      <td>6.641513</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packed</th>\n",
       "      <td>6.933795</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hard Packed Variable</th>\n",
       "      <td>6.570229</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Speed  Snow Quant\n",
       "Condition                                 \n",
       "Compact               7.141269        10.0\n",
       "Wet & Powder          6.063496         0.0\n",
       "Packed Powder         6.736725         6.0\n",
       "Hard Packed           6.942572         8.0\n",
       "Hard                  6.744447         6.0\n",
       "Granular              6.724335         6.0\n",
       "Powder                6.583711         4.0\n",
       "Wet                   6.712424         6.0\n",
       "Fresh                 6.641513         5.0\n",
       "Packed                6.933795         8.0\n",
       "Hard Packed Variable  6.570229         4.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_data = mean_mens_sprint_speed.iloc[:96]\n",
    "\n",
    "snow_averages = []\n",
    "\n",
    "for snow in set(predictor_data['Snow Cond']):\n",
    "    snow_data = predictor_data['Snow Cond'] == snow\n",
    "    snow_averages.append([snow,np.mean(predictor_data[snow_data]['Mean Speed'])])\n",
    "    \n",
    "snow_averages = pd.DataFrame(snow_averages, columns = ['Condition', 'Speed'])\n",
    "snow_averages.set_index('Condition', inplace = True)\n",
    "\n",
    "max_snow = max(snow_averages['Speed'])\n",
    "min_snow = min(snow_averages['Speed'])\n",
    "step = (max_snow-min_snow)/10\n",
    "\n",
    "for snow in snow_averages.index.tolist():\n",
    "    snow_averages.loc[snow, 'Snow Quant'] = int((snow_averages.loc[snow,'Speed']\n",
    "                                                 - min_snow)/step)\n",
    "    \n",
    "snow_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snow_averages.loc['Spring Conditions', 'Snow Quant'] = 6\n",
    "snow_averages.loc['Powder', 'Snow Quant'] = 4\n",
    "snow_averages.loc['Soft', 'Snow Quant'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_averages.to_pickle('weather_averages.pkl')\n",
    "snow_averages.to_pickle('snow_averages.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(mean_mens_sprint_speed)):\n",
    "    mean_mens_sprint_speed.loc[i,'Quant Weather'] = weather_averages.loc[\n",
    "                                mean_mens_sprint_speed.loc[i, 'Weather'], 'Weather Quant']\n",
    "    mean_mens_sprint_speed.loc[i, 'Quant Snow'] = snow_averages.loc[\n",
    "                                mean_mens_sprint_speed.loc[i, 'Snow Cond'], 'Snow Quant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And pickle it\n",
    "\n",
    "mean_mens_sprint_speed.to_pickle(\"mean_mens_sprint_speed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 3: Exploring correlation and impact\n",
    "\n",
    "The other thing that I want to do here is to compute the correlation coefficients and total changes explained for each of the quantitative predictor variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "corr_and_change : for a given pair of variables x and y in a dataframe df, determines the\n",
    "                  strength of their linear relationship and how much of a change in y is\n",
    "                  predicted based on the range given for x\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "x : the column to be treated as the predictor variable\n",
    "y : the column to be treated as the response variables\n",
    "df : the dataframe containing the data\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "[r, r_sq, change] : a list containing the correlation coefficient, the coefficient of \n",
    "                    determination, and the percent of the total change in y  predicted \n",
    "                    over the given range of x by the best fit line through the data\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def corr_and_change(x, y, df):\n",
    "    \n",
    "    correlation_output = stats.linregress(df[x],df[y])\n",
    "    \n",
    "    r = correlation_output[2]\n",
    "    r_sq = correlation_output[2]**2\n",
    "    \n",
    "    max_x = max(df[x])\n",
    "    min_x = min(df[x])\n",
    "    \n",
    "    change = max_x * correlation_output[0] - min_x * correlation_output[0]\n",
    "    total_change = max(df[y]) - min(df[y])\n",
    "    change = abs(change)/total_change\n",
    "\n",
    "    return [r, r_sq, change]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Predictor</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Determination</th>\n",
       "      <th>Percent Change Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Length</td>\n",
       "      <td>0.076656</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.074319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Height Diff</td>\n",
       "      <td>-0.191723</td>\n",
       "      <td>0.036758</td>\n",
       "      <td>0.177647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Max Climb</td>\n",
       "      <td>-0.095647</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.104154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Total Climb</td>\n",
       "      <td>-0.275609</td>\n",
       "      <td>0.075960</td>\n",
       "      <td>0.227985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Snow Temp</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.008645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Air Temp</td>\n",
       "      <td>-0.077797</td>\n",
       "      <td>0.006052</td>\n",
       "      <td>0.096905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>-0.208916</td>\n",
       "      <td>0.043646</td>\n",
       "      <td>0.177597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Wind</td>\n",
       "      <td>-0.109666</td>\n",
       "      <td>0.012027</td>\n",
       "      <td>0.133893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Altitude</td>\n",
       "      <td>-0.162561</td>\n",
       "      <td>0.026426</td>\n",
       "      <td>0.117719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Quant Year</td>\n",
       "      <td>0.231685</td>\n",
       "      <td>0.053678</td>\n",
       "      <td>0.159730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Quant Event</td>\n",
       "      <td>0.059486</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.041085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Quant Weather</td>\n",
       "      <td>0.401326</td>\n",
       "      <td>0.161063</td>\n",
       "      <td>0.536829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mean Speed</td>\n",
       "      <td>Quant Snow</td>\n",
       "      <td>0.452142</td>\n",
       "      <td>0.204432</td>\n",
       "      <td>0.602187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Response      Predictor  Correlation  Determination  \\\n",
       "0   Mean Speed         Length     0.076656       0.005876   \n",
       "1   Mean Speed    Height Diff    -0.191723       0.036758   \n",
       "2   Mean Speed      Max Climb    -0.095647       0.009148   \n",
       "3   Mean Speed    Total Climb    -0.275609       0.075960   \n",
       "4   Mean Speed      Snow Temp    -0.006824       0.000047   \n",
       "5   Mean Speed       Air Temp    -0.077797       0.006052   \n",
       "6   Mean Speed       Humidity    -0.208916       0.043646   \n",
       "7   Mean Speed           Wind    -0.109666       0.012027   \n",
       "8   Mean Speed       Altitude    -0.162561       0.026426   \n",
       "9   Mean Speed     Quant Year     0.231685       0.053678   \n",
       "10  Mean Speed    Quant Event     0.059486       0.003539   \n",
       "11  Mean Speed  Quant Weather     0.401326       0.161063   \n",
       "12  Mean Speed     Quant Snow     0.452142       0.204432   \n",
       "\n",
       "    Percent Change Predicted  \n",
       "0                   0.074319  \n",
       "1                   0.177647  \n",
       "2                   0.104154  \n",
       "3                   0.227985  \n",
       "4                   0.008645  \n",
       "5                   0.096905  \n",
       "6                   0.177597  \n",
       "7                   0.133893  \n",
       "8                   0.117719  \n",
       "9                   0.159730  \n",
       "10                  0.041085  \n",
       "11                  0.536829  \n",
       "12                  0.602187  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_and_changes_speed = []\n",
    "\n",
    "x_values = ['Length', 'Height Diff', 'Max Climb', 'Total Climb',  'Snow Temp', 'Air Temp', \n",
    "            'Humidity', 'Wind', 'Altitude', 'Quant Year', 'Quant Event', 'Quant Weather',\n",
    "            'Quant Snow']\n",
    "\n",
    "y_values = ['Mean Speed']\n",
    "\n",
    "for y in y_values:\n",
    "    for x in x_values:\n",
    "        row_data = [y,x]\n",
    "        row_data.extend(corr_and_change(x,y,mean_mens_sprint_speed.iloc[:96]))\n",
    "        corr_and_changes_speed.append(row_data)\n",
    "        \n",
    "corr_and_changes_speed = pd.DataFrame(corr_and_changes_speed, columns = ['Response',\n",
    "                     'Predictor','Correlation', 'Determination','Percent Change Predicted'])\n",
    "\n",
    "corr_and_changes_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, I want to pluck off the variables that do the best job of predicting...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "collect_best_variables : takes a dataframe produced by corr_and_change and chooses the\n",
    "                         predictor variables with the highest coefficients of determination\n",
    "                         and the highest percent change predicted (in both cases, with \n",
    "                         values at least half as large as the maximal values)\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : the dataframe containing the data\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "best_variables : a list of the variables that seem to best predict the response variable\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def collect_best_variables(df):\n",
    "    \n",
    "    best_variables = []\n",
    "    \n",
    "    df.sort_values('Determination', inplace = True, ascending = False)\n",
    "    \n",
    "    max_deter = df.iloc[0,3]\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i,3] >= 0.5*max_deter:\n",
    "            best_variables.append(df.iloc[i,1])\n",
    "            \n",
    "    df.sort_values('Percent Change Predicted', inplace = True, ascending = False)\n",
    "    \n",
    "    max_change = df.iloc[0,4]\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i,4] >= 0.5*max_change:\n",
    "            best_variables.append(df.iloc[i,1])\n",
    "\n",
    "    best_variables = list(set(best_variables))\n",
    "    \n",
    "    return best_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_variables_speed = collect_best_variables(corr_and_changes_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluating Impacts on Speed\n",
    "\n",
    "We observe here that the variables that do the best jobs individually of predicting speed (assuming that none of the other variables are available for consideration) are Quant(itative) Snow which has an $r$ value of 0.452142 and which explains roughly 60% of the total change in speed, and Quant(itative) Weather which has an $r$ value of 0.401326 and which explains roughly 54% of the total change in speed. There are, of course, some caveats.\n",
    "1. We defined the variables Quant(itative) Snow and Quant(itative) Weather by looking at the average speeds associated with each value taken by the variables Weather and Snow Conditions. While it is clear that these assignments did not yield perfect alignment (since if they did, we would expect the $r$ values to be much closer to 1 than they are), it is not surprising that that they did well relative to the other variables.\n",
    "2. The variables quant_snow in particular likely has some dependence on the other weather related variables, in particular on air temperature and snow temperature.\n",
    "\n",
    "If we eliminate those two variables from consideration, then our best variables are Total Climb with an $r$ value of -0.275609, Humidity with and $r$ value of -0.208916, and Height Diff(erential) with an $r$ value of -0.191723. Note that only the first of these values is even half as large as the values for Quant(itative) Snow and Quant(itative) Weather.\n",
    "\n",
    "Previous Section: [Effect of Conditions on Speed](#Effects-of-conditions-on-speed)\n",
    "\n",
    "\n",
    "Next Section: [Effect of Conditions on Shooting Accuracy](#Effects-of-conditions-on-shooting-accuracy)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of conditions on shooting accuracy\n",
    "\n",
    "#### Objective\n",
    "To understand the impact of individual course and weather condition variables on prone and standing shooting accuracy.\n",
    "\n",
    "#### Step 1 : Collecting the data\n",
    "\n",
    "Because I want to include a certain amount of uncertainty in this shooting data, I'm going to use bootstrap sampling to collect 100 samples (with replacement) from the shooting data for each race. For each of these, I'm going to calculate a mean, and I'm going to use these means to estimate the parameters (mean and standard deviation) of the prone and standing shooting accuracy for each race.\n",
    "\n",
    "1. ```bootstrap_sample_dist```: This function takes a competition analysis dataframe and the name of a column containing shooting data, then draws  100 bootstrap samples and finds their means. The function then converts these values from an average number of missed shots to a percentage of accurate shots. Finally, it returns the mean and standard deviation of this list.\n",
    "\n",
    "Now, we loop through all the World Cup men's sprint races, using ```bootstrap_sample_dist``` to estimate means and standard deviations for both prone and standing shooting. We then create a dataframe with the following columns: Year, Event, Prone Accuracy, Prone Dev, Standing Accuracy, and Standing Dev. As above, we use ```add_course_data``` and ```add_weather_data``` to add information about course conditions for each event, and then use the values we found when quantifying the snow conditions and weather to add Quant Snow and Quant Weather values to our dataframe.\n",
    "\n",
    "#### Step 2: Correlations\n",
    "\n",
    "In order to compare the effects of the various predictor variables on prone shooting we loop through all quantitative predictors (including the quantified versions of the categorical variables), applying ```corr_and_change``` each time. The results are then stored in a dataframe. We repeat this to explore the effects of the various predictor variables on standing shooting. Finally, we use the function ```collect_best_variables``` on each of the resulting dataframes to select which of the predictor variables seem to be most closely tied to prone shooting accuracy and standing shooting accuracy.\n",
    "\n",
    "Previous Section: [Evaluating Impacts on Speed](#Evaluating-Impacts-on-Speed)\n",
    "\n",
    "Next Section: [Evaluating Impacts on Shooting Accuracy](#Evaluating-Impacts-on-Accuracy)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Collecting the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "bootstrap_sample_dist : takes a companal dataframe and column name, then draws and finds\n",
    "                        the means of 100 bootstrap samples and returns the mean and standard\n",
    "                        deviation of the list of sample means\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a competition analysis dataframe\n",
    "shooting : the code for a column containing shot data ('P1' or 'S1')\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "average : the mean of the bootstrap sample means\n",
    "stdev : the standard deviation of the bootstrap sample means\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def bootstrap_sample_dist(df, shooting):\n",
    "    \n",
    "    bootstrap_means = []\n",
    "    \n",
    "    for i in range(100):\n",
    "        bootstrap_sample = np.random.choice((df[shooting].astype(float)), len(df))\n",
    "        bootstrap_means.append((5 - np.mean(bootstrap_sample))/5)\n",
    "        \n",
    "    average = np.mean(bootstrap_means)\n",
    "    stdev = np.std(bootstrap_means, ddof = 1)\n",
    "    \n",
    "    return average, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collecting the shooting data into a single dataframe\n",
    "\n",
    "shooting_accuracy = []\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910', '1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        filename = (\"companal_SMSP_%(season)s_%(event)s.pkl\" \n",
    "                    %{'season' : season, 'event' : event})\n",
    "        \n",
    "        try:\n",
    "            race_data = pd.read_pickle(filename)\n",
    "            if len(race_data) > 50:\n",
    "                shooting = [season, event]\n",
    "                shooting.extend(bootstrap_sample_dist(race_data, 'P1'))\n",
    "                shooting.extend(bootstrap_sample_dist(race_data, 'S1'))\n",
    "                shooting_accuracy.append(shooting)\n",
    "            \n",
    "        except: # there is no file for this race\n",
    "            pass\n",
    "            \n",
    "shooting_accuracy = pd.DataFrame(shooting_accuracy, columns = ['Year','Event', \n",
    "                        'Prone Accuracy', 'Prone Dev', 'Standing Accuracy','Standing Dev']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to add the weather and course data to this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in range(len(shooting_accuracy)):\n",
    "    shooting_accuracy = add_course_data(shooting_accuracy, row)\n",
    "    \n",
    "for row in range(len(shooting_accuracy)):\n",
    "    shooting_accuracy = add_weather_data(shooting_accuracy, row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(mean_mens_sprint_speed)):\n",
    "    shooting_accuracy.loc[i,'Quant Weather'] = weather_averages.loc[\n",
    "        shooting_accuracy.loc[i, 'Weather'], 'Weather Quant']\n",
    "    shooting_accuracy.loc[i, 'Quant Snow'] = snow_averages.loc[\n",
    "        shooting_accuracy.loc[i, 'Snow Cond'], 'Snow Quant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shooting_accuracy.to_pickle('shooting_accuracy.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Predictor</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Determination</th>\n",
       "      <th>Percent Change Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Length</td>\n",
       "      <td>-0.181564</td>\n",
       "      <td>0.032965</td>\n",
       "      <td>0.154681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Height Diff</td>\n",
       "      <td>-0.081244</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.066150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Max Climb</td>\n",
       "      <td>-0.298844</td>\n",
       "      <td>0.089308</td>\n",
       "      <td>0.285957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Total Climb</td>\n",
       "      <td>-0.058446</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.042483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Snow Temp</td>\n",
       "      <td>-0.059271</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.065983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Air Temp</td>\n",
       "      <td>-0.226165</td>\n",
       "      <td>0.051151</td>\n",
       "      <td>0.247550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>-0.171227</td>\n",
       "      <td>0.029319</td>\n",
       "      <td>0.127906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Wind</td>\n",
       "      <td>-0.249846</td>\n",
       "      <td>0.062423</td>\n",
       "      <td>0.268048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Altitude</td>\n",
       "      <td>-0.094331</td>\n",
       "      <td>0.008898</td>\n",
       "      <td>0.060026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Quant Year</td>\n",
       "      <td>0.305661</td>\n",
       "      <td>0.093428</td>\n",
       "      <td>0.185176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Quant Event</td>\n",
       "      <td>0.138914</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>0.084308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Quant Weather</td>\n",
       "      <td>0.499915</td>\n",
       "      <td>0.249915</td>\n",
       "      <td>0.587609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Prone Accuracy</td>\n",
       "      <td>Quant Snow</td>\n",
       "      <td>0.047994</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.056169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Response      Predictor  Correlation  Determination  \\\n",
       "0   Prone Accuracy         Length    -0.181564       0.032965   \n",
       "1   Prone Accuracy    Height Diff    -0.081244       0.006601   \n",
       "2   Prone Accuracy      Max Climb    -0.298844       0.089308   \n",
       "3   Prone Accuracy    Total Climb    -0.058446       0.003416   \n",
       "4   Prone Accuracy      Snow Temp    -0.059271       0.003513   \n",
       "5   Prone Accuracy       Air Temp    -0.226165       0.051151   \n",
       "6   Prone Accuracy       Humidity    -0.171227       0.029319   \n",
       "7   Prone Accuracy           Wind    -0.249846       0.062423   \n",
       "8   Prone Accuracy       Altitude    -0.094331       0.008898   \n",
       "9   Prone Accuracy     Quant Year     0.305661       0.093428   \n",
       "10  Prone Accuracy    Quant Event     0.138914       0.019297   \n",
       "11  Prone Accuracy  Quant Weather     0.499915       0.249915   \n",
       "12  Prone Accuracy     Quant Snow     0.047994       0.002303   \n",
       "\n",
       "    Percent Change Predicted  \n",
       "0                   0.154681  \n",
       "1                   0.066150  \n",
       "2                   0.285957  \n",
       "3                   0.042483  \n",
       "4                   0.065983  \n",
       "5                   0.247550  \n",
       "6                   0.127906  \n",
       "7                   0.268048  \n",
       "8                   0.060026  \n",
       "9                   0.185176  \n",
       "10                  0.084308  \n",
       "11                  0.587609  \n",
       "12                  0.056169  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_and_changes_prone_accuracy = []\n",
    "\n",
    "x_values = ['Length', 'Height Diff', 'Max Climb', 'Total Climb',  'Snow Temp', 'Air Temp', \n",
    "            'Humidity', 'Wind', 'Altitude', 'Quant Year', 'Quant Event', 'Quant Weather', \n",
    "            'Quant Snow']\n",
    "\n",
    "y_values = ['Prone Accuracy']\n",
    "\n",
    "for y in y_values:\n",
    "    for x in x_values:\n",
    "        row_data = [y,x]\n",
    "        row_data.extend(corr_and_change(x,y,shooting_accuracy.iloc[:96]))\n",
    "        corr_and_changes_prone_accuracy.append(row_data)\n",
    "        \n",
    "corr_and_changes_prone_accuracy = pd.DataFrame(corr_and_changes_prone_accuracy, \n",
    "                                         columns = ['Response','Predictor','Correlation',\n",
    "                                                'Determination','Percent Change Predicted'])\n",
    "\n",
    "\n",
    "corr_and_changes_prone_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Predictor</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Determination</th>\n",
       "      <th>Percent Change Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Length</td>\n",
       "      <td>-0.230931</td>\n",
       "      <td>0.053329</td>\n",
       "      <td>0.143778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Height Diff</td>\n",
       "      <td>-0.077414</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>0.046064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Max Climb</td>\n",
       "      <td>-0.209144</td>\n",
       "      <td>0.043741</td>\n",
       "      <td>0.146253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Total Climb</td>\n",
       "      <td>-0.116389</td>\n",
       "      <td>0.013546</td>\n",
       "      <td>0.061827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Snow Temp</td>\n",
       "      <td>0.039142</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.031844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Air Temp</td>\n",
       "      <td>0.052720</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.042171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>-0.129259</td>\n",
       "      <td>0.016708</td>\n",
       "      <td>0.070564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Wind</td>\n",
       "      <td>-0.411383</td>\n",
       "      <td>0.169236</td>\n",
       "      <td>0.322544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Altitude</td>\n",
       "      <td>-0.101569</td>\n",
       "      <td>0.010316</td>\n",
       "      <td>0.047233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Quant Year</td>\n",
       "      <td>0.128357</td>\n",
       "      <td>0.016476</td>\n",
       "      <td>0.056829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Quant Event</td>\n",
       "      <td>0.209197</td>\n",
       "      <td>0.043763</td>\n",
       "      <td>0.092786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Quant Weather</td>\n",
       "      <td>0.459026</td>\n",
       "      <td>0.210704</td>\n",
       "      <td>0.394305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Standing Accuracy</td>\n",
       "      <td>Quant Snow</td>\n",
       "      <td>0.152532</td>\n",
       "      <td>0.023266</td>\n",
       "      <td>0.130459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Response      Predictor  Correlation  Determination  \\\n",
       "0   Standing Accuracy         Length    -0.230931       0.053329   \n",
       "1   Standing Accuracy    Height Diff    -0.077414       0.005993   \n",
       "2   Standing Accuracy      Max Climb    -0.209144       0.043741   \n",
       "3   Standing Accuracy    Total Climb    -0.116389       0.013546   \n",
       "4   Standing Accuracy      Snow Temp     0.039142       0.001532   \n",
       "5   Standing Accuracy       Air Temp     0.052720       0.002779   \n",
       "6   Standing Accuracy       Humidity    -0.129259       0.016708   \n",
       "7   Standing Accuracy           Wind    -0.411383       0.169236   \n",
       "8   Standing Accuracy       Altitude    -0.101569       0.010316   \n",
       "9   Standing Accuracy     Quant Year     0.128357       0.016476   \n",
       "10  Standing Accuracy    Quant Event     0.209197       0.043763   \n",
       "11  Standing Accuracy  Quant Weather     0.459026       0.210704   \n",
       "12  Standing Accuracy     Quant Snow     0.152532       0.023266   \n",
       "\n",
       "    Percent Change Predicted  \n",
       "0                   0.143778  \n",
       "1                   0.046064  \n",
       "2                   0.146253  \n",
       "3                   0.061827  \n",
       "4                   0.031844  \n",
       "5                   0.042171  \n",
       "6                   0.070564  \n",
       "7                   0.322544  \n",
       "8                   0.047233  \n",
       "9                   0.056829  \n",
       "10                  0.092786  \n",
       "11                  0.394305  \n",
       "12                  0.130459  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_and_changes_standing_accuracy = []\n",
    "\n",
    "x_values = ['Length', 'Height Diff', 'Max Climb', 'Total Climb',  'Snow Temp', 'Air Temp', \n",
    "            'Humidity', 'Wind', 'Altitude', 'Quant Year', 'Quant Event', 'Quant Weather', \n",
    "            'Quant Snow']\n",
    "\n",
    "y_values = ['Standing Accuracy']\n",
    "\n",
    "for y in y_values:\n",
    "    for x in x_values:\n",
    "        row_data = [y,x]\n",
    "        row_data.extend(corr_and_change(x,y,shooting_accuracy.iloc[:96]))\n",
    "        corr_and_changes_standing_accuracy.append(row_data)\n",
    "        \n",
    "corr_and_changes_standing_accuracy = pd.DataFrame(corr_and_changes_standing_accuracy, \n",
    "                                         columns = ['Response','Predictor','Correlation',\n",
    "                                                 'Determination','Percent Change Predicted'])\n",
    "\n",
    "\n",
    "corr_and_changes_standing_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_variables_prone_accuracy = collect_best_variables(corr_and_changes_prone_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_variables_standing_accuracy = collect_best_variables(corr_and_changes_standing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluating Impacts on Accuracy\n",
    "\n",
    "Beginning with the impacts of our predictor variables on prone accuracy, we find that the variables with the greatest impact are\n",
    "- Quant Weather, which has an $r$ value of 0.499915 and which explains roughly 59% of the total change in prone accuracy,\n",
    "- Max Climb, which has an $r$ value of -0.298844 and which explains roughly 29% of the total change in prone accuracy,\n",
    "- Wind, which has an $r$ value of -0.249846 and which explains roughly 27% of the total change in prone accuracy, and\n",
    "- Air Temp, which has an $r$ value of -0.226165 and which explains roughly 25% of the total change in prone accuracy.\n",
    "\n",
    "In considering the impacts of our predictor variables on standing accuracy, we find that the variables with the greatest impact are\n",
    "- Quant Weather, which has an $r$ value of 0.459026 and which explains roughly 39% of the total change in standing accuracy, and \n",
    "- Wind, which has an $r$ value of -0.411383 and which explains roughly 32% of the total change in standing accuracy.\n",
    "\n",
    "One observation here is that, despite the fact that shooting accuracy was not considered in any way when assigning numeric values to the different values taken by Weather, Quant Weather remains the variable that is the single best predictor of prone accuracy by a significant amount, and the best predictor of standing accuracy, though it is not significantly larger than Wind in this case.\n",
    "\n",
    "Previous Section: [Effect of Conditions on Shooting Accuracy](#Effects-of-conditions-on-shooting-accuracy)\n",
    "\n",
    "\n",
    "Next Section:[Effect of Conditions on Range and Penalty Times](#Effects-of-conditions-on-range-and-penalty-times)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of conditions on range and penalty times\n",
    "\n",
    "#### Objective\n",
    "To investigate the effect of race conditions on average range times and penalty loop times.\n",
    "\n",
    "#### Step 0 : Calculate prone and standing range + penalty times in seconds\n",
    "\n",
    "Due to the fact that before the 2011-2012 season there was a single value given for each racer that encompassed both range and penalty times, whereas the two are split from the 2011-2012 season on, I combine range and penalty time into a single value, in seconds, for each of prone and standing shooting.\n",
    "\n",
    "1. ```rangetimes_to_seconds```: This function takes a competition analysis dataframe and uses ```convert_to_seconds``` to convert the prone and standing range times (for seasons through 1011) or the sum of the prone and standing range and penalty times (for seasons after 1112) into seconds. It records these values in the new columns Prone Range and Standing Range in the original dataframe, and then returns that dataframe.\n",
    "2. ```ibu_rangetimes_to_seconds```: This function is analogous to ```rangetimes_to_seconds```, with the main difference being that, since penalty times were not recorded separately for ibu races until the 2014-15 season, it converts the range times alone until the beginning of the 2014-15 season.\n",
    "\n",
    "\n",
    "\n",
    "#### Step 1: Collect the data\n",
    "\n",
    "Next, we wish to find values for four different things. The values that we have found above for prone range time and standing range time include all the time elapsed from the point when the racer enters the shooting area until the point where the racer leaves the penalty zone. During that time, the racer\n",
    "- skis to the shooting location (a short distance for standing shooting, a somewhat longer distance for prone shooting)\n",
    "- gets in position\n",
    "- takes 5 shots\n",
    "- skis to the entrance of the penalty zone\n",
    "- skis any penalty loops that are necessary due to missed shots\n",
    "- skis out of the penalty zone\n",
    "\n",
    "What we would like to do is estimate the time spent skiing penalty loops (which is dependent on the number of shots missed) and to isolate it from all the other pieces of the range time. To this end, we will henceforth use the term _range time_ to refer to everything that happens outside of the penalty loops, and _loop time_ to refer to the time required (on average) to ski a single penalty loop. \n",
    "\n",
    "(NB: This raises the question: If you're going to go and split up the range time and the penalty time now, why did you add them together up there? The answer is fairly simple. For the seasons where the penalty time is split off from the range time, the penalty time includes a short section of track/range that every racer needs to ski (which takes about 7 or 8 seconds, on average), regardless of whether or not they have missed shots. As a result, if we estimate loop time for those races by dividing the penalty times by the number of missed shots, our estimates will be higher than the actual loop times by 25-30%.)\n",
    "\n",
    "In order to estimate the average range times and penalty loop times for a given race, we use\n",
    "\n",
    "3. ```calculate_penalty_lines```: This function uses bootstrap sampling to select a collection of missed prone shots and prone range time (and  missed standing shots and standing range time). It then uses linear regression on each sample  to estimate range time and penalty loop time for a given competition. The function returns a list containing the means and standard deviations of the estimates found for prone range time, prone penalty loop time, standing range time, and standing penalty loop time.\n",
    "\n",
    "We then loop through all of the World Cup races, applying ```calculate_penalty_lines``` to each, and store the results in the form of a dataframe with the following columns: Year, Event, prone range mean, prone range dev, prone loop mean, prone loop dev, standing range mean, standing range dev, standing loop mean, and standing loop dev. Finally, as above, we use add_course_data and add_weather_data to add information about course conditions for each event, and then use the values we found when quantifying the snow conditions and weather to add Quant Snow and Quant Weather values to our dataframe.\n",
    "\n",
    "#### Step 2: Correlations for range times\n",
    "\n",
    "In order to compare the effects of the various predictor variables on prone range times we loop through all quantitative predictors (including the quantified versions of the categorical variables), applying corr_and_change each time. The results are then stored in a dataframe. We repeat this to explore the effects of the various predictor variables on standing range times. Finally, we use the function collect_best_variables on each of the resulting dataframes to select which of the predictor variables seem to be most closely tied to prone range times and standing range times.\n",
    "\n",
    "#### Step 3: Correlations for penalty loop times\n",
    "\n",
    "In order to compare the effects of the various predictor variables on prone penalty loop times we loop through all quantitative predictors (including the quantified versions of the categorical variables), applying corr_and_change each time. The results are then stored in a dataframe. We repeat this to explore the effects of the various predictor variables on standing penalty loop times. Finally, we use the function collect_best_variables on each of the resulting dataframes to select which of the predictor variables seem to be most closely tied to prone penalty loop times and standing penalty loop times.\n",
    "\n",
    "Previous Section: [Evaluating Impacts on Shooting Accuracy](#Evaluating-Impacts-on-Accuracy)\n",
    "\n",
    "\n",
    "Next Section: [Evaluating Impacts on Range Times](#Evaluating-Impacts-on-Range-Times)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 0 : Calculate prone and standing range + penalty times in seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "rangetimes_to_seconds : takes a competition analysis dataframe and converts the range times\n",
    "                        (for seasons through 1011) or the sum of the range and penalty times\n",
    "                        (for seasons after 1112) into seconds\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a competition analysis dataframe\n",
    "year : the season in which the competition was held\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : the original dataframe with range times given in seconds \n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def rangetimes_to_seconds(df, year):\n",
    "    \n",
    "    if year in ['0405','0506','0607','0708','0809','0910','1011']:\n",
    "        for i in df.index.tolist():\n",
    "            df.loc[i,'prone range'] = convert_to_seconds(df.loc[i,'r1'])\n",
    "            df.loc[i, 'standing range'] = convert_to_seconds(df.loc[i,'r2'])\n",
    "    else:\n",
    "        for i in df.index.tolist():\n",
    "            df.loc[i,'prone range'] = (convert_to_seconds(df.loc[i,'r1']) + \n",
    "                                                   convert_to_seconds(df.loc[i,'pen1']))\n",
    "            df.loc[i,'standing range'] = (convert_to_seconds(df.loc[i,'r2']) + \n",
    "                                                   convert_to_seconds(df.loc[i,'pen2']))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding range times in seconds to each event dataframe\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        filename = ('companal_SMSP_%(season)s_%(event)s.pkl'\n",
    "                    %{'season' : season, 'event' : event})\n",
    "        \n",
    "        try:\n",
    "            race_data = pd.read_pickle(filename)\n",
    "            race_data_new = rangetimes_to_seconds(race_data,season)\n",
    "            race_data_new.to_pickle(filename)\n",
    "        except: # the race does not exist or does not have data\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_rangetimes_to_seconds : takes an ibu competition analysis dataframe and converts the\n",
    "                            range times (for seasons through 1314) or the sum of the range\n",
    "                            and penalty times (for seasons after 1415) into seconds\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a competition analysis dataframe\n",
    "year : the season in which the competition was held\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "df : the original dataframe with range times given in seconds \n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ibu_rangetimes_to_seconds(df, year):\n",
    "    \n",
    "    if year in ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314']:\n",
    "        for i in df.index.tolist():\n",
    "            df.loc[i,'prone range'] = convert_to_seconds(df.loc[i,'r1'])\n",
    "            df.loc[i, 'standing range'] = convert_to_seconds(df.loc[i,'r2'])\n",
    "    else:\n",
    "        for i in df.index.tolist():\n",
    "            df.loc[i,'prone range'] = (convert_to_seconds(df.loc[i,'r1']) + \n",
    "                                                   convert_to_seconds(df.loc[i,'pen1']))\n",
    "            df.loc[i,'standing range'] = (convert_to_seconds(df.loc[i,'r2']) + \n",
    "                                                      convert_to_seconds(df.loc[i,'pen2']))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding range times in seconds to each event dataframe\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CH__']\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        filename = 'ibu_SMSP_%(season)s_%(event)s.pkl' %{'season' : season, 'event' : event}\n",
    "        \n",
    "        try:\n",
    "            race_data = pd.read_pickle(filename)\n",
    "            race_data_new = ibu_rangetimes_to_seconds(race_data,season)\n",
    "            race_data_new.to_pickle(filename)\n",
    "        except: # the race does not exist or does not have data\n",
    "            pass\n",
    "        \n",
    "        filename = 'ibu_SMSPS_%(season)s_%(event)s.pkl' %{'season' : season, 'event' : event}\n",
    "        \n",
    "        try:\n",
    "            race_data = pd.read_pickle(filename)\n",
    "            race_data_new = ibu_rangetimes_to_seconds(race_data,season)\n",
    "            race_data_new.to_pickle(filename)\n",
    "        except: # the race does not exist or does not have data\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 1 : Collect the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "calculate_penalty_lines : uses bootstrap sampling and linear regression to estimate range time\n",
    "                            and penalty loop time for a given competition\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "season : the years of the biathlon season, in form y1y2 where y1 is the last 2 digits of the\n",
    "        first year and y2 is the last two digits of the second year\n",
    "event : a four character code specifying the event. Possibilities are “CP01”, “CP02”, . . .,\n",
    "        “CP09”, “OG__”,”CH__”\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "penalty_data : a list containing the averages and standard deviations of the predicted\n",
    "                range and loop times for prone and standing shooting for the given\n",
    "                competition\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def calculate_penalty_lines(season,event):\n",
    "    \n",
    "    filename = 'companal_SMSP_%(season)s_%(event)s.pkl' %{'event' : event, 'season' : season}\n",
    "    \n",
    "    race_data = pd.read_pickle(filename)\n",
    "    \n",
    "    bootstrap_data = []\n",
    "    \n",
    "    linreg_prone = LinearRegression()\n",
    "    linreg_stand = LinearRegression()\n",
    "    \n",
    "    for i in range(100):\n",
    "        try: # because it's possible that somehow all of the racers in a sample will \n",
    "             #have the same missed shots\n",
    "            bootstrap_sample = np.random.choice(race_data.index.tolist(), \n",
    "                                                            len(race_data)).tolist()\n",
    "            df = race_data.loc[bootstrap_sample]\n",
    "        \n",
    "            linreg_prone.fit(df['P1'].values.reshape(-1, 1), \n",
    "                                         df['prone range'].values.reshape(-1, 1))\n",
    "            linreg_stand.fit(df['S1'].values.reshape(-1, 1), \n",
    "                                         df['standing range'].values.reshape(-1, 1))\n",
    "        \n",
    "            fit_data = [linreg_prone.intercept_.tolist()[0], \n",
    "                            linreg_prone.coef_.tolist()[0][0], \n",
    "                                linreg_stand.intercept_.tolist()[0],\n",
    "                                    linreg_stand.coef_.tolist()[0][0]]\n",
    "\n",
    "            bootstrap_data.append(fit_data)\n",
    "        except: # If all racers in the bootstrap sample missed the same number of shots\n",
    "            pass\n",
    "        \n",
    "    bootstrap_data = pd.DataFrame(bootstrap_data, columns = ['prone range','prone loop',\n",
    "                                                             'standing range','standing loop'])\n",
    "    penalty_data = [np.mean(bootstrap_data['prone range']), \n",
    "                            np.std(bootstrap_data['prone range']),\n",
    "                   np.mean(bootstrap_data['prone loop']), \n",
    "                            np.std(bootstrap_data['prone loop']),\n",
    "                   np.mean(bootstrap_data['standing range']), \n",
    "                            np.std(bootstrap_data['standing range']),\n",
    "                   np.mean(bootstrap_data['standing loop']), \n",
    "                            np.std(bootstrap_data['standing loop'])]\n",
    "    \n",
    "    return penalty_data\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collecting the range and penalty data into a single dataframe\n",
    "\n",
    "penalties = []\n",
    "\n",
    "seasons = ['0405', '0506', '0607', '0708', '0809', '0910', '1011', '1112', '1213', '1314',\n",
    "           '1415', '1516', '1617', '1718']\n",
    "events = ['CP01', 'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', \n",
    "          'CH__', 'OG__']\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        try:\n",
    "            filename = ('companal_SMSP_%(season)s_%(event)s.pkl'\n",
    "                        %{'event' : event, 'season' : season})\n",
    "            if len(pd.read_pickle(filename)) > 50:\n",
    "                penalty = [season, event]\n",
    "                penalty.extend(calculate_penalty_lines(season, event))\n",
    "                penalties.append(penalty)\n",
    "            \n",
    "        except: # there is no file for this race\n",
    "            pass\n",
    "            \n",
    "penalty_info = pd.DataFrame(penalties, columns = ['Year', 'Event', 'prone range mean', \n",
    "                                                  'prone range dev', 'prone loop mean',\n",
    "                                                  'prone loop dev', 'standing range mean', \n",
    "                                                  'standing range dev', 'standing loop mean',\n",
    "                                                  'standing loop dev'])           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to add the weather and course information to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in range(len(penalty_info)):\n",
    "    penalty_info = add_course_data(penalty_info, row)\n",
    "    \n",
    "for row in range(len(penalty_info)):\n",
    "    penalty_info = add_weather_data(penalty_info, row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(penalty_info)):\n",
    "    penalty_info.loc[i,'Quant Weather'] = (weather_averages.loc[penalty_info.loc[i, 'Weather'],\n",
    "                                                               'Weather Quant'])\n",
    "    penalty_info.loc[i, 'Quant Snow'] = (snow_averages.loc[penalty_info.loc[i, 'Snow Cond'],\n",
    "                                                          'Snow Quant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And, just in case there are some NaN values, let's get rid of them\n",
    "\n",
    "penalty_info.dropna(how = 'any', axis = 'rows', inplace = True)\n",
    "penalty_info.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "penalty_info.to_pickle('penalty_info.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Correlations for range times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Predictor</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Determination</th>\n",
       "      <th>Percent Change Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Length</td>\n",
       "      <td>-0.028273</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Height Diff</td>\n",
       "      <td>0.123728</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.114583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Max Climb</td>\n",
       "      <td>0.030330</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.033009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Total Climb</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.053919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Snow Temp</td>\n",
       "      <td>-0.246668</td>\n",
       "      <td>0.060845</td>\n",
       "      <td>0.312329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Air Temp</td>\n",
       "      <td>-0.232323</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.289230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>0.096898</td>\n",
       "      <td>0.009389</td>\n",
       "      <td>0.082328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Wind</td>\n",
       "      <td>0.113187</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>0.138119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Altitude</td>\n",
       "      <td>-0.005786</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Quant Year</td>\n",
       "      <td>-0.239406</td>\n",
       "      <td>0.057315</td>\n",
       "      <td>0.164966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Quant Event</td>\n",
       "      <td>-0.228858</td>\n",
       "      <td>0.052376</td>\n",
       "      <td>0.157981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Quant Weather</td>\n",
       "      <td>-0.278661</td>\n",
       "      <td>0.077652</td>\n",
       "      <td>0.372549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>prone range mean</td>\n",
       "      <td>Quant Snow</td>\n",
       "      <td>-0.220391</td>\n",
       "      <td>0.048572</td>\n",
       "      <td>0.293372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Response      Predictor  Correlation  Determination  \\\n",
       "0   prone range mean         Length    -0.028273       0.000799   \n",
       "1   prone range mean    Height Diff     0.123728       0.015309   \n",
       "2   prone range mean      Max Climb     0.030330       0.000920   \n",
       "3   prone range mean    Total Climb     0.065217       0.004253   \n",
       "4   prone range mean      Snow Temp    -0.246668       0.060845   \n",
       "5   prone range mean       Air Temp    -0.232323       0.053974   \n",
       "6   prone range mean       Humidity     0.096898       0.009389   \n",
       "7   prone range mean           Wind     0.113187       0.012811   \n",
       "8   prone range mean       Altitude    -0.005786       0.000033   \n",
       "9   prone range mean     Quant Year    -0.239406       0.057315   \n",
       "10  prone range mean    Quant Event    -0.228858       0.052376   \n",
       "11  prone range mean  Quant Weather    -0.278661       0.077652   \n",
       "12  prone range mean     Quant Snow    -0.220391       0.048572   \n",
       "\n",
       "    Percent Change Predicted  \n",
       "0                   0.027397  \n",
       "1                   0.114583  \n",
       "2                   0.033009  \n",
       "3                   0.053919  \n",
       "4                   0.312329  \n",
       "5                   0.289230  \n",
       "6                   0.082328  \n",
       "7                   0.138119  \n",
       "8                   0.004188  \n",
       "9                   0.164966  \n",
       "10                  0.157981  \n",
       "11                  0.372549  \n",
       "12                  0.293372  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_and_changes_prone_range = []\n",
    "\n",
    "x_values = ['Length', 'Height Diff', 'Max Climb', 'Total Climb',  'Snow Temp', 'Air Temp', \n",
    "            'Humidity', 'Wind', 'Altitude', 'Quant Year', 'Quant Event', 'Quant Weather',\n",
    "            'Quant Snow']\n",
    "\n",
    "y_values = ['prone range mean']\n",
    "\n",
    "for y in y_values:\n",
    "    for x in x_values:\n",
    "        row_data = [y,x]\n",
    "        row_data.extend(corr_and_change(x,y,penalty_info.iloc[:96]))\n",
    "        corr_and_changes_prone_range.append(row_data)\n",
    "        \n",
    "corr_and_changes_prone_range = pd.DataFrame(corr_and_changes_prone_range, \n",
    "                                      columns = ['Response','Predictor','Correlation',\n",
    "                                                'Determination','Percent Change Predicted'])\n",
    "\n",
    "corr_and_changes_prone_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Predictor</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Determination</th>\n",
       "      <th>Percent Change Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Length</td>\n",
       "      <td>-0.018747</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.015428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Height Diff</td>\n",
       "      <td>0.074434</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.058545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Max Climb</td>\n",
       "      <td>-0.004905</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.004534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Total Climb</td>\n",
       "      <td>0.065230</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.045803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Snow Temp</td>\n",
       "      <td>-0.257711</td>\n",
       "      <td>0.066415</td>\n",
       "      <td>0.277140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Air Temp</td>\n",
       "      <td>-0.207546</td>\n",
       "      <td>0.043075</td>\n",
       "      <td>0.219449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>0.143971</td>\n",
       "      <td>0.020728</td>\n",
       "      <td>0.103890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Wind</td>\n",
       "      <td>0.130585</td>\n",
       "      <td>0.017052</td>\n",
       "      <td>0.135336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Altitude</td>\n",
       "      <td>0.096176</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.059120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Quant Year</td>\n",
       "      <td>-0.194929</td>\n",
       "      <td>0.037997</td>\n",
       "      <td>0.114078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Quant Event</td>\n",
       "      <td>-0.286692</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>0.168081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Quant Weather</td>\n",
       "      <td>-0.420419</td>\n",
       "      <td>0.176752</td>\n",
       "      <td>0.477372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>standing range mean</td>\n",
       "      <td>Quant Snow</td>\n",
       "      <td>-0.282148</td>\n",
       "      <td>0.079607</td>\n",
       "      <td>0.318984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Response      Predictor  Correlation  Determination  \\\n",
       "0   standing range mean         Length    -0.018747       0.000351   \n",
       "1   standing range mean    Height Diff     0.074434       0.005540   \n",
       "2   standing range mean      Max Climb    -0.004905       0.000024   \n",
       "3   standing range mean    Total Climb     0.065230       0.004255   \n",
       "4   standing range mean      Snow Temp    -0.257711       0.066415   \n",
       "5   standing range mean       Air Temp    -0.207546       0.043075   \n",
       "6   standing range mean       Humidity     0.143971       0.020728   \n",
       "7   standing range mean           Wind     0.130585       0.017052   \n",
       "8   standing range mean       Altitude     0.096176       0.009250   \n",
       "9   standing range mean     Quant Year    -0.194929       0.037997   \n",
       "10  standing range mean    Quant Event    -0.286692       0.082192   \n",
       "11  standing range mean  Quant Weather    -0.420419       0.176752   \n",
       "12  standing range mean     Quant Snow    -0.282148       0.079607   \n",
       "\n",
       "    Percent Change Predicted  \n",
       "0                   0.015428  \n",
       "1                   0.058545  \n",
       "2                   0.004534  \n",
       "3                   0.045803  \n",
       "4                   0.277140  \n",
       "5                   0.219449  \n",
       "6                   0.103890  \n",
       "7                   0.135336  \n",
       "8                   0.059120  \n",
       "9                   0.114078  \n",
       "10                  0.168081  \n",
       "11                  0.477372  \n",
       "12                  0.318984  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_and_changes_standing_range = []\n",
    "\n",
    "x_values = ['Length', 'Height Diff', 'Max Climb', 'Total Climb',  'Snow Temp', 'Air Temp', \n",
    "            'Humidity', 'Wind', 'Altitude', 'Quant Year', 'Quant Event', 'Quant Weather',\n",
    "            'Quant Snow']\n",
    "\n",
    "y_values = ['standing range mean']\n",
    "\n",
    "for y in y_values:\n",
    "    for x in x_values:\n",
    "        row_data = [y,x]\n",
    "        row_data.extend(corr_and_change(x,y,penalty_info.iloc[:96]))\n",
    "        corr_and_changes_standing_range.append(row_data)\n",
    "        \n",
    "corr_and_changes_standing_range = pd.DataFrame(corr_and_changes_standing_range, \n",
    "                                      columns = ['Response','Predictor','Correlation',\n",
    "                                                  'Determination','Percent Change Predicted'])\n",
    "\n",
    "corr_and_changes_standing_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_variables_prone_range = collect_best_variables(corr_and_changes_prone_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_variables_standing_range = collect_best_variables(corr_and_changes_standing_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 3: Correlations for penalty loop times\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Predictor</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Determination</th>\n",
       "      <th>Percent Change Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Length</td>\n",
       "      <td>-0.004694</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.004107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Height Diff</td>\n",
       "      <td>0.043059</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.036008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Max Climb</td>\n",
       "      <td>0.069018</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.067828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Total Climb</td>\n",
       "      <td>0.101540</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.075805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Snow Temp</td>\n",
       "      <td>-0.103892</td>\n",
       "      <td>0.010794</td>\n",
       "      <td>0.118784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Air Temp</td>\n",
       "      <td>-0.067470</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>0.075847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>0.114134</td>\n",
       "      <td>0.013027</td>\n",
       "      <td>0.087564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Wind</td>\n",
       "      <td>0.180649</td>\n",
       "      <td>0.032634</td>\n",
       "      <td>0.199051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Altitude</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.051352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Quant Year</td>\n",
       "      <td>-0.171745</td>\n",
       "      <td>0.029496</td>\n",
       "      <td>0.106861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Quant Event</td>\n",
       "      <td>-0.193010</td>\n",
       "      <td>0.037253</td>\n",
       "      <td>0.120307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Quant Weather</td>\n",
       "      <td>-0.359824</td>\n",
       "      <td>0.129473</td>\n",
       "      <td>0.434383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>prone loop mean</td>\n",
       "      <td>Quant Snow</td>\n",
       "      <td>-0.299457</td>\n",
       "      <td>0.089675</td>\n",
       "      <td>0.359945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Response      Predictor  Correlation  Determination  \\\n",
       "0   prone loop mean         Length    -0.004694       0.000022   \n",
       "1   prone loop mean    Height Diff     0.043059       0.001854   \n",
       "2   prone loop mean      Max Climb     0.069018       0.004763   \n",
       "3   prone loop mean    Total Climb     0.101540       0.010310   \n",
       "4   prone loop mean      Snow Temp    -0.103892       0.010794   \n",
       "5   prone loop mean       Air Temp    -0.067470       0.004552   \n",
       "6   prone loop mean       Humidity     0.114134       0.013027   \n",
       "7   prone loop mean           Wind     0.180649       0.032634   \n",
       "8   prone loop mean       Altitude     0.078575       0.006174   \n",
       "9   prone loop mean     Quant Year    -0.171745       0.029496   \n",
       "10  prone loop mean    Quant Event    -0.193010       0.037253   \n",
       "11  prone loop mean  Quant Weather    -0.359824       0.129473   \n",
       "12  prone loop mean     Quant Snow    -0.299457       0.089675   \n",
       "\n",
       "    Percent Change Predicted  \n",
       "0                   0.004107  \n",
       "1                   0.036008  \n",
       "2                   0.067828  \n",
       "3                   0.075805  \n",
       "4                   0.118784  \n",
       "5                   0.075847  \n",
       "6                   0.087564  \n",
       "7                   0.199051  \n",
       "8                   0.051352  \n",
       "9                   0.106861  \n",
       "10                  0.120307  \n",
       "11                  0.434383  \n",
       "12                  0.359945  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_and_changes_prone_loop = []\n",
    "\n",
    "x_values = ['Length', 'Height Diff', 'Max Climb', 'Total Climb',  'Snow Temp', 'Air Temp', \n",
    "            'Humidity', 'Wind', 'Altitude', 'Quant Year','Quant Event', 'Quant Weather', \n",
    "            'Quant Snow']\n",
    "\n",
    "y_values = ['prone loop mean']\n",
    "\n",
    "for y in y_values:\n",
    "    for x in x_values:\n",
    "        row_data = [y,x]\n",
    "        row_data.extend(corr_and_change(x,y,penalty_info.iloc[:96]))\n",
    "        corr_and_changes_prone_loop.append(row_data)\n",
    "        \n",
    "corr_and_changes_prone_loop = pd.DataFrame(corr_and_changes_prone_loop, \n",
    "                                           columns = ['Response','Predictor','Correlation',\n",
    "                                                 'Determination','Percent Change Predicted'])\n",
    "\n",
    "corr_and_changes_prone_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Predictor</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Determination</th>\n",
       "      <th>Percent Change Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Length</td>\n",
       "      <td>-0.012883</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.008422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Height Diff</td>\n",
       "      <td>0.097568</td>\n",
       "      <td>0.009519</td>\n",
       "      <td>0.060958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Max Climb</td>\n",
       "      <td>0.213295</td>\n",
       "      <td>0.045495</td>\n",
       "      <td>0.156611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Total Climb</td>\n",
       "      <td>0.099593</td>\n",
       "      <td>0.009919</td>\n",
       "      <td>0.055550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Snow Temp</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.005213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Air Temp</td>\n",
       "      <td>-0.021165</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.017776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Humidity</td>\n",
       "      <td>0.128615</td>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.073722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Wind</td>\n",
       "      <td>0.159715</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.131483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Altitude</td>\n",
       "      <td>0.143037</td>\n",
       "      <td>0.020460</td>\n",
       "      <td>0.069842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Quant Year</td>\n",
       "      <td>-0.059052</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.027451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Quant Event</td>\n",
       "      <td>-0.098729</td>\n",
       "      <td>0.009748</td>\n",
       "      <td>0.045978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Quant Weather</td>\n",
       "      <td>-0.287663</td>\n",
       "      <td>0.082750</td>\n",
       "      <td>0.259454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>standing loop mean</td>\n",
       "      <td>Quant Snow</td>\n",
       "      <td>-0.279078</td>\n",
       "      <td>0.077885</td>\n",
       "      <td>0.250623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Response      Predictor  Correlation  Determination  \\\n",
       "0   standing loop mean         Length    -0.012883       0.000166   \n",
       "1   standing loop mean    Height Diff     0.097568       0.009519   \n",
       "2   standing loop mean      Max Climb     0.213295       0.045495   \n",
       "3   standing loop mean    Total Climb     0.099593       0.009919   \n",
       "4   standing loop mean      Snow Temp    -0.006103       0.000037   \n",
       "5   standing loop mean       Air Temp    -0.021165       0.000448   \n",
       "6   standing loop mean       Humidity     0.128615       0.016542   \n",
       "7   standing loop mean           Wind     0.159715       0.025509   \n",
       "8   standing loop mean       Altitude     0.143037       0.020460   \n",
       "9   standing loop mean     Quant Year    -0.059052       0.003487   \n",
       "10  standing loop mean    Quant Event    -0.098729       0.009748   \n",
       "11  standing loop mean  Quant Weather    -0.287663       0.082750   \n",
       "12  standing loop mean     Quant Snow    -0.279078       0.077885   \n",
       "\n",
       "    Percent Change Predicted  \n",
       "0                   0.008422  \n",
       "1                   0.060958  \n",
       "2                   0.156611  \n",
       "3                   0.055550  \n",
       "4                   0.005213  \n",
       "5                   0.017776  \n",
       "6                   0.073722  \n",
       "7                   0.131483  \n",
       "8                   0.069842  \n",
       "9                   0.027451  \n",
       "10                  0.045978  \n",
       "11                  0.259454  \n",
       "12                  0.250623  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_and_changes_standing_loop = []\n",
    "\n",
    "x_values = ['Length', 'Height Diff', 'Max Climb', 'Total Climb',  'Snow Temp', 'Air Temp', \n",
    "            'Humidity', 'Wind', 'Altitude', 'Quant Year','Quant Event', 'Quant Weather', \n",
    "            'Quant Snow']\n",
    "\n",
    "y_values = ['standing loop mean']\n",
    "\n",
    "for y in y_values:\n",
    "    for x in x_values:\n",
    "        row_data = [y,x]\n",
    "        row_data.extend(corr_and_change(x,y,penalty_info.iloc[:96]))\n",
    "        corr_and_changes_standing_loop.append(row_data)\n",
    "        \n",
    "corr_and_changes_standing_loop = pd.DataFrame(corr_and_changes_standing_loop, \n",
    "                                           columns = ['Response','Predictor','Correlation',\n",
    "                                                  'Determination','Percent Change Predicted'])\n",
    "\n",
    "corr_and_changes_standing_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_variables_prone_loop = collect_best_variables(corr_and_changes_prone_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_variables_standing_loop = collect_best_variables(corr_and_changes_standing_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluating Impacts on Range Times\n",
    "\n",
    "Beginning with the impacts of our predictor variables on prone range times, we find that the variables with the greatest impact are\n",
    "<!--, which has an $r$ value of 0.499915 and which explains roughly 59% of the total change in prone accuracy,-->\n",
    "- Quant Weather, which has an $r$ value of -0.278661 and which explains roughly 37% of the total change in prone range times,\n",
    "- Snow Temp, which has an $r$ value of -0.246668 and which explains roughly 31% of the total change in prone range times,\n",
    "- Quant Year, which has an $r$ value of -0.239406 and which explains roughly 16% of the total change in prone range times,\n",
    "- Air Temp, which has an $r$ value of -0.232323 and which explains roughly 29% of the total change in prone range times,\n",
    "- Quant Event, which has an $r$ value of -0.228858 and which explains roughly 16% of the total change in prone range times, and\n",
    "- Quant Snow, which has an $r$ value of -0.220391 and which explains roughly 29% of the total change in prone range times.\n",
    "  \n",
    "In considering the impacts of our predictor variables on standing range times, we find that the variables with the greatest impact are\n",
    "- Quant Weather, which has an $r$ value of -0.420419 and which explains roughly 47% of the total change in standing range times,\n",
    "- Quant Snow, which has an $r$ value of -0.282148 and which explains roughly 32% of the total change in standing range times, and\n",
    "- Snow Temp, which has an $r$ value of -0.257711 and which explains roughly 28% of the total change in standing range times.\n",
    "  \n",
    "One observation here is that, despite the fact that range times were not considered in any way when assigning numeric values to the different values taken by Weather and Snow Conditions, Quant Weather and Quant Snow remain variables that are highly (relatively speaking) correlated with both prone and standing range times.\n",
    "\n",
    "Previous Section: [Effect of Conditions on Range and Penalty Times](#Effects-of-conditions-on-range-and-penalty-times)\n",
    "\n",
    "Next Section: [Evaluating Impacts on Penalty Loop Times](#Evaluating-Impacts-on-Penalty-Loop-Times)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluating Impacts on Penalty Loop Times\n",
    "\n",
    "Beginning with the impacts of our predictor variables on prone penalty loop times, we find that the variables with the greatest impact are\n",
    "<!--, which has an $r$ value of 0.499915 and which explains roughly 59% of the total change in prone accuracy,-->\n",
    "- Quant Weather, which has an $r$ value of -0.359824 and which explains roughly 43% of the total change in prone loop times, and\n",
    "- Quant Snow, which has an $r$ value of -0.299457 and which explains roughly 36% of the total change in prone loop times.\n",
    "\n",
    "In considering the impacts of our predictor variables on standing range times, we find that the variables with the greatest impact are\n",
    "- Quant Weather, which has an $r$ value of -0.287663 and which explains roughly 26% of the total change in standing loop times\n",
    "- Quant Snow, which has an $r$ value of -0.279078 and which explains roughly 25% of the total change in standing loop times\n",
    "- Max Climb, which has an $r$ value of 0.213295 and which explains roughly 16% of the total change in standing loop times\n",
    "- Wind, which has an $r$ value of 0.159715 and which explains roughly 13% of the total change in standing loop times\n",
    "\n",
    "One observation here is that, despite the fact that penalty loop times were not considered in any way when assigning numeric values to the different values taken by Weather and Snow Conditions, Quant Weather and Quant Snow remain variables that are highly (relatively speaking) correlated with both prone and standing penalty loop times.\n",
    "\n",
    "Previous Section: [Evaluating Impacts on Range Times](#Evaluating-Impacts-on-Range-Times)\n",
    "\n",
    "\n",
    "Next Section: [Handing off to the next notebook](#Handing-off-to-the-next-notebook)\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Handing off to the next notebook\n",
    "\n",
    "In order to prevent individual notebooks from becoming too unwieldy, I'm choosing to split this project over multiple notebooks. At this point, I have completed the following tasks:\n",
    "1. Read in the competition analysis files for all World Cup and IBU Cup men's sprint races from 2004-05 to 2017-18, parsed their contents, and stored the desired data in dataframes.\n",
    "2. Read in the competition data summary files for all World Cup and IBU Cup men's sprint races over the same period, parsed their contents, and stored the desired data in dataframes.\n",
    "3. Investigated the impact of each of the recorded conditions (length, wind, snow temperature, etc) on the various aspects of the race (speed, accuracy, range times, penalty loop times) and isolated the variables that seemed to have the most individual impact on each aspect.\n",
    "\n",
    "In the next notebook, I intend to use build models for each aspect, which will use results of previous races to predict performance in the race of interest. In each case, I will use the similarity between conditions in races to weight the importance of each previous race for predicting the outcome of the current race. Furthermore, since it appears from the results found in this notebook that some condition variables are far more important than others for making these predictions, I'm going to isolate individual variables when considering these weightings. In other words, I'm going to test my model with a weighting that is based entirely on how similar the course lengths are, then test it with a weighting based entirely on altitude, and so on until I've used each individual predictor variable as a basis for a weighting. I'm then going to determine which of the variables did the best job of predicting outcomes, and save those for future use.\n",
    "\n",
    "The last thing that I need to do before finishing up this notebook is to collect all of my best variables in a dictionary and then to pickle them so that they will available for use in the following and subsequent notebooks.\n",
    "\n",
    "Previous Section: [Evaluating Impacts on Penalty Loop Times](#Evaluating-Impacts-on-Penalty-Loop-Times)\n",
    "\n",
    "\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_variables = {'speed' : best_variables_speed, 'prone_acc' : best_variables_prone_accuracy,\n",
    "                  'standing_acc' : best_variables_standing_accuracy, \n",
    "                  'prone_range' : best_variables_prone_range,\n",
    "                  'standing_range' : best_variables_standing_range,\n",
    "                  'prone_loop' : best_variables_prone_loop,\n",
    "                  'standing_loop' : best_variables_standing_loop}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('best_variables.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_variables, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prone_acc': ['Quant Weather'],\n",
       " 'prone_loop': ['Quant Weather', 'Quant Snow'],\n",
       " 'prone_range': ['Snow Temp',\n",
       "  'Quant Snow',\n",
       "  'Air Temp',\n",
       "  'Quant Event',\n",
       "  'Quant Year',\n",
       "  'Quant Weather'],\n",
       " 'speed': ['Quant Snow', 'Quant Weather'],\n",
       " 'standing_acc': ['Quant Weather', 'Wind'],\n",
       " 'standing_loop': ['Quant Weather', 'Quant Snow', 'Max Climb', 'Wind'],\n",
       " 'standing_range': ['Quant Weather', 'Quant Snow', 'Snow Temp']}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
