{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc\"></a>\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "<a href=\"#introduction\">Introduction</a>\n",
    "\n",
    "<a href=\"#adding_total_times\">Adding Total Times</a>\n",
    "\n",
    "<a href=\"#functions\">Prediction Functions</a>\n",
    "\n",
    "<a href=\"#eval_functions\">Functions for Evaluation</a>\n",
    "\n",
    "<a href=\"#normal_functions\">Normal functions</a>\n",
    "\n",
    "<a href = \"#comparisons\">Comparing Weights Take 1</a>\n",
    "\n",
    "<a href = \"#comparisons2\">Comparing Weights Take 2</a>\n",
    "\n",
    "<a href = \"#combivars\">Considering Combined Variables</a>\n",
    "\n",
    "<a href = \"#conclusions\">Conclusions</a>\n",
    "\n",
    "<!--<a href=\"#weighted_season\">Weighted season predictions</a>-->\n",
    "\n",
    "<!--<a href=\"#model_comparisons\">Model comparisons</a>-->\n",
    "\n",
    "<!--<a href=\"#complications\">Complications</a>-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First a cell to prepare the notebook for the stuff that I might need to use. \n",
    "# More imports can be added as necessary.\n",
    "\n",
    "# special IPython command to prepare the notebook for matplotlib\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from pattern import web\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# And the additional modules that I've used\n",
    "\n",
    "import fnmatch\n",
    "import os\n",
    "import pickle\n",
    "from PyPDF2 import PdfFileReader\n",
    "from tabula import read_pdf\n",
    "import urllib\n",
    "import random\n",
    "import sklearn\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib as mpl\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this notebook, we continue the process that we began in Biathlon Report A and continued in Biathlon Report B. At this point, we have collected data on which predictor variables seem to correlate most strongly with the various pieces that make up the totality of the time for a biathlon sprint race. For most of these pieces, we have two or three possibilities, which means that even if we were to choose only a single variable to consider for each of the pieces, we still end up with 24 total possible weight combinations. As a result, we wish to compare the effectiveness of these combinations by running a small number (100) trials on a subset of races using each of these combinations and seeing which of the weight combinations seem to result in the best predictions.\n",
    "\n",
    "To begin with, there is some data that needs to be loaded in.\n",
    "\n",
    "<a href=\"#toc\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('best_variables1.pickle', 'rb') as handle:\n",
    "    best_variables = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "absolute_mens_speed = pd.read_pickle('absolute_mens_speed.pkl')\n",
    "absolute_mens_prone_range = pd.read_pickle('absolute_mens_prone_range.pkl')\n",
    "absolute_mens_prone_shooting = pd.read_pickle('absolute_mens_prone_shooting.pkl')\n",
    "absolute_mens_standing_range = pd.read_pickle('absolute_mens_standing_range.pkl')\n",
    "absolute_mens_standing_shooting = pd.read_pickle('absolute_mens_standing_shooting.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc_quant_snow_similarities = pd.read_pickle('wc_quant_snow_similarities.pkl')\n",
    "ibu_quant_snow_similarities = pd.read_pickle('ibu_quant_snow_similarities.pkl')\n",
    "\n",
    "wc_quant_weather_similarities = pd.read_pickle('wc_quant_weather_similarities.pkl')\n",
    "ibu_quant_weather_similarities = pd.read_pickle('ibu_quant_weather_similarities.pkl')\n",
    "\n",
    "wc_altitude_similarities = pd.read_pickle('wc_altitude_similarities.pkl')\n",
    "ibu_altitude_similarities = pd.read_pickle('ibu_altitude_similarities.pkl')\n",
    "\n",
    "wc_wind_c_similarities = pd.read_pickle('wc_wind_c_similarities.pkl')\n",
    "ibu_wind_c_similarities = pd.read_pickle('ibu_wind_c_similarities.pkl')\n",
    "\n",
    "wc_season_similarities = pd.read_pickle('wc_season_similarities.pkl')\n",
    "ibu_season_similarities = pd.read_pickle('ibu_season_similarities.pkl')\n",
    "\n",
    "wc_maximum_climb_similarities = pd.read_pickle('wc_maximum_climb_similarities.pkl')\n",
    "ibu_maximum_climb_similarities = pd.read_pickle('ibu_maximum_climb_similarities.pkl')\n",
    "\n",
    "wc_event_similarities = pd.read_pickle('wc_event_similarities.pkl')\n",
    "ibu_event_similarities = pd.read_pickle('ibu_event_similarities.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"adding_total_times\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"adding_total_times\"></a>\n",
    "\n",
    "# Adding total times\n",
    "\n",
    "One of our first steps here is to add the total race times for each racer to our sprint dataframes. In order to do this, we'll use two (slightly) different functions, one for world cup sprint competitions, and the other for ibu cup sprint competitions. Those functions are\n",
    "1. <a href=\"#add_total_times\">```add_total_times```</a>: This function takes the dataframe associated to a competition, loads the pickle file, adds a file that is the sum of the ski time and prone and standing range times for each racer, and repickles the resulting dataframe.\n",
    "2. <a href=\"#ibu_add_total_times\">```ibu_add_total_times```</a>:  This function performs exactly the same thing as ```add_total_times```, but is slightly adapted to reflect the fact that ibu cup events can have multiple sprint race competitions.\n",
    "\n",
    "<a href=\"#toc\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"add_total_times\"></a>\n",
    "\n",
    "<a href=\"#adding_total_times\">Back to Adding times</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "add_total_times : for a given world cup sprint race, loads the saved dataframe, computes \n",
    "                  the total race times as the sum of the ski times and the prone and \n",
    "                  standing range times, adds them to the dataframe, and then pickles\n",
    "                  the resulting dataframe \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "Stores an altered pickle file on the hard drive\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_total_times(year, event):\n",
    "    \n",
    "    filename = 'companal_SMSP_%(year)s_%(event)s.pkl' %{'year': year, 'event' : event}\n",
    "    event_data = pd.read_pickle(filename)\n",
    "\n",
    "    for i in range(len(event_data)):\n",
    "        total = (event_data.loc[i,'Total Ski'] + event_data.loc[i,'prone range'] \n",
    "                         + event_data.loc[i,'standing range'])\n",
    "        event_data.loc[i,'Total Time'] = total\n",
    "        \n",
    "    event_data.to_pickle(filename)\n",
    "    #return event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        try:\n",
    "            add_total_times(season, event)\n",
    "        except: #race doesn't exist\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ibu_add_total_times\"></a>\n",
    "\n",
    "<a href=\"#adding_total_times\">Back to Adding times</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ibu_add_total_times : for a given ibu cup sprint race, loads the saved dataframe, computes \n",
    "                      the total race times as the sum of the ski times and the prone and \n",
    "                      standing range times, adds them to the dataframe, and then pickles\n",
    "                      the resulting dataframe \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "code : a string that codes the sprint competition at the given event. This is necessary\n",
    "       because the ibu cup often has two different sprint races at a single event, which\n",
    "       contrasts with the world cup, which never has more than one. Possible values are\n",
    "       'SMSP' and 'SMSPS'\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "Stores a pickle object on the hard drive\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def ibu_add_total_times(year, event,code):\n",
    "    \n",
    "    filename = ('ibu_%(code)s_%(year)s_%(event)s.pkl'\n",
    "                        %{'code' : code, 'year': year, 'event' : event})\n",
    "    event_data = pd.read_pickle(filename)\n",
    "\n",
    "    for i in range(len(event_data)):\n",
    "        total = (event_data.loc[i,'Total Ski'] + event_data.loc[i,'prone range']\n",
    "                         + event_data.loc[i,'standing range'])\n",
    "        event_data.loc[i,'Total Time'] = total\n",
    "        \n",
    "    event_data.to_pickle(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__','OG__']\n",
    "codes = ['SMSP','SMSPS']\n",
    "\n",
    "for season in seasons:\n",
    "    for event in events:\n",
    "        for code in codes:\n",
    "            try:\n",
    "                add_total_times(season, event)\n",
    "                ibu_add_total_times(year,event,code)\n",
    "            except: #race doesn't exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"functions\"></a>\n",
    "\n",
    "# Prediction Functions\n",
    "\n",
    "Now that total times have been added to our dataframe, we build a handful of functions that allow us to use the data from previous races to make predictions about the outcomes of later races. Those functions are\n",
    "In order to have models to compare, we need to make predictions. Because these functions are taken directly from the previous notebook (Biathlon Report C-a), I'm going to write fairly minimally here. The five functions found below are:\n",
    "1. <a href=\"#adjust_times\">```adjust_times```</a>: It appears from looking at the data for individual racers that racer speeds generally increase somewhat linearly over time. The function adjust_times allows early career race speeds to be adjusted based on a linear fit of racer speed over all previous seasons (performed on a season by season basis) in order to try to mitigate the effects of time on speeds.\n",
    "2. <a href=\"#build_racer_speed_distribution\">```build_racer_speed_distribution```</a>: Creates a list of racer speeds by taking all of a given racer's speeds and then repeating them with a multiplicity that is dependent on the similarities between the conditions for the current competition and the individual prior competitions. A prior race with a similarity value of 1 (all predictor variables under consideration have identical or nearly identical values) would have its speed appear 10 times in the list. A prior race with a similarity value of 0.5 (the distance between the predictor variables under consideration is roughly half of the total spread for that variable) would have its speed appear 5 times on the list. ```build_racer_speed_distribution``` calls ```adjust_times``` to adjust the speeds for events that were held before the season under consideration to reflect general improvements in speed.\n",
    "3. <a href=\"#build_racer_pr_distribution\">```build_racer_pr_distribution```</a>:  Creates a list of n predicted total range times for a given racer in a given event. This is a fairly complicated process that involves the following steps:\n",
    "    1. For the given racer, determine which previous races (of both types) that racer has competed in. Collect range and accuracy (either prone or standing, depending on circumstance) into a pair of lists.\n",
    "    2. Using the weights associated to range times, produce a pair of weighted lists for range times and accuracy. Take a paired bootstrap sample (use the same ordered list of indices for both lists) and perform a linear regression on the results. The intercept is taken as the shooting time, and the slope is taken as the penalty loop time for any missed shots.\n",
    "    3. Using the weights associated to accuracy, produce a weighted list for accuracy. The mean of this list (or rather $a = (5-\\bar{x})/5$) will be taken to be the expected probability of making a particular shot. Five random values in the interval [0,1] are then computed. For each value, if the value is below $a$, the shot is made. Otherwise, the shot is missed.\n",
    "    4. The total range time is calculated as the sum of the shooting time and the product of the number of missed shots and the penalty loop time.\n",
    "4. <a href=\"#racer_time_predict\">```racer_time_predict```</a>: For a given racer, this function \n",
    "    1. first calls ```racer_speed_distribution```. Then, for each of the n desired predictions, it randomly selects a sample of size 10 from the returned list and computes the average.\n",
    "    2. next calls ```build_racer_pr_distribution``` twice, once for prone range times and once for standing range times. \n",
    "    3. finally, adds together the times in the resulting lists to produce a single list of n predicted race times for the given biathlete in the given race\n",
    "5. <a href=\"#race_time_predictions\">```race_time_predictions```</a>: calls ```racer_time_predict``` for each of the racers competing in a given race and stitches the outcomes together to form a single dataframe.\n",
    "\n",
    "\n",
    "<a href=\"#toc\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = 'adjust_times'></a>\n",
    "<a href=\"#functions\">Back to Prediction Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "adjust_times : Takes a racer and his event speeds over the course of a career, finds the\n",
    "               best fit line through the data, and adjusts early speeds to reflect what \n",
    "               they would be predicted to be if the race were run in the season under \n",
    "               consideration.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "season : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "adjusted_racer : a list containing the racer's speed adjusted for the season under \n",
    "                 consideration\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# To be called inside speed_predictions\n",
    "\n",
    "def adjust_times(racer, season):\n",
    "    \n",
    "    indices = racer.index.tolist()\n",
    "    years = [item.split(':')[1] for item in indices]\n",
    "    years = [''.join(['20',item[2:4]]) for item in years]\n",
    "    years = [float(item) for item in years]\n",
    "    years = np.array(years).reshape(-1,1)\n",
    "    speeds = np.array(racer.tolist()).reshape(-1,1)\n",
    "    \n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(years , speeds)\n",
    "    \n",
    "    coef = linreg.coef_[0][0]\n",
    "    \n",
    "    # Now that we know the slope of the best fit line here, I want to create a \n",
    "    # time adjusted version of the speeds\n",
    "    \n",
    "    adjusted_racer = racer.copy()\n",
    "    \n",
    "    for i in range(len(racer)):\n",
    "        time_delta = float(season)- years[i][0]\n",
    "        adjusted_racer[i] = adjusted_racer[i] + coef*time_delta\n",
    "        \n",
    "    return adjusted_racer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = 'build_racer_speed_distribution'></a>\n",
    "<a href=\"#functions\">Back to Prediction Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "build_racer_speed_distribution : takes a racer, season, event, and dataframes containing\n",
    "                                 similarity data about world cup vs world cup and world cup\n",
    "                                 vs ibu cup race conditions, and returns a list of speeds\n",
    "                                 from previous races, where the multiplicity of the speed\n",
    "                                 is determined by the degree of similarity between the race\n",
    "                                 of interest and the race from which the speed was taken\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "season : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "wc_sim : a dataframe containing values between 0 and 1 which codes the degree of similarity \n",
    "         between world cup (wc) races in a pairwise fashion. Pairs with a similarity value\n",
    "         of 1 were run under nearly identical conditions, while those with a similarity\n",
    "         value of 0 were run under extremely different conditions\n",
    "ibu_sim : a dataframe containing values between 0 and 1 which codes the degree of similarity \n",
    "          between world cup (wc) races and ibu cup races in a pairwise fashion. \n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "racer_predict : a list of speeds derived from the speeds of the racer's previous events. Each \n",
    "                prior speed is in the list with multiplicity n, where n is the rounded\n",
    "                product of 10 and the similarity score for the pairing between the current\n",
    "                event and the prior event\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_racer_speed_distribution(racer, season, event, wc_sim, ibu_sim):\n",
    "\n",
    "    col_name = ':'.join(['wc',season, event])\n",
    "    racer_data = absolute_mens_speed.loc[racer, :col_name]\n",
    "    name = racer\n",
    "    actual = float(racer_data[col_name])\n",
    "    short_racer_data = racer_data[2:-1].copy()\n",
    "    short_racer_data.dropna(inplace = True)\n",
    "    \n",
    "    # To drop later\n",
    "    #short_racer_data = short_racer_data[-20:]\n",
    "    \n",
    "    predictors = len(short_racer_data)\n",
    "    \n",
    "    year = ''.join(['20',season[2:4]])\n",
    "    short_racer_data = adjust_times(short_racer_data, year)\n",
    "    indices = short_racer_data.index.tolist()\n",
    "\n",
    "    race_weights = []\n",
    "    for index in indices:\n",
    "        split_index = index.split(':')\n",
    "        try:\n",
    "            if split_index[0] == 'wc':\n",
    "                race_weights.append(wc_sim.loc[':'.join([season,event]),\n",
    "                                               ':'.join([split_index[1],split_index[2]])])\n",
    "            else:\n",
    "                race_weights.append(ibu_sim.loc[':'.join([season,event]),\n",
    "                                                ':'.join([split_index[1],split_index[2]])])\n",
    "        except:\n",
    "            race_weights.append(0.0)\n",
    "    race_weights_rounded = [int(round(item,1)*10) for item in race_weights]\n",
    "\n",
    "    race_weights_rounded\n",
    "\n",
    "    # Making the weighted data list\n",
    "    \n",
    "    racer_predict = []\n",
    "\n",
    "    for i in range(len(short_racer_data)): \n",
    "        for j in range(race_weights_rounded[i]):\n",
    "            racer_predict.append(float(short_racer_data[i]))\n",
    "\n",
    "    return racer_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = 'build_racer_pr_distribution'></a>\n",
    "<a href=\"#functions\">Back to Prediction Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "build_racer_pr_distribution : takes a racer, season, event, and dataframes containing\n",
    "                              similarity data about world cup vs world cup and world cup\n",
    "                              vs ibu cup race conditions, and returns a list of predicted\n",
    "                              range times for the competition under consideration\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "season : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "wc_acc_sim : a dataframe containing values between 0 and 1 which codes the degree of \n",
    "             similarity between world cup (wc) races in a pairwise fashion. Pairs with \n",
    "             a similarity value of 1 were run under nearly identical conditions, while\n",
    "             those with a similarity value of 0 were run under extremely different \n",
    "             conditions. Chosen for predictive power for shooting accuracy\n",
    "ibu_acc_sim : a dataframe containing values between 0 and 1 which codes the degree of \n",
    "              similarity between world cup (wc) races and ibu cup races in a pairwise \n",
    "              fashion. Chosen for predictive power for shooting accuracy\n",
    "wc_range_sim : a dataframe containing values between 0 and 1 which codes the degree of \n",
    "               similarity between world cup (wc) races in a pairwise fashion. Chosen for\n",
    "               predictive power for range time (shooting time together with penalty time)\n",
    "ibu_range_sim : a dataframe containing values between 0 and 1 which codes the degree of \n",
    "                similarity between world cup (wc) races and ibu cup races in a pairwise \n",
    "                fashion. Chosen for predictive power for range time\n",
    "n : the length of the list of predicted range times that is returned\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "racer_predict : a list of predicted range times\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_racer_pr_distribution(racer, season, event, wc_acc_sim, ibu_acc_sim,\n",
    "                                wc_range_sim, ibu_range_sim,n):\n",
    "\n",
    "    col_name = ':'.join(['wc',season, event])\n",
    "    racer_time_data = absolute_mens_prone_range.loc[racer, :col_name]\n",
    "    racer_shot_data = absolute_mens_prone_shooting.loc[racer, :col_name]\n",
    "    name = racer\n",
    "    actual = float(racer_time_data[col_name])\n",
    "    short_racer_time_data = racer_time_data[2:-1].copy()\n",
    "    short_racer_shot_data = racer_shot_data[2:-1].copy()\n",
    "    short_racer_time_data.dropna(inplace = True)\n",
    "    short_racer_shot_data.dropna(inplace = True)\n",
    "    predictors = len(short_racer_shot_data)\n",
    "    \n",
    "    # To drop later\n",
    "    \n",
    "    short_racer_time_data = short_racer_time_data[-20:]\n",
    "    short_racer_shot_data = short_racer_shot_data[-20:]\n",
    "\n",
    "    \n",
    "    year = ''.join(['20',season[2:4]])\n",
    "    indices = short_racer_shot_data.index.tolist()\n",
    "    \n",
    "    # Separate weights for shooting accuracy and range times\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy_weights = []\n",
    "    for index in indices:\n",
    "        split_index = index.split(':')\n",
    "        if split_index[0] == 'wc':\n",
    "            accuracy_weights.append(wc_acc_sim.loc[':'.join([season,event]),\n",
    "                                                   ':'.join([split_index[1],split_index[2]])])\n",
    "        else:\n",
    "            try:\n",
    "                accuracy_weights.append(ibu_acc_sim.loc[':'.join([season,event]),\n",
    "                                                    ':'.join([split_index[1],split_index[2]])])\n",
    "            except: # There is missing data\n",
    "                accuracy_weights.append(0.0)\n",
    "    accuracy_weights_rounded = [int(round(item,1)*10) for item in accuracy_weights]\n",
    "\n",
    "    # Range times\n",
    "    range_weights = []\n",
    "    for index in indices:\n",
    "        split_index = index.split(':')\n",
    "        if split_index[0] == 'wc':\n",
    "            range_weights.append(wc_range_sim.loc[':'.join([season,event]),\n",
    "                                                  ':'.join([split_index[1],split_index[2]])])\n",
    "        else:\n",
    "            try:\n",
    "                range_weights.append(ibu_range_sim.loc[':'.join([season,event]),\n",
    "                                                    ':'.join([split_index[1],split_index[2]])])\n",
    "            except: # There is missing data\n",
    "                range_weights.append(0.0)\n",
    "    range_weights_rounded = [int(round(item,1)*10) for item in range_weights]\n",
    "\n",
    "    # And I'm going to have to split now to build my distributions\n",
    "    \n",
    "        # Making the weighted data list\n",
    "    \n",
    "    racer_accuracy = []\n",
    "    racer_shot = []\n",
    "    racer_time = []\n",
    "    racer_predict = []\n",
    "\n",
    "    for i in range(len(short_racer_shot_data)): \n",
    "        for j in range(accuracy_weights_rounded[i]):\n",
    "            racer_accuracy.append(float(short_racer_shot_data[i]))\n",
    "            \n",
    "    for i in range(len(short_racer_time_data)):\n",
    "        for j in range(range_weights_rounded[i]):\n",
    "            racer_shot.append(float(short_racer_shot_data[i]))\n",
    "            racer_time.append(float(short_racer_time_data[i]))\n",
    "            \n",
    "    # Build a model for shooting time and penalty loop time\n",
    "    \n",
    "    for k in range(n): \n",
    "        index_sample = np.random.choice(range(len(racer_shot)), \n",
    "                                                len(racer_shot), replace = True)\n",
    "        racer_shot_sample = [racer_shot[i] for i in index_sample]\n",
    "        racer_time_sample = [racer_time[i] for i in index_sample]\n",
    "        \n",
    "        accuracy = np.mean(racer_accuracy)/5\n",
    "        \n",
    "        linreg = LinearRegression()\n",
    "        linreg.fit(np.array(racer_shot_sample).reshape(-1,1), \n",
    "                           np.array(racer_time_sample).reshape(-1,1))\n",
    "        loop = linreg.coef_\n",
    "        shot_time = linreg.intercept_\n",
    "        \n",
    "    # And predict number of missed shots\n",
    "\n",
    "        shooting = np.random.sample(5)\n",
    "        count = 0\n",
    "        for m in range(5):\n",
    "            if shooting[m] < accuracy:\n",
    "                count += 1\n",
    "        range_time = shot_time + count*loop\n",
    "        racer_predict.append(range_time[0][0])\n",
    "          \n",
    "    return racer_predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = 'racer_time_predict'></a>\n",
    "<a href=\"#functions\">Back to Prediction Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "racer_time_predict : produces a list of predicted times for a given racer in a given \n",
    "                     competition\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer giving the desired number of times predicted\n",
    "length : a float giving the length of the ski portion of the course for the competition\n",
    "         under consideration\n",
    "weight_type : a list of lists containing the world cup and ibu similarity weightings\n",
    "              for the speed prediction and prone and standing range predictions\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "race_predictions : an n element list containing predicted total times for the given\n",
    "                   racer in the given competition\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def racer_time_predict(racer,year,event,n,length, weight_type):\n",
    "    \n",
    "    # First up, produce the speed estimates\n",
    "    \n",
    "    weightings = weight_type\n",
    "    #print weights[weightings[0]][0]\n",
    "    #print weights[weightings[0]][1]\n",
    "    \n",
    "    speed_distribution = build_racer_speed_distribution(racer, year, event, \n",
    "                                    weights[weightings[0]][0], weights[weightings[0]][1])\n",
    "    \n",
    "    ski_time_predictions = []\n",
    "    for i in range(n):\n",
    "        speed_sample = np.random.choice(speed_distribution, 10)\n",
    "        ski_time_predictions.append(length/np.mean(speed_sample))\n",
    "        \n",
    "    prone_range = build_racer_pr_distribution(racer, year, event, weights[weightings[1]][0], \n",
    "                                    weights[weightings[1]][1],weights[weightings[3]][0], \n",
    "                                              weights[weightings[3]][1],n)\n",
    "    standing_range = build_racer_pr_distribution(racer,year,event, weights[weightings[2]][0],\n",
    "                                    weights[weightings[2]][1],weights[weightings[4]][0],\n",
    "                                                 weights[weightings[4]][1],n)\n",
    "    \n",
    "    time_predictions = [x+y+z for x,y,z in zip(ski_time_predictions, \n",
    "                                               prone_range, standing_range)]\n",
    "    \n",
    "    race_predictions = [racer]\n",
    "    race_predictions.extend(time_predictions)\n",
    "    \n",
    "    return race_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = 'race_time_predictions'></a>\n",
    "<a href=\"#functions\">Back to Prediction Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "race_time_predictions : calls racer_time_predict repeatedly to create a dataframe of \n",
    "                        time predictions for all of the racers in a given competition\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer indicating the total number of time predictions to be made for each racer\n",
    "weight_type : a list of lists containing the world cup and ibu similarity weightings\n",
    "              for the speed prediction and prone and standing range predictions\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "predicted_times : a dataframe containing whose index consists of those racers who competed\n",
    "                  in the given race, and whose rows are the n predicted times for those\n",
    "                  racers\n",
    "problem_racers : a list of racers for whom racer_time_predict was unable to be executed,\n",
    "                 generally due to lack of prior race data\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def race_time_predictions(year,event,n, weight_type):\n",
    "    \n",
    "    # Find the length of the race\n",
    "    course_url = 'course_summary_%(year)s_M.pkl' %{'year' : year}\n",
    "    course_data = pd.read_pickle(course_url)\n",
    "    length = course_data.loc[course_data['Event'] == event]['Length'].tolist()[0]\n",
    "        \n",
    "    # Get the list of racers\n",
    "    \n",
    "    race_code = ':'.join(['wc', year, event])\n",
    "    racer_indices = absolute_mens_speed[race_code].dropna().index.tolist()\n",
    "\n",
    "    # Make the predictions\n",
    "    \n",
    "    predicted_times = []\n",
    "    problem_racers = []\n",
    "    \n",
    "    for racer in racer_indices[:-1]:\n",
    "        try:\n",
    "            predicted_times.append(racer_time_predict(racer,year,event,n, length, weight_type))\n",
    "        except:\n",
    "            problem_racers.append(racer)\n",
    "    \n",
    "    predicted_times = pd.DataFrame(predicted_times)\n",
    "    predicted_times.set_index(0, drop = True, inplace = True)\n",
    "    \n",
    "    return predicted_times, problem_racers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"eval_functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"eval_functions\"></a>\n",
    "# Evaluation Functions\n",
    "\n",
    "In order to evaluate the outcomes produced by our models, we need a number of functions that allow us to compare outcomes across multiple models. In order to do this, we define several additional functions.\n",
    "1. <a href=\"#place_counts\">```place_counts```</a>: The function <a href='#race_time_predictions'>```race_time_predictions```</a> produces a dataframe containing one row for each competitor in a competition (with the exception of those racers who have at most a single prior race) and with $n$ columns of predicted times. The function ```place_counts``` treats each column as a running of the race, and determines, for each column, the order of finish predicted for each racer. It then aggregates this data to produce a dataframe which again has a row for each competitor, but whose columns contain the number of predicted first place finishes, second place finishes, third place finishes, etc.  \n",
    "2. <a href=\"#finding_percentiles\">```finding_percentiles```</a>: The function <a href='#race_time_predictions'>```race_time_predictions```</a> produces a dataframe containing one row for each competitor in a competition (with the exception of those racers who have at most a single prior race) and with $n$ columns of predicted times. The function ```finding_percentiles``` calculates, for each individual racer, various attributes of the distribution of times predicted for that racer, among them mean, median, 25th and 75th percentiles, and the difference between the racer's actual time and the mean of their predicted times. It then returns a dataframe which contains one row for each racer and columns for each attribute of their time distributions.\n",
    "3. <a href=\"#evaluating_percentiles\">```evaluating_percentiles```</a>: This function takes the dataframe that is output by ```finding_percentiles``` and, for each racer, determines where in that racers distribution of time predictions their actual time falls. The results are then returned as a two column dataframe containing the racers' names in the first column and the code for the location of their actual times within their distributions as the second. The codes for the different parts of the distribution range are as follows:\n",
    "    - 0 : actual time is faster than the minimum predicted time\n",
    "    - 1 : actual time is between the minimum and the 5th percentile of the predicted times\n",
    "    - 2 : actual time is between the 5th percentile and the 10th percentile of the predicted times\n",
    "    - 3 : actual time is between the 10th percentile and the 25th percentile of the predicted times\n",
    "    - 4 : actual time is between the 25th percentile and the median of the predicted times\n",
    "    - 5 : actual time is between the median and the 75th percentile of the predicted times\n",
    "    - 6 : actual time is between the 75th percentile and the 90th percentile of the predicted times\n",
    "    - 7 : actual time is between the 90th percentile and the 95th percentile of the predicted times\n",
    "    - 8 : actual time is between the 95th percentile and the maximum of the predicted times\n",
    "    - 9 : actual time is slower than the maximum predicted time\n",
    "4. <a href=\"#evaluating_place_counts\">```evaluating_place_counts```</a>: This function takes the dataframe that is output by ```place_counts``` and, for each racer, determines with what frequency the predicted place is correct, within one place of being correct, within two places of being correct, within five places of being correct, within ten places of being correct, and within twenty places of being correct. (For example, a racer who actually finished 35th would have all predicted finishes between 30th and 40th counted when determining the value for beining within five places of being correct.) It then returns a dataframe with one row for each competitor and one column for each of the seven categories under consideration.\n",
    "5. <a href=\"#evaluating_dist_from_mean\">```evaluating_dist_from_mean```</a>: This function takes the dataframe output by ```finding_percentiles```. It uses the column ```dist_from_mean``` to determine for what percentage of the biathletes the mean of their distributions were within 10 seconds of their actual times, within 25 seconds of their actual times, within 50 seconds of their actual times, within 100 seconds of their actual times, within 150 seconds of their actual times, and within 200 seconds of their actual times. The result is then returned as a dataframe with only a single column.\n",
    "<!--6. <a href=\"#average_from_mean\">```average_from_mean```</a>: This function takes the ```diff from mean``` column from the dataframe output by ```finding_percentiles``` and calculates both the mean of the values in the column and the mean of the absolute values of the entries in the column and returns both values as floats. The first computation gives us some indication of how balanced our errors in prediction are with respect to the actual times, since positive and negative values will cancel each other out. The second computation gives us an indication of overall error. In the case that the absolute values of these two numbers are the same (or nearly the same), it is an indication that the values being predicted tend to fall consistantly too high or too low.-->\n",
    "<!--7. <a href=\"#check_predictions\">```check_predictions```</a>: This function takes the dataframe produced by ```finding_percentiles``` and returns a dataframe that, for each racer, indicates (via ```True``` or ```False```) whether that racer's actual time fell in the middle 50% of his predicted time distribution and whether his actual time fell in the middle 90% of his predicted time distribution.-->\n",
    "<!--8. <a href=\"#inside_outside\">```inside_outside```</a>: This function takes the dataframe produced by ```check_predictions``` and returns two floats. The first is the percentage of the racers for whom the actual time fell inside of the middle 50% of their distribution, and the second is the percentage of the racers for whom the actual time fell inside of the middle 90% of their distribution.-->\n",
    "\n",
    "<a href=\"#toc\">Table of Contents</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"place_counts\"></a>\n",
    "<a href=\"#eval_functions\">Back to Evaluation Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "place_counts : treats each column of a race_time_predictions dataframe as an instance\n",
    "               of a race, and determines the finishing places for each racer within \n",
    "               that race\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "times : a dataframe that is the output of a call to race_time_predictions\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "place_count : a dataframe of integers indicating which place a given racer would have \n",
    "              had if the kth column of \n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def place_counts(times):\n",
    "    \n",
    "    predicted_places = times.copy()\n",
    "\n",
    "    for j in range(predicted_places.shape[1]):\n",
    "        predicted_places.sort_values(j+1,inplace = True)\n",
    "        for i in range(len(predicted_places)):\n",
    "            predicted_places.iloc[i,j] = i+1\n",
    "\n",
    "    place_counts = pd.DataFrame(columns = range(1, len(times)+1), index = times.index)\n",
    "    \n",
    "    for racer in place_counts.index:\n",
    "        for i in place_counts.columns:\n",
    "            count = len([item for item in predicted_places.loc[racer] if item == i])\n",
    "            place_counts.loc[racer,i] = count\n",
    "            \n",
    "    return place_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"finding_percentiles\"></a>\n",
    "<a href=\"#eval_functions\">Back to Evaluation Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "finding_percentiles : takes the output of a call to race_time_predictions and returns a\n",
    "                      dataframe containing information about the distribution of predicted\n",
    "                      times for each racer\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "times : a dataframe that is the output of a call to race_time_predictions\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "racer_percentiles : a dataframe containing the name, mean time, standard deviation of time,\n",
    "                    minimum time, fifth percentile, tenth percentile, twenty-fifth percentile,\n",
    "                    median, seventy-fifth percentile, ninetieth percentile, ninety-fifth\n",
    "                    percentile, maximum time, actual time, and difference between the actual\n",
    "                    time and the mean predicted time for each racer\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def finding_percentiles(times, year, event):\n",
    "    \n",
    "    racer_percentiles = []\n",
    "    \n",
    "    filename = 'companal_SMSP_%(year)s_%(event)s.pkl' %{'year': year, 'event' : event}\n",
    "    event_data = pd.read_pickle(filename)\n",
    "    \n",
    "    for i in range(len(times)):\n",
    "        name = times.index[i]\n",
    "        racer = [name]\n",
    "        racer_data = times.iloc[i,:]\n",
    "        mean = np.mean(racer_data)\n",
    "        stdev = np.std(racer_data)\n",
    "        minimum = min(racer_data)\n",
    "        per5 = np.percentile(racer_data,5)\n",
    "        per10 = np.percentile(racer_data, 10)\n",
    "        per25 = np.percentile(racer_data,25)\n",
    "        median = np.median(racer_data)\n",
    "        per75 = np.percentile(racer_data,75)\n",
    "        per90 = np.percentile(racer_data,90)\n",
    "        per95 = np.percentile(racer_data,95)\n",
    "        maximum = max(racer_data)\n",
    "        actual = event_data.loc[event_data['Name'] == name]['Total Time'].tolist()[0]\n",
    "        difference = actual - mean\n",
    "        racer.extend([mean,stdev,minimum,per5, per10, per25,median,per75,per90,per95,maximum,\n",
    "                      actual, difference])\n",
    "        racer_percentiles.append(racer)\n",
    "        \n",
    "    racer_percentiles = pd.DataFrame(racer_percentiles, columns = ['Name','mean','deviation',\n",
    "                                    'min', '5th per','10th per','25th per','median',\n",
    "                                    '75th per','90th per', '95th per','maximum','actual time', \n",
    "                                    'diff from mean'])\n",
    "    \n",
    "    return racer_percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"evaluating_percentiles\"></a>\n",
    "<a href=\"#eval_functions\">Back to Evaluation Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "evaluating_percentiles : takes a dataframe that is the output of a call to finding \n",
    "                         percentiles and returns a dataframe that indicates for each \n",
    "                         racer into what part of that racer's predicted times distribution\n",
    "                         his actual time falls\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "percentiles : a dataframe that is the output of a call to finding_percentiles\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "evaluation : a dataframe containing, for each racer, a code indicating in which portion \n",
    "             of that racer's predicted distribution his actual time falls.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def evaluating_percentiles(percentiles):\n",
    "    \n",
    "    evaluation = []\n",
    "    for i in range(len(percentiles)):\n",
    "        racer_data = percentiles.iloc[i,:]\n",
    "        name = racer_data[0]\n",
    "        actual = racer_data[-2]\n",
    "        if actual < percentiles.iloc[i,3]:\n",
    "            loc = 0\n",
    "        elif percentiles.iloc[i,3] <= actual < percentiles.iloc[i,4]:\n",
    "            loc = 1\n",
    "        elif percentiles.iloc[i,4] <= actual < percentiles.iloc[i,5]:\n",
    "            loc = 2\n",
    "        elif percentiles.iloc[i,5] <= actual < percentiles.iloc[i,6]:\n",
    "            loc = 3\n",
    "        elif percentiles.iloc[i,6] <= actual < percentiles.iloc[i,7]:\n",
    "            loc = 4\n",
    "        elif percentiles.iloc[i,7] <= actual < percentiles.iloc[i,8]:\n",
    "            loc = 5\n",
    "        elif percentiles.iloc[i,8] <= actual < percentiles.iloc[i,9]:\n",
    "            loc = 6\n",
    "        elif percentiles.iloc[i,9] <= actual < percentiles.iloc[i,10]:\n",
    "            loc = 7\n",
    "        elif percentiles.iloc[i,10] <= actual < percentiles.iloc[i,11]:\n",
    "            loc = 8\n",
    "        else:\n",
    "            loc = 9\n",
    "\n",
    "        evaluation.append([name,loc])\n",
    "        \n",
    "    evaluation = pd.DataFrame(evaluation, columns = ['Name','Location'])\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"evaluating_place_counts\"></a>\n",
    "<a href=\"#eval_functions\">Back to Evaluation Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "evaluating_place_counts : takes a dataframe that is the output of place_counts and, for each\n",
    "                          racer, determines how many of the trial races had the racer placed\n",
    "                          correctly, within one place of his actual finish place, within two \n",
    "                          places of his actual finish place, etc \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "place_data : a dataframe that is the output of place_counts\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "       y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "       last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "place_evaluations : a dataframe containing a row for each racer in the competition. Each\n",
    "                    row in turn contains the number of predicted finishes places that were\n",
    "                    correct, within one place of the true place,within two places of the \n",
    "                    true place,within three places of the true place, within five places \n",
    "                    of the true place, within ten places of the true place, and within\n",
    "                    twenty places of the true place.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def evaluating_place_counts(place_data,year,event):\n",
    "    \n",
    "    # Get the actual places\n",
    "    filename = 'companal_SMSP_%(year)s_%(event)s.pkl' %{'year' : year, 'event' : event}\n",
    "    finish_place = pd.read_pickle(filename)[['Name','Total Time']]\n",
    "    finish_place.set_index('Name', inplace = True, drop = True)\n",
    "    finish_place = finish_place.loc[place_data.index]\n",
    "    finish_place.sort_values('Total Time', inplace = True)\n",
    "    finish_place.reset_index(inplace = True)\n",
    "    finish_place.reset_index(inplace = True)\n",
    "    finish_place.columns = ['Place','Name','Total Time']\n",
    "    finish_place.set_index('Name', inplace = True, drop = True)\n",
    "    \n",
    "    place_evaluations = []\n",
    "    \n",
    "    for racer in place_data.index.tolist():\n",
    "        racer_places = place_data.loc[racer]\n",
    "        racer_actual = finish_place.loc[racer,'Place']+1\n",
    "        racer_evaluation = [racer, racer_actual]\n",
    "        \n",
    "        for i in [0, 1, 2, 3, 5, 10, 20]:\n",
    "            \n",
    "            racer_range = range(racer_actual- i, racer_actual+i+1)\n",
    "            possible_range = set(range(1,len(racer_places)+1))\n",
    "            check_range = set(possible_range.intersection(racer_range))\n",
    "            count = 0\n",
    "            for j in check_range:\n",
    "                count = count + racer_places[j]\n",
    "            racer_evaluation.append(count)\n",
    "            \n",
    "        place_evaluations.append(racer_evaluation)\n",
    "        \n",
    "    place_evaluations = pd.DataFrame(place_evaluations)\n",
    "    place_evaluations.columns = ['Name','Place','Correct','Within 1','Within 2','Within 3',\n",
    "                                'Within 5','Within 10','Within 20']\n",
    "    \n",
    "    place_evaluations.sort_values('Place',inplace = True)\n",
    "    return place_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"evaluating_dist_from_mean\"></a>\n",
    "<a href=\"#eval_functions\">Back to Evaluation Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "evaluating_dist_from_mean : takes the output of finding_percentiles and returns a dataframe\n",
    "                            that indicates how well the centers of the individual racer's\n",
    "                            time distributions align with their actual times\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "diff_from_mean : a dataframe that is the output of a call to finding_percentiles\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "evaluated_distances : a dataframe containing a single column indicating what percentage\n",
    "                      of differences from the mean (the difference between the mean of a\n",
    "                      racer's time distribution and his actual time) for a given competition\n",
    "                      were within 10 seconds of the actual time, within 25 seconds of the \n",
    "                      actual time, within 50 seconds of the actual time, within 100 seconds\n",
    "                      of the actual time, within 150 seconds of the actual time, and within\n",
    "                      200 seconds of the actual time.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def evaluating_dist_from_mean(diff_from_mean):\n",
    "    \n",
    "    evaluated_distances = []\n",
    "    \n",
    "    for i in [10,25,50,100,150,200]:\n",
    "        count = 0\n",
    "        for j in range(len(diff_from_mean)):\n",
    "            if abs(diff_from_mean[j])<= i:\n",
    "                count += 1\n",
    "        evaluated_distances.append(float(count)/len(diff_from_mean)*100)\n",
    "        \n",
    "    evaluated_distances = pd.DataFrame(evaluated_distances, \n",
    "                            index = ['in 10', 'in 25', 'in 50', 'in 100', 'in 150','in 200'])\n",
    "    \n",
    "    return evaluated_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"normal_functions\"></a>\n",
    "\n",
    "# Normal Predictions\n",
    "\n",
    "In order to evaluate the quality of the predictions made by a model of this sort, it seems that there are two possibilities. The first, and most obvious, is to simply compare the outcomes of these models to the actual race times. Using this method of evaluation, we might say that if a racer's time for a particular race is 1500 seconds, and it is predicted to be 1501 seconds, that the model is good, while if the racer's time is predicted to be 1500 seconds but is actually 1530 seconds, the model is bad. Similarly, if a racer's most likely finish is predicted to be in the top 5, and that racer finishes third, we might say that the model is good, while if the racer finishes seventh, we might say that the model is bad. The problem here is that, given the nature of biathlon, if these are the requirements to find a good model, it may well be impossible to find anything but a bad model.\n",
    "\n",
    "The second option is to compare these models with a naive model of what biathlon times might look like. That might be constructed in the following way: \n",
    "\n",
    "1. Observe that, for each racer, we find a wide variety of total times for the sprint race. For instance, for the last season's sprint races for Tarjei Boe, we find total times ranging from 1360 seconds to 1642 seconds, with a mean time of 1519 seconds and a standard deviation of 82.53 seconds. As a result, any naive model we choose should reflect this uncertaintly.\n",
    "2. Observe that it seems reasonable to think of these values as being drawn from some sort of distribution, and that the most obvious distribution is the Gaussian or normal distribution. \n",
    "3. Decide that a reasonable (naive) model would be to assume that the total times for each racer are drawn from a normal distribution, and that, further, the mean and standard deviation of total times for a racer's most recent 20 races (to account for speed drift over time) are good estimates of the actual mean and standard deviation of these normal distributions.\n",
    "\n",
    "Note here that this approach requires first creating a dataframe of all the racers for whom we have data and all of the races for which we have data, ordered chronologically, in order to determine, given a racer and a race, what his 20 most recent competitions were. Thus, we begin with a <a href=\"#event_order\">cell</a><a name=\"event_order_back\"></a> to interleave ibu cup and world cup events for each season (since racers often bounce back and forth between the two levels) and <a href=\"#season_order\">another</a><a name = \"season_order_back\"></a> to put the individual seasons in order.  A third <a href=\"#collecting_times\">cell</a><a name = \"collecting_times_back\"></a> then loops through all of the events in order, merging the subdataframes containing racer names and total times in order to create a single large dataframe, which is then pickled.\n",
    "\n",
    "Given these assumptions, we build our model using the following functions:\n",
    "1. <a href=\"#racer_from_normal\">```racer_from_normal```</a>: This function takes the name of a racer and the codes (year and event) specifying a particular sprint competition, finds the mean and standard deviation of the racer's times for his most recent 20 sprint competitions (or all sprint competitions, in the event that he has fewer than 20). These are then used to define a normal distribution from which $n$ predicted times are randomly drawn.\n",
    "2. <a href=\"#predictions_from_normal\">```predictions_from_normal```</a>: This function calls ```racer_from_normal``` for each competitor in a given race and returns a dataframe with a row for each competitor and $n$ columns, one for each predicted time (for a given competitor).\n",
    "<!--3. <a href=\"#normal_evaluate_on_season\">```normal_evaluate_on_season```</a>: This function calls ```predictions_from_normal``` for each event in the given season. It then returns four objects (which are the same as the objects returned by <a href=\"#evaluate_on_season\">```evaluate_on_season```</a> in <a href=\"#weighted_season\">weighted season prediction</a>).-->\n",
    "<!--    1. median_places_correct : a dataframe containing the concatenated outputs of the medians of the results of <a href=\"#evaluating_place_counts\">```evaluating_place_counts```</a> for the events in the given season. In other words, a dataframe which contains one column for each event in the season, whose entries are the medians of the columns of the dataframes produced by ```evaluating_place_counts```. -->\n",
    " <!--   2. distance_percentages : a dataframe containing the concatenated outputs of all the results of <a href=\"#evaluating_dist_from_mean\">```evaluating_dist_from_mean```</a> for the events in the given season.-->\n",
    " <!--   3. averages_from_mean : an array containing the outputs of <a href=\"#average_from_mean\">```average_from_mean```</a> for each event in the given season.-->\n",
    " <!--   4. percentages_in_center : an array containing the outputs of <a href=\"#inside_outside\">```inside_outside```</a> for each event of the season-->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"#toc\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"event_order\"></a>\n",
    "<a href=\"#event_order_back\">Back to Normal Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_0405 = [['ibu', 'CP01'], ['ibu', 'CP02'], ['companal', 'CP01'], ['companal', 'CP02'], \n",
    "               ['ibu', 'CP03'], ['companal', 'CP03'], ['ibu', 'CP04'], ['companal', 'CP04'], \n",
    "               ['ibu', 'CP05'], ['companal', 'CP05'], ['ibu', 'CP06'], ['companal', 'CP06'], \n",
    "               ['companal', 'CP07'], ['companal', 'CP08'], ['ibu', 'CH__'], \n",
    "               ['companal', 'CH__'], ['ibu', 'CP07'], ['ibu', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_0506 = [['companal', 'CP01'], ['ibu', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP02'], \n",
    "               ['ibu', 'CP03'], ['companal', 'CP03'], ['companal', 'CP04'], \n",
    "               ['companal', 'CP05'], ['ibu', 'CP04'], ['ibu', 'CP05'], ['companal', 'CP06'],\n",
    "               ['companal', 'OG__'], ['ibu', 'CH__'], ['companal', 'CP07'], ['ibu', 'CP06'], \n",
    "               ['companal', 'CP08'], ['ibu', 'CP07'], ['companal', 'CP09']]\n",
    "\n",
    "events_0607 = [['companal', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP01'], \n",
    "               ['companal', 'CP03'], ['ibu', 'CP02'], ['companal', 'CP04'], \n",
    "               ['companal', 'CP05'], ['ibu', 'CP03'], ['companal', 'CP06'], \n",
    "               ['ibu', 'CP04'], ['companal', 'CH__'], ['ibu', 'CP06'], ['ibu', 'CH__'],\n",
    "               ['companal', 'CP07'], ['companal', 'CP08'], ['ibu', 'CP07'], \n",
    "               ['companal', 'CP09'], ['ibu', 'CP08']]\n",
    "\n",
    "events_0708 = [['ibu', 'CP01'], ['companal', 'CP01'], ['ibu', 'CP02'], ['companal', 'CP02'],\n",
    "               ['ibu', 'CP03'], ['companal', 'CP03'], ['ibu', 'CP04'], ['companal', 'CP04'],\n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CP06'],\n",
    "               ['companal', 'CH__'], ['ibu', 'CH__'], ['companal', 'CP07'], \n",
    "               ['companal', 'CP08'], ['ibu', 'CP07'], ['companal', 'CP09'], ['ibu', 'CP08']]\n",
    "\n",
    "events_0809 = [['ibu', 'CP01'], ['companal', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP02'],\n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'],\n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CP06'], \n",
    "               ['companal', 'CH__'], ['ibu', 'CP07'], ['ibu', 'CH__'], ['companal', 'CP07'], \n",
    "               ['ibu', 'CP08'], ['companal', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_0910 = [['ibu', 'CP01'], ['companal', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP02'],\n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'],\n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CP06'],\n",
    "               ['ibu', 'CP07'], ['companal', 'OG__'], ['ibu', 'CH__'], ['companal', 'CP07'],\n",
    "               ['ibu', 'CP08'], ['companal', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_1011 = [['ibu', 'CP01'], ['companal', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP02'],\n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'],\n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], \n",
    "               ['companal', 'CP07'], ['ibu', 'CP06'], ['companal', 'CP08'], ['ibu', 'CP07'], \n",
    "               ['ibu', 'CH__'], ['companal', 'CH__'], ['ibu', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_1112 = [['ibu', 'CP01'], ['companal', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP02'],\n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'], \n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CH__'],\n",
    "               ['companal', 'CP07'], ['companal', 'CP08'], ['ibu', 'CP06'], ['ibu', 'CP07'], \n",
    "               ['companal', 'CH__'], ['ibu', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_1213 = [['companal', 'CP01'], ['ibu', 'CP01'], ['ibu', 'CP02'], ['companal', 'CP02'], \n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'],\n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CP06'], \n",
    "               ['companal', 'CH__'], ['ibu', 'CP07'], ['ibu', 'CH__'], ['companal', 'CP07'],\n",
    "               ['companal', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_1314 = [['companal', 'CP01'], ['ibu', 'CP01'], ['ibu', 'CP02'], ['companal', 'CP02'], \n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'],\n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CP06'],\n",
    "               ['ibu', 'CH__'], ['companal', 'OG__'], ['ibu', 'CP07'], ['companal', 'CP07'],\n",
    "               ['companal', 'CP08'], ['ibu', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_1415 = [['companal', 'CP01'], ['ibu', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP02'],\n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'], \n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CH__'], \n",
    "               ['companal', 'CP07'], ['ibu', 'CP06'], ['companal', 'CP08'], ['ibu', 'CP07'], \n",
    "               ['companal', 'CH__'], ['ibu', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_1516 = [['companal', 'CP01'], ['ibu', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP02'], \n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'], \n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CP06'], \n",
    "               ['companal', 'CP07'], ['companal', 'CP08'], ['ibu', 'CP07'], ['ibu', 'CH__'], \n",
    "               ['companal', 'CH__'], ['ibu', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_1617 = [['companal', 'CP01'], ['ibu', 'CP01'], ['companal', 'CP02'], ['ibu', 'CP02'], \n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'], \n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CH__'], \n",
    "               ['ibu', 'CP06'], ['companal', 'CH__'], ['companal', 'CP07'], ['ibu', 'CP07'], \n",
    "               ['companal', 'CP08'], ['ibu', 'CP08'], ['companal', 'CP09']]\n",
    "\n",
    "events_1718 = [['ibu', 'CP01'], ['companal', 'CP01'], ['ibu', 'CP02'], ['companal', 'CP02'],\n",
    "               ['companal', 'CP03'], ['ibu', 'CP03'], ['companal', 'CP04'], ['ibu', 'CP04'],\n",
    "               ['companal', 'CP05'], ['ibu', 'CP05'], ['companal', 'CP06'], ['ibu', 'CH__'], \n",
    "               ['ibu', 'CP06'], ['companal', 'OG__'], ['companal', 'CP07'], ['ibu', 'CP07'],\n",
    "               ['companal', 'CP08'], ['ibu', 'CP08'], ['companal', 'CP09']              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"season_order\"></a>\n",
    "<a href=\"#season_order_back\">Back to Normal Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordered_events ={'0405' : events_0405, '0506' : events_0506, '0607' : events_0607,\n",
    "                 '0708' : events_0708, '0809' : events_0809, '0910' : events_0910, \n",
    "                 '1011' : events_1011, '1112' : events_1112, '1213' : events_1213, \n",
    "                 '1314' : events_1314, '1415' : events_1415, '1516' : events_1516, \n",
    "                 '1617' : events_1617, '1718' : events_1718}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"collecting_times\"></a>\n",
    "<a href=\"#collecting_times_back\">Back to Normal Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0607 ['companal', 'CP08'] has no companal file\n",
      "0607 ['companal', 'CP09'] has no companal file\n",
      "0809 ['companal', 'CH__'] has no companal file\n",
      "1314 ['companal', 'CP05'] has no companal file\n",
      "1516 ['companal', 'CP05'] has no companal file\n",
      "1617 ['companal', 'CP06'] has no companal file\n",
      "1718 ['companal', 'CP05'] has no companal file\n"
     ]
    }
   ],
   "source": [
    "# And collecting the total time data\n",
    "\n",
    "absolute_mens_time = pd.DataFrame(columns = ['Name'])\n",
    "\n",
    "seasons = ['0405','0506','0607','0708','0809','0910','1011','1112','1213','1314','1415',\n",
    "           '1516','1617','1718']\n",
    "\n",
    "for season in seasons:\n",
    "    events = ordered_events[season]\n",
    "    for event in events:\n",
    "        if event[0] == 'ibu':\n",
    "            filename1 = ('%(cup)s_SMSP_%(season)s_%(event)s.pkl' \n",
    "                             %{'cup' : event[0], 'season' : season, 'event' : event[1]})\n",
    "            filename2 = ('%(cup)s_SMSPS_%(season)s_%(event)s.pkl' \n",
    "                             %{'cup' : event[0], 'season' : season, 'event' : event[1]})\n",
    "\n",
    "            colname1 = ':'.join(['ibu',season,event[1]])\n",
    "            colname2 = ':'.join(['ibuS',season,event[1]])\n",
    "            try:\n",
    "                df = pd.read_pickle(filename1)[['Name','Total Time']]\n",
    "                df.columns = ['Name',colname1]\n",
    "                absolute_mens_time = absolute_mens_time.merge(df, how = 'outer', on = 'Name')\n",
    "            \n",
    "            except:\n",
    "                #print season, event, 'has no companal file'\n",
    "                pass\n",
    "            try:\n",
    "                df = pd.read_pickle(filename2)[['Name','Total Time']]\n",
    "                df.columns = ['Name',colname2]\n",
    "                absolute_mens_time = absolute_mens_time.merge(df, how = 'outer', on = 'Name')\n",
    "            \n",
    "            except:\n",
    "                #print season, event, 'has no companal file'\n",
    "                pass\n",
    "        else:\n",
    "            filename = ('%(cup)s_SMSP_%(season)s_%(event)s.pkl' \n",
    "                            %{'cup' : event[0], 'season' : season, 'event' : event[1]})\n",
    "            colname = ':'.join(['wc',season,event[1]])\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_pickle(filename)[['Name','Total Time']]\n",
    "                df.columns = ['Name',colname]\n",
    "                absolute_mens_time = absolute_mens_time.merge(df, how = 'outer', on = 'Name')\n",
    "            \n",
    "            except:\n",
    "                print season, event, 'has no companal file'\n",
    "                #pass\n",
    "\n",
    "last_row = len(absolute_mens_time)\n",
    "\n",
    "for col in absolute_mens_time.columns.tolist():\n",
    "    absolute_mens_time.loc[last_row, col] = \"\".join(['20', col[2:4]])\n",
    "    \n",
    "for i in range(len(absolute_mens_time)):\n",
    "    absolute_mens_time.loc[i,'count'] = absolute_mens_time.loc[i].count() - 1\n",
    "    \n",
    "absolute_mens_time.loc[last_row,'Name'] = 'Year'\n",
    "\n",
    "absolute_mens_time.set_index('Name', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "absolute_mens_time.to_pickle('absolute_mens_time.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"racer_from_normal\"></a>\n",
    "<a href=\"#normal_functions\">Back to Normal Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "racer_from_normal : predicts n race times for a given racer in a given event based on the\n",
    "                    premise that the racer's times are normally distributed and that the \n",
    "                    distribution is well represented by the racer's times on his previous 20\n",
    "                    races\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "racer : a string containing the name of a biathlete\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer giving the number of predicted times desired for the given racer.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "racer_predict : a list of n predicted race times\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def racer_from_normal(racer, year, event, n):\n",
    "    \n",
    "    col_name = ':'.join(['wc',year, event])\n",
    "    racer_data = absolute_mens_time.loc[racer, :col_name]\n",
    "    name = racer\n",
    "    actual = float(racer_data[col_name])\n",
    "    short_racer_data = racer_data[2:-1].copy()\n",
    "    short_racer_data.dropna(inplace = True)\n",
    "    \n",
    "    short_racer_data = short_racer_data[-20:]\n",
    "    predictors = len(short_racer_data)\n",
    "    \n",
    "    mean = np.mean(short_racer_data)\n",
    "    stdev = np.std(short_racer_data)\n",
    "    \n",
    "    predictions = np.random.normal(mean, stdev, n)\n",
    "    \n",
    "    racer_predict = [name]#, actual]\n",
    "    \n",
    "    racer_predict.extend(predictions)\n",
    "\n",
    "    return racer_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"predictions_from_normal\"></a>\n",
    "<a href=\"#normal_functions\">Back to Normal Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "predictions_from_normal : for a given competition, repeats predict_from_normal for all\n",
    "                          competitors and returns the time predictions in the form of\n",
    "                          a dataframe\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer giving the number of predicted times desired for the given race.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "predicted_times : a dataframe containing all n of the predicted times for each racer.\n",
    "problem_racers : a list of racers for whome predict_from_normal failed to execute. This\n",
    "                 is typically due to a lack of prior races for a given competitor.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def predictions_from_normal(year, event, n):\n",
    "    \n",
    "    # Get the list of racers\n",
    "    \n",
    "    race_code = ':'.join(['wc', year, event])\n",
    "    racer_indices = absolute_mens_speed[race_code].dropna().index.tolist()\n",
    "\n",
    "    # Make the predictions\n",
    "    \n",
    "    predicted_times = []\n",
    "    problem_racers = []\n",
    "    \n",
    "    for racer in racer_indices[:-1]:\n",
    "        try:\n",
    "            predicted_times.append(racer_from_normal(racer, year,event,n))\n",
    "        except:\n",
    "            problem_racers.append(racer)\n",
    "    \n",
    "    predicted_times = pd.DataFrame(predicted_times)\n",
    "    predicted_times.set_index(0, drop = True, inplace = True)\n",
    "    \n",
    "    return predicted_times, problem_racers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"comparisons\"></a>\n",
    "\n",
    "# Comparing Weights Take 1\n",
    "\n",
    "We begin here by carrying over the predictor variables that seemed to have the most impact on the various pieces of our model from our previous notebooks. Note that this leaves us with a total of 24 combinations to check, in contrast with $13^5 = 371293$ possibilities if we were to simply consider every possible combination of 5 weights (and far more than that if we were to include multiple different variables for a single piece of the model). From there, a series of nested ```for``` loops creates an array of weight combinations to try.\n",
    "\n",
    "Of course, in order to choose a weight combination, we need some way of comparing what comes out of the prediction functions. We have two functions that we use here\n",
    "1. <a href = \"#compare_weightings\">```compare_weightings```</a>: this function takes a competition and each of a collection of weights and, for each weight in the collection, predicts a fixed number of times for each racer. It then calls <a href = \"#evaluating_place_counts\"> ```evaluating_place_counts```</a> and <a href=\"#evaluating_dist_from_mean\">```evaluating_dist_from_mean```</a> to measure the goodness of the results for each weight under consideration.\n",
    "2. <a href = \"#pluck_best_weights\">```pluck_best_weights```</a>: this function takes either of the two dataframes output by ```compare_weightings``` and assigns scores to each weight combination based on how it fairs in comparison with the other weight combinations for each of the events under consideration.\n",
    "\n",
    "Next, I randomly selected ten events from the 2009-2010 to 2013-2014 seasons, and ran ```compare_weightings``` and ```pluck_best_weights``` using a fairly small value of $n$ (100 trials) on each of them, storing the outputs in dataframes ```race_place_scores``` and ```race_distance_scores```. At that point, I had information about which of the weightings under consideration were the best  for each of the individual races, but the idea was to find a weighting (weightings) that would fair the best overall across all ten of these events, under the theory that a weight model that faired well across all ten of the randomly chosen events would likely fair well more generally. For this, I added two more functions\n",
    "3. <a href = \"#score_of_scores\">```score_of_scores```</a>: takes a dataframe produced by concatenating the outputs of ```pluck_best_weights``` via ```compare_weightings``` over a collection of events, and, for each column, gives a score of 3 for every value above the 83rd percentile for that column, a score of 2 for every value above 75th percentile, and a score of -1.5 for every value below the 17th percentile. \n",
    "4. <a href = \"#rank_weightings\">```rank_weightings```</a>: takes two dataframes produced by concatenating the outputs of ```pluck_best_weights``` via ```compare_weightings``` over a collection of events and  applies ```score_of_scores``` to each of them. It then considers four results:\n",
    "    1. the output of ```score_of_scores``` applied to the race distances dataframe,\n",
    "    2. the output of ```score_of_scores``` applied to the race places dataframe,\n",
    "    3. the sum (across columns) of the race distances dataframe, and\n",
    "    4. the sum (across columns) of the race places dataframe,\n",
    "    \n",
    "and, for each category, gives a positive point if the value is in the top quartile and a negative point if it is in the bottom quartile.\n",
    "\n",
    "\n",
    "I initially chose to keep all weightings for which the score was at least three, but after realizing that left me with only 3 weightings, and, recognizing that running only 100 trials on each weight meant that my results were perhaps less stable than I would have liked, I decided to lower the threshhold score to 2. This resulted in having 8 weightings to consider, which gave me a list to carry forward to the second round of comparing weights.\n",
    "\n",
    "<a href=\"#toc\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 201.68 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "weights = {\"quant_snow\" : [wc_quant_snow_similarities, ibu_quant_snow_similarities],\n",
    "\"quant_weather\" : [wc_quant_weather_similarities, ibu_quant_weather_similarities],\n",
    "\"altitude\" : [wc_altitude_similarities, ibu_altitude_similarities],\n",
    "\"wind_c\" : [wc_wind_c_similarities, ibu_wind_c_similarities],\n",
    "\"quant_season\" : [wc_season_similarities, ibu_season_similarities],\n",
    "\"max_climb\" : [wc_maximum_climb_similarities, ibu_maximum_climb_similarities],\n",
    "\"quant_event\" : [wc_event_similarities, ibu_event_similarities]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prone_acc': ['quant_weather'],\n",
       " 'prone_range': ['quant_weather', 'quant_snow', 'quant_event'],\n",
       " 'speed': ['wind_c', 'quant_snow'],\n",
       " 'standing_acc': ['altitude', 'wind_c'],\n",
       " 'standing_range': ['quant_weather', 'quant_snow']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_combos = []\n",
    "for speed in best_variables['speed']:\n",
    "    for pronea in best_variables['prone_acc']:\n",
    "        for standinga in best_variables['standing_acc']:\n",
    "            for proner in best_variables['prone_range']:\n",
    "                for standingr in best_variables['standing_range']:\n",
    "                    weight_list = [speed, pronea, standinga, proner, standingr]\n",
    "                    weight_combos.append(weight_list)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"compare_weightings\"></a>\n",
    "<a href = \"#comparisons\">Back to Comparing Weights Take 1</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "compare_weightings : for a given competition and each of a collection of weights, predicts\n",
    "                     a set number of times for each racer, and then uses \n",
    "                     evaluating_place_counts and evaluating_dist_from_mean to measure\n",
    "                     the goodness of results for each weight under consideration\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "year : a string that codes the season under consideration. It is of the form y1y2 where\n",
    "         y1 is the last two digits of the year in which the season started, and y2 is the \n",
    "         last two digits of the year in which the season ended.\n",
    "event : a string that codes the event under consideration. Possible values are 'CP01', \n",
    "        'CP02', 'CP03', 'CP04', 'CP05', 'CP06', 'CP07', 'CP08', 'CP09', 'CH__', 'OG__'\n",
    "n : an integer indicating the total number of time predictions to be made for each racer\n",
    "weights : a list of lists containing the world cup and ibu similarity weightings\n",
    "          for the speed prediction and prone and standing range predictions\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "race_places1 : A dataframe that contains one column for each weight that was considered \n",
    "               for a given race, and whose rows consist of the median number of correct \n",
    "               places (averaged over all of the competitors in the race), the median number \n",
    "               of times the prediction was within one place of the actual race, and so on \n",
    "               for two places, three places, five places, ten places, and twenty places.\n",
    "race_distances1 : A dataframe that contains one column for each weight that was considered \n",
    "                  for a given race, and whose rows consist of the percentage of racers for \n",
    "                  whom the mean of the distribution is within 10 seconds of their actual \n",
    "                  times, within 25 seconds of their actual times, and so on for 50 seconds, \n",
    "                  100 seconds, 150 seconds, and 200 seconds\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compare_weightings(year, event, n, weights):\n",
    "\n",
    "    race_places1 = pd.DataFrame()\n",
    "    race_distances1 = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(weights)): \n",
    "        #print 'Beginning cycle', i\n",
    "        weightings = weights[i]\n",
    "        print weightings\n",
    "        predicted_times, problem_racers = race_time_predictions(year,event,\n",
    "                                                n, weightings)\n",
    "                                        \n",
    "        places = place_counts(predicted_times)\n",
    "                    \n",
    "        found_percentiles = finding_percentiles(predicted_times, year,event)\n",
    "                    \n",
    "        place_order_evaluations = evaluating_place_counts(places,year,event)\n",
    "        place_order = pd.DataFrame(place_order_evaluations.median(axis = 'rows'), \n",
    "                                               columns = [i])\n",
    "        race_places1 = race_places1.merge(place_order, how='outer', left_index=True,\n",
    "                                                    right_index=True)\n",
    "                    \n",
    "        distances = evaluating_dist_from_mean(found_percentiles['diff from mean'])\n",
    "        distances.columns = [i]\n",
    "        race_distances1 = race_distances1.merge(distances, how='outer', \n",
    "                                                          left_index=True, right_index=True)\n",
    "    \n",
    "    return race_places1, race_distances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n"
     ]
    }
   ],
   "source": [
    "race_places, race_distances = compare_weightings('1314', 'CP02', 10,weight_combos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"pluck_best_weights\"></a>\n",
    "<a href = \"#comparisons\">Back to Comparing Weights Take 1</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "pluck_best_weights : takes a dataframe output by compare_weightings and assigns\n",
    "                     scores based on how each weighting fairs in comparison with\n",
    "                     the others in each of the categories\n",
    "\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe produced by compare_weightings (either race_places or race_distances)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "score : a pandas series of integers calculated by assigning 4 points for each time\n",
    "        a particular weighting scores the best of all weightings, 2 points for each\n",
    "        time it scores in the top quartile, and 1 point for each time it scores in the\n",
    "        top half and then summing these values\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pluck_best_weights(df):\n",
    "    \n",
    "    score = pd.DataFrame(columns = df.columns.tolist(), index = df.index.tolist())\n",
    "    if len(df) >= 8:\n",
    "        for i in range(0,len(df)):\n",
    "            #print df.iloc[i]\n",
    "            max_value = np.percentile(df.iloc[i,:],100)\n",
    "            quartile_3 = np.percentile(df.iloc[i,:],75)\n",
    "            median = np.percentile(df.iloc[i,:],50)\n",
    "        #print i, max_value,quartile_3, median\n",
    "            for j in range(df.shape[1]):\n",
    "                if df.iloc[i,j] == max_value:\n",
    "                    score.iloc[i,j] = 4\n",
    "                elif quartile_3 <= df.iloc[i,j] < max_value:\n",
    "                    score.iloc[i,j] = 2\n",
    "                elif median <= df.iloc[i,j] < quartile_3:\n",
    "                    score.iloc[i,j] = 1\n",
    "                else:\n",
    "                    score.iloc[i,j] = 0\n",
    "    else:\n",
    "        for i in range(0,len(df)):\n",
    "            #print df.iloc[i]\n",
    "            max_value = np.percentile(df.iloc[i,:],100)\n",
    "            quartile_3 = np.percentile(df.iloc[i,:],200/len(df))\n",
    "            median = np.percentile(df.iloc[i,:],50)\n",
    "        #print i, max_value,quartile_3, median\n",
    "            for j in range(df.shape[1]):\n",
    "                if df.iloc[i,j] == max_value:\n",
    "                    score.iloc[i,j] = 4\n",
    "                elif quartile_3 <= df.iloc[i,j] < max_value:\n",
    "                    score.iloc[i,j] = 2\n",
    "                elif median <= df.iloc[i,j] < quartile_3:\n",
    "                    score.iloc[i,j] = 1\n",
    "                else:\n",
    "                    score.iloc[i,j] = 0\n",
    "\n",
    "        \n",
    "                \n",
    "    score = score.sum(axis = 'rows')\n",
    "    \n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1213', 'CP08'],\n",
       " ['1213', 'CP06'],\n",
       " ['1314', 'CP09'],\n",
       " ['0910', 'CP08'],\n",
       " ['1011', 'CP04'],\n",
       " ['1213', 'CH__'],\n",
       " ['1011', 'CP01'],\n",
       " ['1112', 'CP05'],\n",
       " ['1213', 'CP04'],\n",
       " ['1112', 'CP03']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw 10 random events from the seasons 2009-10, 2010-11, 2011-12, 2012-13, 2013-14\n",
    "\n",
    "seasons = ['0910', '1011', '1112', '1213', '1314']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__']\n",
    "\n",
    "chosen_events = []\n",
    "for i in range(10):\n",
    "    redraw = 0\n",
    "    while redraw == 0:\n",
    "        competition = [random.choice(seasons), random.choice(events)]\n",
    "        redraw = 1\n",
    "        if competition in chosen_events:\n",
    "            redraw = 0\n",
    "        if competition[0] == '1314':\n",
    "            if competition[1] == 'CP05':\n",
    "                redraw = 0\n",
    "    if competition[0] in ['0910', '1314']:\n",
    "        if competition[1] == 'CH__':\n",
    "            competition[1] = 'OG__'\n",
    "\n",
    "    chosen_events.append(competition)\n",
    "    \n",
    "chosen_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now what I want to do is to run through these events using all of weightings, but I need some way of keeping track of how well each weighting does for each race..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213 CP08\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "1213 CP06\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "1314 CP09\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "0910 CP08\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "1011 CP04\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "1213 CH__\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "1011 CP01\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "1112 CP05\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "1213 CP04\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n",
      "1112 CP03\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['wind_c', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_event', 'quant_snow']\n"
     ]
    }
   ],
   "source": [
    "race_place_scores = pd.DataFrame()\n",
    "race_distance_scores = pd.DataFrame()\n",
    "\n",
    "for competition in chosen_events:\n",
    "    year = competition[0]\n",
    "    event = competition[1]\n",
    "    print year, event\n",
    "    race_places, race_distances = compare_weightings(year, event, 100,weight_combos)\n",
    "    \n",
    "    distances = pluck_best_weights(race_distances)\n",
    "    places = pluck_best_weights(race_places)\n",
    "    \n",
    "    race_place_scores = pd.concat([race_place_scores, places], axis = 'columns')\n",
    "    race_distance_scores = pd.concat([race_distance_scores, distances], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>26.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     0     0     0     0     0     0     0     0     0\n",
       "0   10.0  17.0  13.0  14.0  12.0  19.0   7.0  22.0   9.0  16.0\n",
       "1   14.0  24.0  11.0  10.0  22.0  12.0   8.0  20.0  21.0  16.0\n",
       "2   12.0  17.0   9.0  22.0  30.0  14.0  20.0  22.0   6.0  19.0\n",
       "3    9.0  16.0  14.0  24.0  18.0  19.0  17.0   9.0  19.0  17.0\n",
       "4   17.0   6.0  19.0   9.0  18.0   8.0  12.0  10.0  18.0  11.0\n",
       "5   15.0   7.0  17.0  17.0  24.0  13.0  14.0  15.0   4.0  12.0\n",
       "6   13.0  20.0  21.0  23.0  17.0   9.0  13.0  16.0  19.0  15.0\n",
       "7   12.0  12.0  14.0  23.0  16.0  13.0  16.0  22.0  13.0  15.0\n",
       "8   12.0   7.0  20.0  17.0  16.0   7.0  15.0  20.0  17.0  19.0\n",
       "9   12.0  15.0  22.0  19.0  17.0  10.0  13.0  12.0  11.0  10.0\n",
       "10  24.0   8.0  10.0  17.0  11.0  11.0  12.0  19.0  15.0  15.0\n",
       "11  17.0  18.0  11.0  12.0  23.0  14.0  12.0  18.0  21.0  11.0\n",
       "12  16.0  29.0  13.0  10.0  16.0  20.0  18.0  14.0  16.0  16.0\n",
       "13  10.0  22.0  12.0  19.0  18.0  26.0  18.0  26.0  15.0  21.0\n",
       "14  25.0  19.0  28.0  13.0  18.0  12.0   9.0  18.0  11.0  20.0\n",
       "15  24.0  21.0  13.0  13.0  14.0  16.0  14.0  18.0  28.0  15.0\n",
       "16  24.0  19.0  14.0  18.0  24.0  22.0  18.0  13.0  15.0  21.0\n",
       "17  12.0  19.0  26.0  13.0  12.0  15.0  15.0  15.0  17.0  18.0\n",
       "18  23.0  11.0  11.0  13.0  14.0  26.0  13.0  21.0  21.0  19.0\n",
       "19  20.0  13.0  24.0  16.0  21.0  14.0  12.0  20.0  19.0  13.0\n",
       "20  26.0  21.0  16.0  11.0  16.0  17.0  21.0  15.0   9.0  15.0\n",
       "21  26.0  19.0  22.0   8.0  18.0  16.0  22.0  10.0   9.0  17.0\n",
       "22  15.0  18.0  25.0  21.0  15.0  18.0  19.0  12.0  21.0  11.0\n",
       "23  23.0  16.0  15.0   8.0  18.0  13.0  28.0  18.0  18.0  21.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_place_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     0     0     0     0     0     0     0     0     0\n",
       "0    4.0  10.0  12.0  10.0  12.0   6.0  10.0  16.0  12.0  16.0\n",
       "1   12.0  12.0  14.0  12.0  16.0   4.0   8.0  18.0  14.0  12.0\n",
       "2    8.0  16.0  12.0  10.0  16.0   6.0  12.0  14.0  12.0  16.0\n",
       "3   12.0  14.0  14.0  12.0  18.0   2.0  14.0  12.0  16.0  14.0\n",
       "4    6.0  14.0   8.0  10.0  14.0   6.0  16.0  10.0  10.0  12.0\n",
       "5    8.0  16.0  12.0   6.0  10.0   8.0  10.0  12.0  12.0  10.0\n",
       "6    6.0  10.0  14.0   8.0  16.0   8.0  10.0  12.0  14.0  14.0\n",
       "7   10.0  10.0  16.0  12.0  14.0  10.0   8.0  16.0  18.0  16.0\n",
       "8    6.0  14.0  12.0  10.0  14.0   8.0   8.0  16.0  14.0  16.0\n",
       "9    4.0  10.0  10.0  14.0  10.0  12.0  10.0  16.0  12.0  14.0\n",
       "10   6.0  12.0   8.0  12.0  18.0  10.0  12.0  14.0  12.0  12.0\n",
       "11   8.0  14.0  12.0  12.0  18.0   6.0  14.0  16.0  10.0  16.0\n",
       "12  14.0  10.0  12.0  12.0  16.0  16.0  18.0  16.0  14.0  12.0\n",
       "13  18.0  10.0   8.0  14.0  16.0  20.0  18.0  16.0  14.0  10.0\n",
       "14  12.0   8.0  10.0  16.0  14.0  12.0  16.0  14.0  18.0   8.0\n",
       "15  12.0  10.0  18.0  14.0  14.0  12.0  18.0  20.0  14.0  10.0\n",
       "16  14.0  18.0  12.0   8.0  14.0  12.0  16.0  18.0  18.0   8.0\n",
       "17  14.0  14.0   8.0  12.0  14.0  12.0  18.0  16.0  18.0   6.0\n",
       "18  14.0  14.0  10.0  12.0  12.0  14.0  16.0  18.0  14.0  16.0\n",
       "19  16.0  18.0  12.0  16.0  14.0  14.0  18.0  18.0  14.0  14.0\n",
       "20  14.0  10.0  12.0  14.0  14.0  18.0  16.0  20.0  10.0  10.0\n",
       "21  16.0  12.0  16.0  12.0  14.0  16.0  16.0  18.0  14.0   6.0\n",
       "22  14.0  16.0  10.0  16.0  12.0  16.0  16.0  14.0  12.0  12.0\n",
       "23  12.0  18.0  10.0  12.0  12.0  16.0  14.0  16.0  14.0  12.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_distance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"score_of_scores\"></a>\n",
    "<a href = \"#comparisons\">Back to Comparing Weights Take 1</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "score_of_scores : takes a dataframe and, for each column, gives a score of 3 for every\n",
    "                  value above the 83rd percentile for that column, a score of 2 for \n",
    "                  every value above 75th percentile, and a score of -1.5 for every value \n",
    "                  below the 17th percentile\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe that is the output of pluck_best_weights applied to one of the ouputs\n",
    "    of compare_weightings\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "scores : a dataframe that contains a score for each of the weightings under consideration\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def score_of_scores(df):\n",
    "    \n",
    "    scores = pd.DataFrame(index = df.index.tolist(), columns = ['scores'])\n",
    "    scores.fillna(0, inplace = True)\n",
    "    for j in range(df.shape[1]):\n",
    "        # find the 83rd percentile\n",
    "        best = np.percentile(df.iloc[:,j], 83)\n",
    "        # find the 75th percentile\n",
    "        better = np.percentile(df.iloc[:,j],75)\n",
    "        # find the 17th percentile\n",
    "        bad = np.percentile(df.iloc[:,j],17)\n",
    "        for i in range(len(df)):\n",
    "        # score 3 for everything above the 83rd percentile\n",
    "            if df.iloc[i,j] >= best:\n",
    "                scores.loc[i,'scores'] = scores.loc[i,'scores'] + 3\n",
    "        # score 2 for everything above the 75th percentile\n",
    "            elif better <= df.iloc[i,j] < best:\n",
    "                scores.loc[i,'scores'] = scores.loc[i,'scores'] + 2\n",
    "        # score -1.5 for everything below the 17th percentile\n",
    "            elif df.iloc[i,j] <= bad:\n",
    "                scores.loc[i,'scores'] = scores.loc[i,'scores'] - 1.5\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now I have some way of calculating which weightings fair the best when it comes to predictions. In fact, I have four different values: the sum of race distance scores, the sum of race place scores, and the score of scores for both race distance scores and race place scores. I want to give each weighting a point for each time it scores in the top quartile (and a negative point for each time it scores in the bottom quartile(?)), and then choose the top 5-6 weightings.\n",
    "\n",
    "<a name = \"rank_weightings\"></a>\n",
    "<a href = \"#comparisons\">Back to Comparing Weights Take 1</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "rank_weightings : takes two dataframes produced by concatenating the outputs of \n",
    "                  pluck_best_weights via compare_weightings over a collection of \n",
    "                  events, applies score_or_scores to each of them, and, for\n",
    "                  each weighting, gives a positive point if they are in the top \n",
    "                  quartile and a negative point if they are in the bottom quartile\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df1 : a dataframe that is the output of pluck_best_weights applied to one of the ouputs\n",
    "      of compare_weightings\n",
    "df2 : a dataframe that is the output of pluck_best_weights applied to the other ouput\n",
    "      of compare_weightings\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "scores : a dataframe with a integer value between -4 and 4 for each weighting under\n",
    "         consideration\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rank_weightings(df1, df2):\n",
    "    \n",
    "    scores = pd.DataFrame(index = df1.index.tolist(), columns = ['scores']).fillna(0)\n",
    "    \n",
    "    # Deal with df1\n",
    "    \n",
    "    score_scores = score_of_scores(df1)\n",
    "    sum_of_scores = df1.sum(axis = 'columns')\n",
    "    \n",
    "    #return score_scores\n",
    "    for i in range(len(df1)):\n",
    "        if score_scores.loc[i,'scores'] >= np.percentile(score_scores, 75):\n",
    "            scores.loc[i,'scores'] = scores.loc[i,'scores'] + 1\n",
    "        elif score_scores.loc[i,'scores'] <= np.percentile(score_scores,25):\n",
    "            scores.loc[i,'scores'] = scores.loc[i,'scores'] - 1\n",
    "            \n",
    "    for i in range(len(df1)):\n",
    "        if sum_of_scores.loc[i] >= np.percentile(sum_of_scores, 75):\n",
    "            scores.loc[i,'scores'] = scores.loc[i,'scores'] + 1\n",
    "        elif sum_of_scores.loc[i] <= np.percentile(sum_of_scores,25):\n",
    "            scores.loc[i,'scores'] = scores.loc[i,'scores'] - 1\n",
    "\n",
    "    # Deal with df2\n",
    "    \n",
    "    score_scores = score_of_scores(df2)\n",
    "    sum_of_scores = df2.sum(axis = 'columns')\n",
    "    \n",
    "    for i in range(len(df1)):\n",
    "        if score_scores.loc[i,'scores'] >= np.percentile(score_scores, 75):\n",
    "            scores.loc[i,'scores'] = scores.loc[i,'scores'] + 1\n",
    "        elif score_scores.loc[i,'scores'] <= np.percentile(score_scores,25):\n",
    "            scores.loc[i,'scores'] = scores.loc[i,'scores'] - 1\n",
    "            \n",
    "    for i in range(len(df1)):\n",
    "        if sum_of_scores.loc[i] >= np.percentile(sum_of_scores, 75):\n",
    "            scores.loc[i,'scores'] = scores.loc[i,'scores'] + 1\n",
    "        elif sum_of_scores.loc[i] <= np.percentile(sum_of_scores,25):\n",
    "            scores.loc[i,'scores'] = scores.loc[i,'scores'] - 1\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_rankings = rank_weightings(race_place_scores, race_distance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 15, 19]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_combos_to_keep = [i for i in weight_rankings.index.tolist() \n",
    "                                     if weight_rankings.loc[i,'scores']>=3]\n",
    "weight_combos_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, this is rather fewer of these than I might like. What happens if I look at the outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    scores\n",
       "0       -4\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4       -4\n",
       "5       -4\n",
       "6       -2\n",
       "7        0\n",
       "8       -3\n",
       "9       -4\n",
       "10      -4\n",
       "11       0\n",
       "12       2\n",
       "13       4\n",
       "14       2\n",
       "15       4\n",
       "16       2\n",
       "17      -1\n",
       "18       2\n",
       "19       3\n",
       "20       1\n",
       "21       2\n",
       "22       1\n",
       "23       1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 13, 14, 15, 16, 18, 19, 21]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_combos_to_keep = [i for i in weight_rankings.index.tolist() \n",
    "                                     if weight_rankings.loc[i,'scores']>=2]\n",
    "weight_combos_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather'],\n",
       " ['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow'],\n",
       " ['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather'],\n",
       " ['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow'],\n",
       " ['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather'],\n",
       " ['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather'],\n",
       " ['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow'],\n",
       " ['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_weight_combos = []\n",
    "for i in range(len(weight_combos)):\n",
    "    if i in weight_combos_to_keep:\n",
    "        kept_weight_combos.append(weight_combos[i])\n",
    "        \n",
    "kept_weight_combos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I have 8 weightings that seem to be the best of the best. It's probably worth looking at what they are, and then I want to run cycles of 1000 trials on them to see how they fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"comparisons2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"comparisons2\"></a>\n",
    "\n",
    "# Comparing Weights Take 2\n",
    "\n",
    "Having reduced the number of weight combinations under consideration to 8, I wanted to compare them for a larger number of trials over a new list of 10 events. My initial evaluation of their relative merits failed to show that any weightings were as much stronger than the others as I would have <a href = \"#too_weak\">liked</a>. As a result, I put together three additional functions for comparing the outcomes that I got:\n",
    "1. <a href = \"#score_the_scores_v2\">```score_the_scores_v2```</a>: this function takes a dataframe produced by concatenating outputs of <a href = \"#pluck_best_weights\">```pluck_best_weights</a> and, for each column, assigns a score of 4 for each entry that is the maximal value for that column, a score of 2 for each entry that is in the top quartile of values for that column, and a score of -1.5 for each entry that is in the bottom quartile for that column. Scores are then summed along rows in order to produce a single score for each row (corresponding to a weight combination) in the dataframe.\n",
    "2. <a href = \"#ranking_points\">```ranking_points```</a>:  this function takes a one column dataframe and assigns scores to each index based on the order of the values of the entries in the column (first place gets 1  point, second place gets 2, etc. Tied values get points for their highest ranking).\n",
    "3. <a href = \"#ranking_by_places\">```ranking_by_places```</a>:  this function takes two dataframes df1 and df2 obtained by concatenating the outputs of ```pluck_best_weights``` applied to the results of <a href = \"#compare_weights\">```compare_weights```</a> over a selection of races. It then uses ```score_the_scores_v2``` and ```ranking_points``` to assign four different place values to each weight combination. The sums of these four values are then returned in a dataframe.\n",
    "\n",
    "\n",
    "\n",
    "<a href = \"#toc\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, setting up to run 500 cycles on a randomly selected batch of 10 races from the seasons 2009-2010 to 2013-2014, and seeing what comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1213', 'CH__'],\n",
       " ['1314', 'CP08'],\n",
       " ['1314', 'CP09'],\n",
       " ['1112', 'CP06'],\n",
       " ['1112', 'CH__'],\n",
       " ['1314', 'CP06'],\n",
       " ['1213', 'CP05'],\n",
       " ['1314', 'CP03'],\n",
       " ['0910', 'CP02'],\n",
       " ['1213', 'CP03']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seasons = ['0910', '1011', '1112', '1213', '1314']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__']\n",
    "\n",
    "chosen_events = []\n",
    "for i in range(10):\n",
    "    redraw = 0\n",
    "    while redraw == 0:\n",
    "        competition = [random.choice(seasons), random.choice(events)]\n",
    "        redraw = 1\n",
    "        if competition in chosen_events:\n",
    "            redraw = 0\n",
    "        if competition[0] == '1314':\n",
    "            if competition[1] == 'CP05':\n",
    "                redraw = 0\n",
    "    if competition[0] in ['0910', '1314']:\n",
    "        if competition[1] == 'CH__':\n",
    "            competition[1] = 'OG__'\n",
    "\n",
    "    chosen_events.append(competition)\n",
    "    \n",
    "chosen_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213 CH__\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "1314 CP08\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "1314 CP09\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "1112 CP06\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "1112 CH__\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "1314 CP06\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "1213 CP05\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "1314 CP03\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "0910 CP02\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n",
      "1213 CP03\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_event', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'wind_c', 'quant_snow', 'quant_snow']\n"
     ]
    }
   ],
   "source": [
    "race_place_scores = pd.DataFrame()\n",
    "race_distance_scores = pd.DataFrame()\n",
    "\n",
    "for competition in chosen_events:\n",
    "    year = competition[0]\n",
    "    event = competition[1]\n",
    "    print year, event\n",
    "    race_places, race_distances = compare_weightings(year, event, 500,kept_weight_combos)\n",
    "    \n",
    "    distances = pluck_best_weights(race_distances)\n",
    "    places = pluck_best_weights(race_places)\n",
    "    \n",
    "    race_place_scores = pd.concat([race_place_scores, places], axis = 'columns')\n",
    "    race_distance_scores = pd.concat([race_distance_scores, distances], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     0     0     0     0     0     0     0     0     0\n",
       "0  12.0  12.0  12.0  18.0  12.0  14.0  20.0  20.0  16.0  20.0\n",
       "1  10.0  20.0  20.0  20.0  16.0  18.0  16.0  14.0  14.0  16.0\n",
       "2  10.0  16.0  14.0  16.0  16.0  18.0  18.0  14.0  16.0  20.0\n",
       "3  12.0  20.0  20.0  18.0  12.0  10.0  16.0  16.0   8.0  18.0\n",
       "4  14.0  18.0  12.0  18.0  18.0  16.0  20.0  14.0  14.0  20.0\n",
       "5  18.0  14.0  14.0  16.0  16.0  12.0  14.0  16.0  12.0  18.0\n",
       "6  18.0  14.0  16.0  18.0  12.0  12.0  16.0  20.0  12.0  18.0\n",
       "7  18.0  16.0  18.0  16.0  12.0  16.0  22.0  20.0  14.0  16.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_distance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     0     0     0     0     0     0     0     0     0\n",
       "0  18.0  11.0  11.0  23.0  11.0   9.0  24.0  13.0  11.0   9.0\n",
       "1  11.0   9.0  20.0  20.0  11.0  21.0   6.0  13.0  15.0  10.0\n",
       "2  22.0  19.0  21.0  15.0   5.0  12.0  15.0  10.0  23.0  17.0\n",
       "3   7.0   7.0  17.0   7.0  12.0   7.0  11.0  13.0   9.0  11.0\n",
       "4   6.0  15.0  12.0  10.0  18.0   5.0  10.0   8.0   7.0  14.0\n",
       "5  16.0  12.0  20.0  11.0   9.0  16.0  17.0  13.0  19.0  10.0\n",
       "6  19.0  20.0   9.0  11.0  10.0  22.0  14.0  26.0   6.0  15.0\n",
       "7   7.0  10.0  12.0  10.0  22.0   8.0  15.0   9.0  11.0  20.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_place_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"too_weak\"></a>\n",
    "<a href = \"#comparisons2\">Back to Comparing Weights Take 2</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scores\n",
       "0       1\n",
       "1       2\n",
       "2       2\n",
       "3      -3\n",
       "4       0\n",
       "5      -1\n",
       "6       1\n",
       "7       1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_weightings(race_distance_scores, race_place_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    150.0\n",
       "5    150.0\n",
       "0    156.0\n",
       "6    156.0\n",
       "2    158.0\n",
       "1    164.0\n",
       "4    164.0\n",
       "7    168.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_distance_scores.sum(axis = 'columns').sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scores\n",
       "5    -1.5\n",
       "6     1.5\n",
       "3     3.0\n",
       "7     4.5\n",
       "2     7.5\n",
       "1     9.0\n",
       "4     9.0\n",
       "0    10.5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_of_scores(race_distance_scores).sort_values('scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    101.0\n",
       "4    105.0\n",
       "7    124.0\n",
       "1    136.0\n",
       "0    140.0\n",
       "5    143.0\n",
       "6    152.0\n",
       "2    159.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_place_scores.sum(axis = 'columns').sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scores\n",
       "4    -6.0\n",
       "3    -3.0\n",
       "7     1.5\n",
       "0     6.0\n",
       "1     7.5\n",
       "5     9.0\n",
       "6     9.0\n",
       "2    13.5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_of_scores(race_place_scores).sort_values('scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the scoring, it seems somewhat unclear whether or not there are any weightings that are significantly better than the others, so I'm going to go back to the dataframes ```race_place_scores``` and ```race_distance_scores``` and see what I can tease out from those. In particular, I'm going to consider slightly changing my function for ```score_of_scores``` in order to weight having the best result in a particular race slightly more heavily (from 3 to 4), but I'm going to leave all of the other values alone. I'll then have\n",
    "\n",
    "<a name = \"score_the_scores_v2\"></a>\n",
    "<a href = \"#comparisons2\">Back to Comparisons Take 2</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "score_the_scores_v2 : takes a dataframe and, for each column, assigns a score of 4 for\n",
    "                      each entry that is the maximal value for that column, a score of\n",
    "                      2 for each entry that is in the top quartile of values for that \n",
    "                      column, and a score of -1.5 for each entry that is in the bottom\n",
    "                      quartile for that column. Scores are then summed along rows in \n",
    "                      order to produce a single score for each row (which correspond\n",
    "                      to weight combinations) in the dataframe.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe that is the output of running compare_weights and pluck_best_weights\n",
    "     over a selection of races. It could be either the dataframe of distance measures\n",
    "     or the dataframe of place accuracy\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "summed_scores : a dataframe with a single column containing the summed scores assigned\n",
    "                to each weight combination (index value)\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def score_the_scores_v2(df):\n",
    "    \n",
    "    scores = pd.DataFrame(index = df.index.tolist(), columns = range(0,df.shape[1]))\n",
    "    \n",
    "    for i in range(df.shape[1]):\n",
    "        max_value = max(df.iloc[:,i])\n",
    "        #print max_value\n",
    "        quart_3 = np.percentile(df.iloc[:,i],75)\n",
    "        #print quart_3\n",
    "        #print max_value - quart_3\n",
    "        quart_1 = np.percentile(df.iloc[:,i],25)\n",
    "        \n",
    "        for j in range(len(df)):\n",
    "            if df.iloc[j,i] >= max_value:\n",
    "                scores.iloc[j,i] = 4\n",
    "            elif max_value > df.iloc[j,i] >= quart_3:\n",
    "                scores.iloc[j,i] = 2\n",
    "            elif df.iloc[j,i] <= quart_1:\n",
    "                scores.iloc[j,i] = -1.5\n",
    "            else:\n",
    "                scores.iloc[j,i] = 0\n",
    "    scores = pd.DataFrame(scores)\n",
    "    summed_scores = scores.sum(axis = 'columns')\n",
    "    \n",
    "    return summed_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then in order to rank the weightings, I want a somewhat different function than I was using before. In particular, for the 4 different measures that we had, I was giving a weighting one point for being in the top 25%, and removing a point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"ranking_points\"></a>\n",
    "<a href = \"#comparisons2\">Back to Comparing Weights Take 2</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ranking_points : takes a single column dataframe and assigns scores to each index based on\n",
    "                 the order of the values of the entries in the column (first place gets 1 \n",
    "                 point, second place gets 2, etc. Tied values get points for their highest\n",
    "                 ranking).\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df : a dataframe containing a single column of values\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "scores : a single column dataframe consisting of an index of weight combinations and\n",
    "         a column of places (scores)\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def ranking_points(df):\n",
    "    \n",
    "    df = df.sort_values(ascending = False)\n",
    "    \n",
    "    scores = []\n",
    "    current_score = 0\n",
    "    current_value = 1000\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i] == current_value:\n",
    "            scores.append([df.index[i],current_score])\n",
    "        else:\n",
    "            current_value = df.iloc[i]\n",
    "            current_score = i+1\n",
    "            scores.append([df.index[i],current_score])\n",
    "    \n",
    "    scores = pd.DataFrame(scores, columns = ['weight','place'])\n",
    "    scores.set_index('weight', drop = True, inplace = True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"ranking_by_places\"></a>\n",
    "<a href = \"#comparisons2\">Back to Comparing Weights Take 2</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "\n",
    "ranking_by_places : takes two dataframes df1 and df2 obtained by running compare_weights\n",
    "                    and pluck_best_weights over a selection of races. It then uses\n",
    "                    score_the_scores_v2 and ranking_points to assign four different \n",
    "                    place values to each weight combination. The sums of these four values\n",
    "                    are then returned in a dataframe.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "df1 : a dataframe that is the output of running compare_weights and pluck_best_weights\n",
    "      over a selection of races. It could be either the dataframe of distance measures\n",
    "      or the dataframe of place accuracy\n",
    "df2 : the dataframe that is the pair to df1. It is distance measures where df1 is place\n",
    "      accuracy, or place accuracy where df1 is distance measures\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "ranking : a dataframe containing the weight combinations as the index and a score\n",
    "          as the sole column\n",
    "\n",
    "Examples\n",
    "--------\n",
    "\"\"\"\n",
    "\n",
    "def ranking_by_places(df1,df2):\n",
    "    \n",
    "    points_1 = ranking_points(df1.sum(axis = 'columns'))\n",
    "    points_2 = ranking_points(score_the_scores_v2(df1))\n",
    "    points_3 = ranking_points(df2.sum(axis = 'columns'))\n",
    "    points_4 = ranking_points(score_the_scores_v2(df2))\n",
    "    \n",
    "    ranking = points_1+points_2+points_3+points_4\n",
    "    \n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        place\n",
       "weight       \n",
       "0          14\n",
       "1          13\n",
       "2           9\n",
       "3          28\n",
       "4          21\n",
       "5          22\n",
       "6          16\n",
       "7          17"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_by_places(F, race_distance_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looking at both this and the result of cell [47], it appears that the strongest weights here are numbers 2 and 1, with 2 the stronger of them. These correspond to weights 13 and 14 in our original list of weight combinations, which have as weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n"
     ]
    }
   ],
   "source": [
    "print weight_combos[13]\n",
    "print weight_combos[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"combivars\"></a>\n",
    "# Considering combined variables\n",
    "\n",
    "Looking at the two best weight combinations above, we see that they agree on three of the five weight inputs, and have opposite values for the last two (range) weight inputs. As a result, it seems not unreasonable to me to consider these two weightings in conjunction with a few combined weightings, namely replacing either the prone range or standing range weighting (or both) with the geometric mean of ```quant_weather``` and ```quant_snow```. \n",
    "\n",
    "From there, we again choose 10 random events from our five seasons and run 500 trials for each combination of event and weighting. We evaluate the results using both <a href = \"#rank_weightings\">```rank_weightings```</a> and <a href = \"#weightings_by_places\">```weightings_by_places```</a>, and we find that one weighting, number 4 of the weightings that we consider here which corresponds to number 14 in the original list of weight combinations, is clearly the best performer over these races. We see that its <a href = \"#ranking_by_ranks_score\">```rank_weightings```</a> result is 4, which indicates that it is in the top quartile for all four measures of goodness of fit, and that its <a href = \"#ranking_by_places_score\">```weightings_by_places```</a> result is 5, which means that it was the best weight combination in three of the four measures, and the second best combination for the fourth. \n",
    "\n",
    "<a href = \"#toc\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc_product = (wc_quant_weather_similarities*wc_quant_snow_similarities)**(0.5)\n",
    "ibu_product = (ibu_quant_weather_similarities*ibu_quant_snow_similarities)**(0.5)\n",
    "weights['product'] = [wc_product, ibu_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_combos_to_test = [\n",
    "    ['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow'],\n",
    "    ['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow'],\n",
    "    ['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product'],\n",
    "    ['quant_snow', 'quant_weather', 'altitude', 'product', 'product'],\n",
    "    ['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather'],\n",
    "    ['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather'],\n",
    "    ['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now I want to do the same thing that I did above, namely running tests of 500 trials over 10 different races to see which of these (if any) ends up being clearly the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1112', 'CP02'],\n",
       " ['1314', 'CP07'],\n",
       " ['1314', 'CP09'],\n",
       " ['1112', 'CP07'],\n",
       " ['0910', 'CP05'],\n",
       " ['1112', 'CP08'],\n",
       " ['1314', 'OG__'],\n",
       " ['1213', 'CP09'],\n",
       " ['1011', 'CP04'],\n",
       " ['0910', 'CP06']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seasons = ['0910', '1011', '1112', '1213', '1314']\n",
    "events = ['CP01','CP02','CP03','CP04','CP05','CP06','CP07','CP08','CP09','CH__']\n",
    "\n",
    "chosen_events = []\n",
    "for i in range(10):\n",
    "    redraw = 0\n",
    "    while redraw == 0:\n",
    "        competition = [random.choice(seasons), random.choice(events)]\n",
    "        redraw = 1\n",
    "        if competition in chosen_events:\n",
    "            redraw = 0\n",
    "        if competition[0] == '1314':\n",
    "            if competition[1] == 'CP05':\n",
    "                redraw = 0\n",
    "    if competition[0] in ['0910', '1314']:\n",
    "        if competition[1] == 'CH__':\n",
    "            competition[1] = 'OG__'\n",
    "\n",
    "    chosen_events.append(competition)\n",
    "    \n",
    "chosen_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112 CP02\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "1314 CP07\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "1314 CP09\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "1112 CP07\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "0910 CP05\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "1112 CP08\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "1314 OG__\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "1213 CP09\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "1011 CP04\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n",
      "0910 CP06\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_snow']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_weather', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'product']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'product', 'quant_weather']\n",
      "['quant_snow', 'quant_weather', 'altitude', 'quant_snow', 'product']\n"
     ]
    }
   ],
   "source": [
    "race_place_scores = pd.DataFrame()\n",
    "race_distance_scores = pd.DataFrame()\n",
    "\n",
    "for competition in chosen_events:\n",
    "    year = competition[0]\n",
    "    event = competition[1]\n",
    "    print year, event\n",
    "    race_places, race_distances = compare_weightings(year, event, 500,more_combos_to_test)\n",
    "    \n",
    "    distances = pluck_best_weights(race_distances)\n",
    "    places = pluck_best_weights(race_places)\n",
    "    \n",
    "    race_place_scores = pd.concat([race_place_scores, places], axis = 'columns')\n",
    "    race_distance_scores = pd.concat([race_distance_scores, distances], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     0     0     0     0     0     0     0     0     0\n",
       "0  12.0   7.0  11.0  20.0  16.0   7.0  16.0  19.0  23.0  25.0\n",
       "1   7.0  13.0  21.0  18.0  17.0  14.0   9.0   8.0  14.0  14.0\n",
       "2  25.0  15.0  25.0   8.0   6.0   8.0  10.0   6.0  12.0  13.0\n",
       "3   7.0  10.0  12.0  10.0  13.0  19.0  14.0  17.0  20.0  14.0\n",
       "4  10.0  22.0  11.0  11.0  25.0  15.0  23.0  24.0   9.0  13.0\n",
       "5  19.0   8.0  18.0  19.0   6.0  13.0   4.0   9.0  10.0  13.0\n",
       "6  13.0  18.0   9.0   8.0  17.0  25.0  11.0  17.0  23.0   8.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_place_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     0     0     0     0     0     0     0     0     0\n",
       "0  16.0  16.0  18.0  20.0  16.0  18.0  14.0  19.0  18.0  12.0\n",
       "1  14.0  16.0  14.0  14.0  22.0  20.0  14.0  16.0  18.0  16.0\n",
       "2  14.0  18.0  22.0  12.0  16.0  22.0  16.0  19.0  18.0  18.0\n",
       "3  20.0  18.0  18.0  16.0  16.0  20.0  18.0  17.0  20.0  18.0\n",
       "4  16.0  16.0  14.0  16.0  18.0  16.0  22.0  20.0  20.0  20.0\n",
       "5  12.0  12.0  16.0  14.0  18.0  20.0  18.0  15.0  20.0  12.0\n",
       "6  14.0  12.0  16.0  12.0  14.0  20.0  20.0  15.0  14.0  18.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_distance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"ranking_by_places_score\"></a>\n",
    "<a href = \"#combivars\">Back to Combined Variables</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        place\n",
       "weight       \n",
       "0          11\n",
       "1          21\n",
       "2          18\n",
       "3          11\n",
       "4           5\n",
       "5          25\n",
       "6          20"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_by_places(race_place_scores, race_distance_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"ranking_by_ranks_score\"></a>\n",
    "<a href = \"#combivars\">Back to Combined Variables</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scores\n",
       "0       2\n",
       "1      -1\n",
       "2      -2\n",
       "3       2\n",
       "4       4\n",
       "5      -3\n",
       "6      -1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_weightings(race_distance_scores, race_place_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"conclusions\"></a>\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "Based on the work in this notebook, it seems that we have found a single weight combination which, at least under the conditions that we have applied, appears to be superior to the other combinations.  It uses the weights\n",
    "- speed : ```quant_snow```\n",
    "- prone accuracy : ```quant_weather```\n",
    "- standing accuracy : ```altitude```\n",
    "- prone range time : ```quant_snow```\n",
    "- standing range time :```quant_weather```\n",
    "\n",
    "To this point, however, all of the work that we have done has considered the effectiveness of our model on races from the middle part of our data, chronologically. At this point, we need to see how effective our model is on more recent races, and to see how it fairs in comparison with a more naive model of these races. For that, we will move to a new notebook.\n",
    "\n",
    "<a href = \"#toc\">Table of Contents</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
